{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4984e466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syedusmani/Documents/Projects/RAG_Assistant/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3afa801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever is ready. Connected to ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "vector_store = Chroma(embedding_function=embeddings, persist_directory=\"../chroma_db\")\n",
    "\n",
    "query_engine = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Retriever is ready. Connected to ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd4d7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents.\n",
      "From: data/ADVANCED_ML_ALGORITHMS/Auto-context and Its Application to High-level Vision Tasks.pdf\n",
      "# Auto-context and Its Application to High-level Vision Tasks and 3D Brain Image Segmentation  \n",
      "Zhuowen Tu and Xiang Bai  \n",
      "Lab of Neuro Imaging, University of California, Los Angeles  \n",
      "_{_ ztu,xiang.b\n",
      "---\n",
      "From: data/ADVANCED_ML_ALGORITHMS/Efficient Scale Space Auto-Context for Image Segmentation and Labeling.pdf\n",
      "## **Efficient Scale Space Auto-Context for Image Segmentation and Labeling**  \n",
      "## Jiayan Jiang and Zhuowen Tu  \n",
      "Lab of Neuro Imaging, Department of Neurology, Department of Computer Science, UCLA _{_\n",
      "---\n",
      "From: data/DEEP_LEARNING/Transformer_07-2.pdf\n",
      "# Residual Connections  \n",
      "OMITTED IMAGE  \n",
      "‚Ä¢ Each encoder has a residual connection, followed by  \n",
      "layer normalization  \n",
      "## Non-Autoregressive Encoding  \n",
      "OMITTED IMAGE  \n",
      "## Autoregressive Decoding  \n",
      "OMI\n",
      "---\n",
      "From: data/DEEP_LEARNING/CNN_04-1.pdf\n",
      "# _if we want to recognize even a few basic patterns at each location,_\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "test_docs = query_engine.invoke(\"How does auto-context work?\")\n",
    "print(f\"Retrieved {len(test_docs)} documents.\")\n",
    "for doc in test_docs:\n",
    "    print(f\"From: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(doc.page_content[:200])  # First 200 chars\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19122c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=5)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d6d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\")\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a school readings and lectures. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model=llm, tools=[retrieve_context], system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36f3ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is Monte Carlo Dropout and why is it used?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (d174e36a-36e9-4ec1-bbc0-b20980a8e93b)\n",
      " Call ID: d174e36a-36e9-4ec1-bbc0-b20980a8e93b\n",
      "  Args:\n",
      "    query: Monte Carlo Dropout and its purpose\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'header': '', 'chunk_number': 0, 'source': 'data/AI_ALGORITHMS/06_MonteCarlo.pdf', 'course': 'AI_ALGORITHMS'}\n",
      "Content: **Monte Carlo methods \"I'd rather be vaguely right than precisely wrong.\" ‚Äî John Maynard Keynes l**  \n",
      "**‚ÄúLike an octopus under a rock, Le Chiffre watched him from the other side of the table. Bond reached out a steady right hand and drew the cards towards him.‚Äù ‚Äî Ian Fleming,** _**Casino Royale**_ **l**  \n",
      "**Jason Fleischer, PhD Department of Cognitive Science, UC San Diego https://jgfeischer.coml**  \n",
      "## Delay A3/A4 one week?  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Assignment Currently Proposed<br>A3 - MDP and DP<br>Week 7: Feb 19 Week 8: Feb 26<br>(Could add MC)<br>A4 - TD-Learning<br>Week 9: Mar 5 Week 10: Mar 12<br>RL NNs<br>**----- End of picture text -----**<br>  \n",
      "||Sample backup|Full backup|\n",
      "|---|---|---|\n",
      "|Bootstrap sample|||\n",
      "|Full sample|||\n",
      "\n",
      "Source: {'source': 'data/AI_ALGORITHMS/06_MonteCarlo.pdf', 'header': 'Learned blackjack state-value functions', 'chunk_number': 3, 'course': 'AI_ALGORITHMS'}\n",
      "Content: # Learned blackjack state-value functions  \n",
      "OMITTED IMAGE  \n",
      "## Backup diagram for Monte Carlo  \n",
      "‚ùê Entire rest of episode included  \n",
      "OMITTED IMAGE  \n",
      "‚ùê Only one choice considered at each state (unlike DP)  \n",
      "OMITTED IMAGE  \n",
      " thus, there will be an explore/ exploit dilemma  \n",
      "OMITTED IMAGE  \n",
      "‚ùê Does not bootstrap from successor states‚Äôs values (unlike DP)  \n",
      "‚ùê Time required to estimate one state does not depend on the total number of states  \n",
      "OMITTED IMAGE  \n",
      "R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction  \n",
      "terminal state  \n",
      "## Monte Carlo Estimation of Action Values (Q)  \n",
      "‚ùê Monte Carlo is most useful when a model is not available  \n",
      "- _*_  \n",
      "- We want to learn _q_  \n",
      "- ‚ùê _s_ and action _a q_ œÄ( _s,a_ ) - average return starting from state œÄ  \n",
      "- following  \n",
      "- ‚ùê Converges asymptotically _if_ every state-action pair is visited  \n",
      "- ‚ùê _Exploring starts:_ Every state-action pair has a non-zero probability of being the starting pair  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Monte Carlo Control<br>**----- End of picture text -----**<br>  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "evaluation<br>Q q‚á°<br>‚á°<br>Q<br>‚á°<br>greedy( Q )<br>improvement<br>**----- End of picture text -----**<br>  \n",
      "‚ùê MC policy iteration: Policy evaluation using MC methods followed by policy improvement  \n",
      "‚ùê Policy improvement step: greedify with respect to value (or action-value) function  \n",
      "## Convergence of MC Control  \n",
      "- ‚ùê Greedified policy meets the conditions for policy improvement:  \n",
      "= _s q‚á°k_ ( _s, ‚á°k_ +1( )) _q‚á°k_ ( _s,_ argmax _q‚á°k_ ( _s, a_ ))  \n",
      "_a_  \n",
      "= max  \n",
      "_q‚á°k_ ( _s, a_ )  \n",
      "_a_  \n",
      "_‚â•_  \n",
      "_s q‚á°k_ ( _s, ‚á°k_ ( ))  \n",
      "= ‚â•  \n",
      "= ‚â•  \n",
      "_v s . ‚á° k_ ( )  \n",
      "- ‚ùê And thus must be ‚â• œÄ _k_ by the policy improvement theorem  \n",
      "- ‚ùê This assumes exploring starts and infinite number of episodes  \n",
      "for MC policy evaluation  \n",
      "for MC policy evaluation  \n",
      "To solve the latter:  \n",
      "‚ùê To solve the latter:  \n",
      "- update only to a given level of performance  \n",
      "- alternate between evaluation and improvement per episode  \n",
      "11  \n",
      "R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction  \n",
      "## Monte Carlo Exploring Starts  \n",
      "_s 2_ S _a 2_ A _s_ Initialize, for all , ( ): _Q_ ( _s, a_ ) arbitrary _‚á° s_ ( ) arbitrary _Returns_ ( _s, a_ ) empty list  \n",
      "Fixed point is optimal policy œÄ[*]  \n",
      "Now proven (almost)  \n",
      "Repeat forever: Choose _S_ 0 _2_ S and _A_ 0 _2_ A _S_ 0 _>_ 0 ( ) s.t. all pairs have probability _S ‚á°_ Generate an episode starting from 0 _, A_ 0, following For each pair _s, a_ appearing in the episode: _G_ return following the first occurrence of _s, a G_ to _Returns_ Append ( _s, a_ ) _Returns Q_ ( _s, a_ ) average( ( _s, a_ )) For each _s_ in the episode: _‚á° s_ ar max _s a_ ( ) g _a Q_ ( _,_ )  \n",
      "> R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction 12  \n",
      "## Blackjack example continued  \n",
      "OMITTED IMAGE  \n",
      "OMITTED MATH EQUATION  \n",
      "## ‚ùê Exploring starts ‚ùê Initial policy as described before  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "v [*]<br>! [*] V * [*]<br>*<br>21<br>STICK 20<br>19<br>18<br>Usable +1<br>17<br>ace 16<br>\"<br>15 1<br>HIT 14<br>13<br>12<br>11<br>A 2 3 4 5 6 7 8 9 10<br>21<br>20<br>STICK 19<br>No 18 +1<br>17<br>usable 16<br>15 \"1<br>ace<br>HIT 14<br>13<br>12<br>11<br>A 2 3 4 5 6 7 8 9 10<br>Dealer showing<br>Dealer showing<br>Dealer showing<br>Dealer showing<br>AA<br>AA<br>0<br>1010<br>1010<br>1212<br>1212<br>21<br>21<br>21<br>21<br>Player sum<br>Player sum<br>Player sum<br>Player sum<br>**----- End of picture text -----**<br>  \n",
      "R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction  \n",
      "On-policy Monte Carlo Control  \n",
      "‚ùê _On-policy:_ learn about policy currently executing ‚ùê How do we get rid of exploring starts?  ~~e ete~~ : The policy must b rnally _soft_  \n",
      "‚Äì œÄ _a_ | _s > 0_ for all _s_ and _a_ ( )  e.g. Œµ-soft policy: _‚úè ‚úè_ ‚Äì or 1 _‚àí ‚úè_ + probability of an action = A _s_ A _s_ max (greedy) _|_ ( ) _| |_ ( ) _|_ non-max  \n",
      "max (greedy)  \n",
      "\\mathbf{\\Pi}^{\\mathsf{P}}  \n",
      "‚ùê Similar to GPI: mo _towards_ ~~ve po~~ licy greedy policy  \n",
      "Œµ (e.g., -greedy)  \n",
      "‚ùê Œµ Converges to best -soft policy  \n",
      "14  \n",
      "## On-policy MC Control  \n",
      "_s 2_ S _a 2_ A _s_ Initialize, for all , ( ):  \n",
      "_Q_ ( _s, a_ ) arbitrary _Returns_ ( _s, a_ ) empty list  \n",
      "_‚á° a s \"_ ( _|_ ) an arbitrary -soft policy  \n",
      "Repeat forever:  \n",
      "_‚á°_ (a) Generate an episode using  \n",
      "(b) For each pair _s, a_ appearing in the episode:  \n",
      "_G_ return following the first occurrence of _s, a_  \n",
      "_G_ to _Returns_ Append ( _s, a_ )  \n",
      "_Returns Q_ ( _s, a_ ) average( ( _s, a_ ))  \n",
      "_s_ (c) For each in the episode:  \n",
      "_A[‚á§]_ arg max _a Q_ ( _s, a_ )  \n",
      "For all _a 2_ A _s_ ( ):  \n",
      "_s_ 1 _‚àí \"_ + A _s \"/|_ ( ) _|_  \n",
      "if _a_ = _A[‚á§]_  \n",
      "_‚á° a s_ ( _|_ )  \n",
      "= if _a A[‚á§] 6_  \n",
      "_\"/|_ A( _s_ ) _|_  \n",
      "R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction  \n",
      "## What we‚Äôve learned about Monte Carlo so far  \n",
      "‚ùê MC has several advantages over DP:  \n",
      " Can learn directly from interaction with environment  \n",
      "- No need for full models  \n",
      "- No need to learn about ALL states (no bootstrapping)  \n",
      "- Less harmed by violating Markov property (later in book)  \n",
      "- ‚ùê MC methods provide an alternate policy evaluation process ‚ùê One issue to watch for: maintaining sufficient exploration  exploring starts, soft policies  \n",
      "## Off-policy methods  \n",
      "‚ùê Learn the value of the _target policy_ œÄ from experience due to _behavior policy_ ùúá  \n",
      "‚ùê For example, œÄ is the greedy policy (and ultimately the ùúÄ optimal policy) while ùúá is exploratory (e.g., -soft)  \n",
      "‚ùê , i.e., that In general, we only require _coverage_ ùúá generates behavior that covers, or includes, œÄ  \n",
      "at which _s,a_ for every  \n",
      "_‚á° a s >_ 0 ( _|_ )  \n",
      "_a s >_ 0 at which _‚á° a s >_ 0 _¬µ_ ( _|_ ) for every _s,a_ ( _|_ )  \n",
      "‚ùê Idea: _importance sampling_  \n",
      "Idea: _importance sampling_  \n",
      "of  \n",
      "‚Äì of  \n",
      "‚Äì Weight each return by the _ratio of the probabilities_ of the trajectory under the two policies  \n",
      "24  \n",
      "## Importance Sampling Ratio  \n",
      "‚ùê _S_ Probability of the rest of the trajectory, after _t_ , under œÄ:  \n",
      "Pr _A S ‚àí ‚á° { t, St_ +1 _, At_ +1 _, . . . , ST | t, At_ : _T_ 1 _‚á† }_  \n",
      "= _‚á° A S S S ‚á° A S ¬∑ ¬∑ ¬∑ S S ‚àí ‚àí_ ( _t| t_ ) _p_ ( _t_ +1 _| t, At_ ) ( _t_ +1 _| t_ +1) _p_ ( _T | T_ 1 _, AT_ 1) _‚àí T_ 1 = _‚á° A S S S_ ( _k| k_ ) _p_ ( _k_ +1 _| k, Ak_ ) _,_ Y  \n",
      "_‚àí T_ 1 Y = _k t_  \n",
      "‚ùê In importance sampling, each return is weighted by the relative probability of the trajectory under the two policies  \n",
      "\\frac{\\prod_{k=t}^{T-1}\\pi(A_{k}|S_{k})p(S_{k+1}|S_{k},A_{k})}{\\prod_{k=t}^{T-1}\\mu(A_{k}|S_{k})p(S_{k+1}|S_{k},A_{k})}=\\prod_{k=t}\\frac{\\pi(A_{k}|S_{k})}{\\mu(A_{k}|S_{k})}  \n",
      "\\rho_{t}^{T}\\,=\\,  \n",
      "OMITTED MATH EQUATION  \n",
      "OMITTED MATH EQUATION  \n",
      "‚ùê This is called the _importance sampling ratio_  \n",
      "‚ùê All importance sampling ratios have expected value 1  \n",
      "_‚á° A S k k_ ( _|_ )  \n",
      "\\underline{{{\\sum}}}_{a}^{\\phantom{\\infty}}\\Pi(a\\vert S_{k}^{\\dagger})\\frac{\\pi(a\\vert S_{k}^{\\phantom{*}})}{\\pi^{\\phantom{*}}(a\\vert S_{k}^{\\phantom{*}})}\\longrightarrow\\Lambda_{*}^{\\phantom{*}}\\pi(a\\vert S_{k}^{\\phantom{a}})=1.  \n",
      "\\mathbb{E}_{A_{k}\\sim\\mu}\\Big|\\frac{n\\left(x^{+}\\kappa\\right|^{1/k}}{\\mu(A_{k}|S_{k})}\\Big|=\\sum\\mu(a|S_{k})\\frac{n\\left(\\alpha|\\right)^{-}k\\right)}{\\mu(a|S_{k})}=\\sum\\pi(a|S_{k})=1.  \n",
      "_A S ¬µ_ ( _k| k_ )  \n",
      "_a a_  \n",
      "_a a_  \n",
      "25  \n",
      "25  \n",
      "## Importance Sampling  \n",
      "- ‚ùê New notation: time steps increase across episode boundaries:  \n",
      "OMITTED IMAGE  \n",
      "\\mathrm{\\boldmath\\large\\Gamma~}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!  \n",
      "{\\begin{array}{l l l l l l l l l l l}{9}&{4}&{9}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{{}\\pm}&{}&{}&{}&{{}\\pm}&{}&{{}\\pm}&{}&{}&{}&{}&{}&{{}\\pm}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}&{}\\end{array}}  \n",
      "\\begin{array}{c c c c c c c c c c c c c c c c c c c c}{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{}}&{{\\prod}}\\end{array}  \n",
      "T _s_ 4 20 _T T_ ( ) = _{ , }_ (4) = 9 (20) = 25 set of start times next termination times  \n",
      "_,_  \n",
      "‚ùê _Ordinary importance sampling_ forms estimate  \n",
      "> _[T][t]_  \n",
      "V{\\left(s\\right)}\\ \\doteq\\ \\frac{\\sum_{t\\in\\Gamma(s)}\\rho_{t}^{T(t)}G_{t}}{\\left|\\Im\\left(s\\right)\\right|}  \n",
      "- ‚ùê Whereas _weighted importance sampling_ forms estimate  \n",
      "_weighted importance sampling_  \n",
      "V{\\big(}s{\\big)}\\ \\;\\doteq\\atop{}{\\frac{t{\\in}\\Gamma{\\big(}s{\\big)}\\,{\\mathcal{P}}_{t}}{{\\sum}{t{\\in}\\Gamma{\\mathfrak{S}}{\\mathfrak{S}}{\\big(}s{\\big)}\\,{\\mathcal{P}}_{t}}}}  \n",
      "## Example of infinite variance  \n",
      "## under _ordinary_ importance sampling  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "R  = +1<br>‚á° (left |s ) = 1  = 1<br>0.1 ùú∏ ‚á° (right |s ) ‚á° (left |s )<br>right<br>left s R  = 0 s [=] 0 2<br>0.9 ¬µ (right | ) left s [=]<br>¬µ ( | )<br>¬µ (left |s ) = [1] v‚á° ( s ) = 1<br>2<br>R  = 0<br>Trajectory G 0 ‚á¢ [T] 0<br>s,  left ,  0 , s, R left = +1 ,  0 , s,  left ,  0 , s,  right ,  0 ,  ‚ñ® 0 0 OIS:<br>‚á° left s<br>( | ) = 1<br>s,  left ,  0 , s,  left ,  0 , s,  left ,  0 , s,  left ,  +1 ,  ‚ñ® 1 16<br>0.1<br>left s right P t2 T( s ) [‚á¢] t [T] [(] [t] [)] Gt<br>0.9 R  = 0 V s  ,<br> ( )<br>¬µ (left |s ) = [1] | T(( s ) |<br>2<br>R  = 0<br>2<br>Monte-Carlo<br>estimate of  WIS:<br>          with  v s<br>‚á° ( )<br>G<br>ordinary P t2 T s [‚á¢] t [T] [(] [t] [)] t<br>( )<br>V s  ,<br>1  ( )<br>importance<br>sampling P t2 T( s ) [‚á¢] t [T] [(] [t] [)]<br>(ten runs)<br>0<br>1 10 100 1000 10,000 100,000 1,000,000 10,000,000 100,000,000<br>Episodes (log scale)<br>**----- End of picture text -----**<br>  \n",
      "0 2  \n",
      "_s |_ T(( ) _|_  \n",
      "27  \n",
      "## Example: Off-policy Estimation of the value of a _single_ Blackjack State  \n",
      "‚ùê State is player-sum 13, dealer-showing 2, useable ace  \n",
      "‚ùê Target policy is stick only on 20 or 21 ‚ùê Behavior policy is equiprobable  \n",
      "- ‚ùê True value ‚âà ‚àí0.27726  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "4<br>Ordinary<br>Mean importance<br>sampling<br>square<br>2<br>error<br>(average over<br>100 runs)<br>Weighted importance sampling<br>0<br>0 10 100 1000 10,000<br>Episodes (log scale)<br>**----- End of picture text -----**<br>  \n",
      "_‚á°_ **Incremental o‚Üµ-policy every-visit MC policy evaluation (returns** _Q q‚á°_  \n",
      "_‚á°_ Input: an arbitrary target policy _s 2_ S _a 2_ A _s_ Initialize, for all , ( ): _Q_ ( _s, a_ ) arbitrary _C_ 0 ( _s, a_ )  \n",
      "Repeat forever:  \n",
      "_‚á° ¬µ_ any policy with coverage of : Generate an episode using _¬µ S_ 0 _, A_ 0 _, R_ 1 _, . . . , ST ‚àí_ 1 _, AT ‚àí_ 1 _, RT , ST G_ 0 _W_ 1 For _t_ = _T ‚àí_ 1 _, T ‚àí_ 2 _, . . ._ downto 0:  \n",
      "_G Œ≥G_ + _Rt_ +1 _C S C S W_ ( _t, At_ ) ( _t, At_ ) + _W S S Q_ ( _t, At_ ) _Q_ ( _t, At_ ) + _C S_[[] _[G][ ‚àí][Q]_[(] _[S][t][, A][t]_[)]] ( _t,At_ ) _W W[‚á°]_[(] _[A][t][|][S][t]_[)] _A S ¬µ_ ( _t| t_ ) If _W_ = 0 then ExitForLoop  \n",
      "_‚á° ‚á° ‚á° ‚á§_ **O‚Üµ-policy every-visit MC control (returns )**  \n",
      "|Initialize, for all_ s 2_ S,_ a 2_ A(_s_):|||\n",
      "|---|---|---|\n",
      "|_Q_(_s, a_) arbitrary|||\n",
      "|_C_(_s, a_) 0|||\n",
      "|_‚á°_(_s_) argmax_a Q_(_St, a_)<br>(with ties broken consistently)|||\n",
      "|Repeat forever:|||\n",
      "|_¬µ  _any soft policy<br>Generate an episode using_ ¬µ_:<br>_S_0_, A_0_, R_1_, . . . , ST ‚àí_1_, AT ‚àí_1_,_|_ RT , ST_|Target policy is greedy<br>and deterministic|\n",
      "|_G  _0<br>_W  _1<br>For_ t_ =_ T ‚àí_1_, T ‚àí_2_, . . ._ downto 0:||Behavior policy is soft,<br>typicallyùúÄ-greedy|\n",
      "|_G  Œ≥G_ +_ Rt_+1|||\n",
      "|_C_(_St, At_) _C_(_St, At_) +_ W_|||\n",
      "|_Q_(_St, At_) _Q_(_St, At_) +<br>_W_<br>_C_(_St,At_) [_G ‚àíQ_(_St, At_)]|||\n",
      "|_‚á°_(_St_) argmax_a Q_(_St, a_)|(with ties broken consistently)||\n",
      "|If_ At _=_ ‚á°_(_St_) then ExitForLoop|||\n",
      "|_W  W_<br>1<br>_¬µ_(_At|St_)|||  \n",
      "## Discounting-aware Importance Sampling (motivation)  \n",
      "‚ùê So far we have weighted returns without taking into account that they are a discounted sum  \n",
      "‚ùê This can‚Äôt be the best one can do!  \n",
      "‚ùê = 0 For example, suppose ùú∏  \n",
      " Then _G_ 0 will be weighted by _‚á° A S_ 1 1 ( _|_ ) _‚á¢[T]_ 0[=] _[ ‚á°]_[(] _[A]_[0] _[|][S]_[0][)] _A S A S ¬µ_ ( 0 _|_ 0) _¬µ_ ( 1 _|_ 1))  \n",
      "_‚á° A S_ 1 1 ( _|_ ) _A S[¬∑ ¬∑ ¬∑][ ‚á°]_[(] _A[A][T][ ‚àí]_[1] _[|] S[S][T][ ‚àí]_[1][)] _¬µ_ ( 1 _|_ 1)) _¬µ_ ( _T ‚àí_ 1 _| T ‚àí_ 1)  \n",
      " But it really need only be weighted by  \n",
      "> _[‚á°]_[(] _[A]_[0] _[|][S]_[0][)] _‚á¢_[1] 0[=]  \n",
      "_A S ¬µ_ ( 0 _|_ 0)  \n",
      " Which would have much smaller variance  \n",
      "## Discounting-aware Importance Sampling  \n",
      "‚ùê Define the flat partial return:  \n",
      "> _[h]_  \n",
      "> [[+]] _[[ R][t]][[t]]_[[+2]][[+]] _[[ ¬∑ ¬∑ ¬∑]]_[[ +]] _[[ R][h][,]][[h][,]][[,]] t_[,] _[[ R][t]][[t]]_[[+1]]  \n",
      "{\\bar{G}}_{t}^{h}  \n",
      "0 _ t < h   T,_  \n",
      "_t[[ R][t]][[t]]_[[+1]][[+]] _[[ R][t]][[t]]_[[+2]][[+]] _[[ ¬∑ ¬∑ ¬∑]]_[[ +]] _[[ R][h][,]][[h][,]][[,]]_ 0 _ t < h   T,_  \n",
      "‚ùê Then  \n",
      "G_{t}\\stackrel{\\cong}{\\stackrel{\\cong}{\\rightarrow}}\\stackrel{[}{\\sim}\\underbrace{R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdots\\cdots+\\gamma^{T-t-1}R_{T}  \n",
      "OMITTED IMAGE  \n",
      "\\underline{{{\\cal C}}}\\underline{{{\\Sigma}}}\\underline{{{-}}}\\gamma{}\\!\\slash{}\\!\\!\\!\\!J_{\\boldsymbol{t}+1}  \n",
      "\\longrightarrow\\underbrace{\\left(1\\:-\\:\\gamma\\right)M_{\\tilde{t}+1}}_{,\\tilde{v}}  \n",
      "\\mp\\ (\\textsf{i}\\ \\to\\ \\mathcal{V})\\cap\\left(\\frac{}{}\\!{\\cal{P}}_{t+1}\\ 1\\ \\cdots\\ \\mathcal{P}_{t+1}\\emptyset\\right)  \n",
      "\\mp\\ \\left(\\pm\\ \\rightarrow\\gamma\\right)\\gamma\\left({\\cal{H}}_{<}(\\pm\\beta)\\gamma\\left({\\cal{H}}_{+}(\\beta)+\\gamma\\ \\frac{\\lambda(2\\beta)}{\\lambda(\\beta)}\\right)  \n",
      "\\mp\\ \\left(1\\:\\longrightarrow\\gamma\\right)\\gamma^{2}\\left(\\bar{\\cal R}_{t+1}\\:\\ +\\:\\bar{\\cal M}_{t+2}\\:\\mp\\:\\rlap{\\epsilon}\\partial_{t+3}\\right)  \n",
      "...  \n",
      "_‚àí_ + (1  \n",
      "\\left.+\\,\\left(1_{\\,\\tau^{*}\\,\\ \\!\\!\\!\\gamma}^{}\\right)\\gamma^{T-t-2}\\left({\\cal{R}}_{t+1}+{\\cal{R}}_{t+2}+\\cdots+{\\cal{R}}_{T-1}\\right)  \n",
      "\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  \n",
      "\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;  \n",
      "\\begin{array}{r l r l r l}{{\\stackrel{\\circ}{\\sim}}}&{{}}&{{\\stackrel{\\circ}{T-1}}}&{{\\stackrel{\\circ}{\\sim}}}&{{\\stackrel{\\circ}{\\sim}}}&{{\\quad}}&{{}}\\\\ {{\\cdot{\\textbf{t}}}}&{{}}&{{\\sim}}&{{\\textbf{T-t-1}\\bar{C}h}}&{{}}&{{\\cdot}}\\end{array}  \n",
      "\\begin{array}{l l l}{{T\\!\\!\\!\\!\\slash-1}}&{{\\null}}&{{\\null}}\\\\ {{\\downarrow\\downarrow t\\!\\!\\!\\slash=t\\!\\!\\!\\downarrow}}&{{\\sim}}&{{\\gamma\\!\\!\\!\\slash^{\\prime}\\!\\!\\!\\slash}}&{{\\downarrow}}\\\\ {{h\\!\\!\\!\\slash=t\\!\\!\\!\\slash+1}}&{{\\phantom{\\longrightarrow}}}&{{}}\\end{array}  \n",
      "\\begin{array}{c c c}{{\\longrightarrow}}&{{\\displaystyle\\left(\\strut\\bigcap}}&{{\\longrightarrow}}&{{\\prime\\n\\right)}}\\end{array}\\right)}\\,\\!\\geq}\\end{array}  \n",
      "32  \n",
      "32  \n",
      "## Discounting-aware Importance Sampling  \n",
      "‚ùê Define the flat partial return:  \n",
      "> [[[+]]] _[[[ R][t]][[t]]][[[t]]]_[[[+2]]][[[+]]] _[[[ ¬∑ ¬∑ ¬∑]]]_[[[ +]]] _[[[ R][h][,]][[h][,]][[,]]][[[h][,]][[,]]][[[,]]]_ 0 _ t < h    T,_  \n",
      "¬Ø _G[h]_[[[+]]] _[[[ R][t]][[t]]][[[t]]]_[[[+2]]][[[+]]] _[[[ ¬∑ ¬∑ ¬∑]]]_[[[ +]]] _[[[ R][h][,]][[h][,]][[,]]][[[h][,]][[,]]][[[,]]] t_[,] _[[ R][t]][[t]]_[[+1]]  \n",
      "0 _ t < h    T,_  \n",
      "_t[[ R][t]][[t]]_[[+1]][[[+]]] _[[[ R][t]][[t]]][[[t]]]_[[[+2]]][[[+]]] _[[[ ¬∑ ¬∑ ¬∑]]]_[[[ +]]] _[[[ R][h][,]][[h][,]][[,]]][[[h][,]][[,]]][[[,]]]_ 0 _ t < h    T,_  \n",
      "> _[[[[,]]]]_  \n",
      "‚ùê Then  \n",
      "_‚àí T_ 1  \n",
      "_‚àí T_ 1 _Œ≥Rttt_ +2 _Œ≥_ + _[[[h][‚àí]][[‚àí]]][[[‚àí]]] Œ≥[[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[2][1]][[1]]][[[1]]] _R_[[[ ¬Ø]]] _Gttt[[[h]]] t_ +3 ++ +++ ++++ _¬∑ ¬∑ ¬∑Œ≥Œ≥Œ≥_ + _[[[T]]][[[‚àí]]] Œ≥[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]][[[T]]]_[[[1]]] _[[[‚àí]]]_[[[ ¬Ø]]] _G[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]] t_ X = _h t_ +1 _R_  \n",
      "_Gtt_ ,= (1= (1 _Rt ‚àít ‚àí ‚àí_ +1 + + _Œ≥_ ) _Œ≥Rttt_ +2 _Œ≥_ + _[[[h][‚àí]][[‚àí]]][[[‚àí]]] Œ≥[[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[2][1]][[1]]][[[1]]] _R_[[[ ¬Ø]]] _Gttt[[[h]]] t_ +3 ++ +++ ++++ _¬∑ ¬∑ ¬∑Œ≥Œ≥Œ≥_ + _[[[T]]][[[‚àí]]] Œ≥[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]][[[T]]]_[[[1]]] _[[[‚àí]]]_[[[ ¬Ø]]] _G[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]] t_[[1]] _RTT_ X  \n",
      "_Gtt_ ,= (1= (1 _Rt ‚àít ‚àí ‚àí_ +1 + + _Œ≥_ ) _Œ≥Rttt_ +2 _Œ≥_ + _[[[h][‚àí]][[‚àí]]][[[‚àí]]] Œ≥[[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[2][1]][[1]]][[[1]]] _R_[[[ ¬Ø]]] _Gttt[[[h]]] t_ +3 ++ +++ ++++ _¬∑ ¬∑ ¬∑Œ≥Œ≥Œ≥_ + _[[[T]]][[[‚àí]]] Œ≥[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]][[[T]]]_[[[1]]] _[[[‚àí]]]_[[[ ¬Ø]]] _G[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]] t_[[1]] _RTT_  \n",
      "= _h t_ +1 _‚àí R_ = (1 _Œ≥_ ) _t_ +1  \n",
      "= _h t ‚àí R_ = (1 _Œ≥_ ) _t_ +1  \n",
      "_‚àí R R_ + + (1 ) ( )  \n",
      "_‚àí R R_ + (1 _Œ≥_ ) _Œ≥_ ( _t_ +1 + + _t_ +2)  \n",
      "_‚àí Œ≥ Œ≥ t_ +1 + + _t_  \n",
      "_‚àí R_ + (1 _Œ≥_ ) _Œ≥_[[2]] ( _t_ +1 +  \n",
      "_‚àí R R R_ + (1 _Œ≥_ ) _Œ≥_[[2]] ( _t_ +1 + _t_ +2 + + _t_ +3))  \n",
      "_R R_ + _t_ +2 + + _t_ +3))  \n",
      "...  \n",
      "_‚àí_ + (1  \n",
      "+ (1 _‚àí Œ≥_ ) _Œ≥[T][‚àí][t][‚àí]_[2] ( _Rtt_ +1 + + _Rt_ +2 + _¬∑ ¬∑ ¬∑_ + _RT ‚àí_ 1)  \n",
      "> _[[‚àí]]_ ( _Rtt_ +1 + + _Rt ¬∑ ¬∑_ ~~_t_ +1~~ + _R_ ~~_t_ +2~~ +  \n",
      "_¬∑ ¬∑ ¬∑_  \n",
      "+ _Œ≥[[T]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]] ( _R_ ~~_t_ +1~~ + _R_ ~~_t_~~  \n",
      "_¬∑ ¬∑ ¬∑_ + _Œ≥[[T]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]] ( _R_ ~~_t_ +1~~ + _R_ ~~_t_ +2~~ + + _R_ ~~_T_~~ )  \n",
      "+  \n",
      "_‚àí T_ 1 _[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]][ ¬Ø] _[[h]]_  \n",
      "_G[[h]] Œ≥[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]][ ¬Ø] _t_  \n",
      "_G[T] Œ≥[T][‚àí][t][‚àí]_[1][ ¬Ø] _t_  \n",
      "_‚àí_ = (1 _Œ≥_ )  \n",
      "+  \n",
      "_‚àí Œ≥ Œ≥ t Œ≥ t_  \n",
      "= _h t_ +1  \n",
      "~~33~~  \n",
      "~~33~~  \n",
      "Discounting-aware Importance Sampling  \n",
      "‚ùê Define the flat partial return:  \n",
      "> [[[+]]] _[[[ R][t]][[t]]][[[t]]]_[[[+2]]][[[+]]] _[[[ ¬∑ ¬∑ ¬∑]]]_[[[ +]]] _[[[ R][h][,]][[h][,]][[,]]][[[h][,]][[,]]][[[,]]]_ 0 _ t < h    T,_  \n",
      "\\begin{array}{l l}{{\\mathrm{\\l~\\bar{~}~}+2\\bar{~}\\mp\\cdot\\cdot\\cdot\\cdot\\bar{~}\\Gamma~}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\Bigl(\\begin{array}{l l}{{<\\quad\\bar{~}<\\Delta\\leq\\Delta\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\leq\\Lambda\\quad,}}\\\\ {{\\mathrm{\\bar{~}~}+\\Lambda\\quad\\quad\\quad\\quad\\Lambda=t+1\\quad\\Lambda\\quad\\quad\\Lambda\\quad\\Lambda\\quad\\Lambda\\quad\\quad\\quad\\quad\\quad\\quad\\Lambda=t+1\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\Lambda=t+1\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\Lambda=t\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\end{\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\end{\\quad\\quad\\quad\\end{  \n",
      "> _[h]_  \n",
      "> [[[+]]] _[[[ R][t]][[t]]][[[t]]]_[[[+2]]][[[+]]] _[[[ ¬∑ ¬∑ ¬∑]]]_[[[ +]]] _[[[ R][h][,]][[h][,]][[,]]][[[h][,]][[,]]][[[,]]] t_[,] _[[ R][t]][[t]]_[[+1]]  \n",
      "\\widetilde{C_{t}^{Y}}\\,\\triangleq\\,D_{t+1}  \n",
      "0 _ t < h    T,_  \n",
      "_t[[ R][t]][[t]]_[[+1]][[[+]]] _[[[ R][t]][[t]]][[[t]]]_[[[+2]]][[[+]]] _[[[ ¬∑ ¬∑ ¬∑]]]_[[[ +]]] _[[[ R][h][,]][[h][,]][[,]]][[[h][,]][[,]]][[[,]]]_ 0 _ t < h    T,_  \n",
      "> _[[[[,]]]]_  \n",
      "OMITTED MATH EQUATION  \n",
      "‚ùê Then  \n",
      "_‚àí T_ 1  \n",
      "_Gtt Gttt[[[h]]] G[[[T]]][[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]]][[[‚àí][t][‚àí]]]_[[[2][1][ ¬Ø]][[1][ ¬Ø]]][[[1][ ¬Ø]]] _[[[T]]][[[‚àí][t][‚àí][T]]]_[[[1]]] _[[[‚àí]]]_[[[ ¬Ø]]] _[[[t][‚àí]][[‚àí]][[[T]]]][[[‚àí]][[[T]]]]_[[[1]]]  \n",
      "_Gtt_ ,= (1= (1 _Rt ‚àít ‚àí ‚àí ‚àí RG[[[h]]]_ + _¬∑ ¬∑ ¬∑Œ≥Œ≥_ + _G[[[T]]] RTT_  \n",
      "_Gtt_ ,= (1= (1 _Rt ‚àít ‚àí ‚àí_ +1 + + _Œ≥_ ) _Œ≥Rtt_ +2 _Œ≥_ + _[[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]]][[[‚àí][t][‚àí]]] Œ≥[[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[2][1][ ¬Ø]][[1][ ¬Ø]]][[[1][ ¬Ø]]] _RG_[[[ ¬Ø]]] _Gttt[[[h]]] t_ +3 ++ +++ _¬∑ ¬∑ ¬∑Œ≥Œ≥_ + _[[[T]]][[[‚àí][t][‚àí][T]]] Œ≥[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]][[[T]]]_[[[1]]] _[[[‚àí]]]_[[[ ¬Ø]]] _G[[[t][‚àí]][[‚àí]][[[T]]]][[[‚àí]][[[T]]]] t_[[[1]]] _RTT_ X  \n",
      "_Gtt_ ,= (1= (1 _Rt ‚àít ‚àí ‚àí_ +1 + + _Œ≥_ ) _Œ≥Rtt_ +2 _Œ≥_ + _[[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]]][[[‚àí][t][‚àí]]] Œ≥[[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[2][1][ ¬Ø]][[1][ ¬Ø]]][[[1][ ¬Ø]]] _RG_[[[ ¬Ø]]] _Gttt_ +3 ++ +++ _¬∑ ¬∑ ¬∑Œ≥Œ≥_ + _[[[T]]][[[‚àí][t][‚àí][T]]] Œ≥[[[t][‚àí][T]][[‚àí][T]][[T]]][[[‚àí][T]][[T]]][[[T]]]_[[[1]]] _[[[‚àí]]]_[[[ ¬Ø]]] _G[[[t][‚àí]][[‚àí]][[[T]]]][[[‚àí]][[[T]]]] t_[[[1]]] _RTT_  \n",
      "\\begin{array}{c c c}{{\\displaystyle{\\nabla^{*}~{\\cal L}}}}&{{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}}\\\\ {{}}&{{}}&{{h\\underline{{{\\longrightarrow}}}t}}\\end{array}  \n",
      "OMITTED IMAGE  \n",
      "_t_ +1  \n",
      "_t_ +1  \n",
      "= _h t_ +1 _‚àí R_ = (1 _Œ≥_ ) _t_ +1  \n",
      "= _h t ‚àí R_ = (1 _Œ≥_ ) _t_ +1  \n",
      "_‚àí R R_ + + (1 ) ( )  \n",
      "_‚àí R R_ + (1 _Œ≥_ ) _Œ≥_ ( _t_ +1 + + + _t_ +2)  \n",
      "_‚àí R R_ ‚ùê Ordinary discounting-aware IS:+ (1 _Œ≥_ ) _Œ≥_ ( _t_ +1 + + +  \n",
      "_‚àí Œ≥ Œ≥ t_ +1 + + + _t_  \n",
      "_‚àí R_ + (1 _Œ≥_ ) _Œ≥_[[2]] ( _t_ +1 +  \n",
      "_‚àí R R R_ + (1 _Œ≥_ ) _Œ≥_[[2]] ( _t_ +1 + _t_ +2 + + _t_ +3)) _[[[T]]]_[[[(]]] _[[[t]]]_[[[)]]] _[[[[‚àí]]]]_[[[1]]] _[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]] _[[h]][G]_[[¬Ø]] _[[h]]_  \n",
      "_R R_ + _t_ +2 + + _t_ +3)) ¬Ø  \n",
      "_‚àí Œ≥ Œ≥ t ‚àí_[[[P]]] _[[[T]]]_[[[(]]] _[[[t]]]_[[[)]]] _[[[[‚àí]]]]_[[[1]]] (1 _Œ≥_ ) = _h t_ +1  \n",
      "OMITTED IMAGE  \n",
      "_‚àí Œ≥ Œ≥ t ‚àí_[[[P]]] _[[[T]]]_[[[(]]] _[[[t]]]_[[[)]]] _[[[[‚àí]]]]_[[[1]]] (1 _Œ≥_ ) = _h t_ +1  \n",
      "\\begin{array}{c}{{\\left.\\frac{\\gamma\\right|\\stackrel{\\underset{\\mathrm{def}}{}},\\mathrm{d}\\cdots\\mathrm{}\\left|\\stackrel{\\land}{\\operatorname{d}}\\right|\\overset{\\land}{\\operatorname{d}}\\left|\\partial_{t}^{\\prime}\\right|}+\\gamma^{\\left|\\left|\\gamma\\right|\\neq\\frac{1}{\\hbar}}\\right|\\stackrel{\\circ}{\\operatorname{d}}_{\\mu}^{\\prime}\\right|}}{\\left|\\langle\\mathrm{j}\\rangle\\right|^{\\iint_{1}\\mathrm{}\\to\\mathrm{}\\leq\\psi}\\rangle\\mathrm{}\\leq\\mathrm{}\\psi}}}\\\\ {{\\left|\\cdots\\rangle\\left|\\mathrm{}\\right|\\mathrm{}\\right|^{\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}\\frac{1}{\\hbar}}\\mathrm{}\\scriptsize\\in\\mathrm{}\\mathrm{}\\leq}}}\\\\ {{\\left|\\mathrm{}\\leq\\mathrm{}\\mathrm{}\\mathrm{}\\rangle\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{}}\\mathrm{}\\mathrm{}\\mathrm{}\\mathrm{{}\\prime}\\leq\\mathrm{{{\\mathrm{\\mathrm{}}\\mathrm{}\\mathrm{}\\leq\\frac{i}\\mathrm{{}\\mathrm{f}\\mathrm{{f}\\mathrm{}\\leq\\prime}\\mathrm{{{}\\mathrm{}\\frac{\\mathrm{}\\mathrm{\\frac{f}\\leq\\leq}\\mathrm{\\mathrm{}\\mathrm{{{\\leq}\\mathrm{\\frac{f}\\leq}\\mathrm{\\mathrm{{f}\\leq\\leq}\\leq\\leq\\leq\\leq\\  \n",
      "> _[‚àí][‚àí] ‚àí_[[[P]]] _[[[[‚àí]]]]_ + _G_ P _t22_ T . _s_ (1 _Œ≥_ ) _h_ = _t[Œ≥][[h][‚àí][t][‚àí]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]] _[[‚á¢]] t[[h]][G]_[[¬Ø]] _[[h]] t Œ≥ ‚á¢t t_  \n",
      "_‚àí_ + _t22_ T . _s Œ≥ h_ = _t_ +1 _[[[‚àí]]][[‚á¢]] t[[G]] t_ ( ) . _s_ , ~~.~~ ( )  \n",
      "\\V\\left(s\\right)~\\underline{{{\\triangle}}}~\\underline{{{\\sum}}}\\underline{{{s\\in J\\left(s\\right)}}}  \n",
      "OMITTED MATH EQUATION  \n",
      "+ (1 _‚àí Œ≥_ ) _Œ≥[T][‚àí][t][‚àí]_[2] ( _Rt_ +1 + _Rt_ +2 + _¬∑ ¬∑ ¬∑_ + _RT ‚àí_ 1)  \n",
      "- ‚ùê aware IS: _¬∑ ¬∑ ¬∑_ Weighted disco+ _Œ≥_ unting _[[T]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]] ( _R_ ~~_t_ +1~~ + _R_ ~~_t_ +2~~ + + _R_ ~~_T_~~ )  \n",
      "- ‚ùê aware IS:  \n",
      "- aware IS: _¬∑ ¬∑ ¬∑_ + _Œ≥_ unting _[[T]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]_[[1]] ( _R_ ~~_t_ +1~~ + _R_ ~~_t_ +2~~ + + _R_ ~~_T_~~ )  \n",
      "+  \n",
      "\\mathbf{\\hat{e}}  \n",
      "OMITTED IMAGE  \n",
      "_‚àí T_ 1 _‚àí_[[[P]]] _[[[T]]]_[[[(]]] _[[[t]]]_[[[)]]] _[[[‚àí]]]_[[[1]]] P _t2_ T _s_ (1 _Œ≥_ ) _h_ = _t[Œ≥][h][‚àí][t][‚àí]_[1] _[‚á¢] t[h][[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]][[[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]][[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[1]]] _[h]_  \n",
      "_‚àí T_ 1 _‚àí_[[[P]]] _[[[T]]]_[[[(]]] _[[[t]]]_[[[)]]] _[[[‚àí]]]_[[[1]]] (1 _Œ≥_ ) = _h t_ +1 _Œ≥_ ) _Œ≥[[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]][[[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]][[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[1]]] X  \n",
      "_‚àí T_ 1 _‚àí_[[[P]]] _[[[T]]]_[[[(]]] _[[[t]]]_[[[)]]] _[[[‚àí]]]_[[[1]]] (1 _Œ≥_ ) = _h t_ +1 _Œ≥_ ) _Œ≥[[[h][‚àí][t][‚àí]][[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]][[[‚àí][t][‚àí]][[t][‚àí]][[‚àí]]][[[t][‚àí]][[‚àí]]][[[‚àí]]]_[[[1]]] X  \n",
      "\\frac{\\sum\\d g(\\varphi(g)}{\\sum\\sum}\\displaystyle\\frac{\\sum\\d g(\\varphi)}{\\bf V}  \n",
      "\\frac{\\in\\Im\\left(\\,S\\right)\\;\\setminus\\;\\bigcup^{\\infty}\\;}{\\bigcup_{\\mathbb{C}\\setminus\\setminus}}  \n",
      "_V_ ( _s_ ) , ,  \n",
      "_V_ ( _s_ ) , ,  \n",
      "_‚àí Œ≥ Œ≥_ ~~_t_~~ _Œ≥_ ~~_t_~~ _s_ , ) , ,  \n",
      "~~_t_~~ = _t_ +1 _‚àí_ ~~[[P]]~~ _[[T]]_[[(]] _[[t]]_[[)]] _[[‚àí]]_[[1]] (1 _Œ≥_ ) = _h t_ +1  \n",
      "~~_t_~~ = _t_ +1 _‚àí_ ~~[[P]]~~ _[[T]]_[[(]] _[[t]]_[[)]] _[[‚àí]]_[[1]] (1 _Œ≥_ ) = _h t_ +1  \n",
      "t{\\underline{{\\subset}}}\\cap\\left(S\\right)  \n",
      "= _h t_ +1  \n",
      "~~34~~  \n",
      "~~34~~  \n",
      "~~34~~  \n",
      "## Per-reward Importance Sampling  \n",
      "‚ùê = 1 Another way of reducing variance, even if ùú∏ ‚ùê Uses the fact that the return is a _sum of rewards_  \n",
      "_‚á¢[T] t[G][t]_[=] _[ ‚á¢][T] t[R][t]_[+1][+] _[ Œ≥‚á¢][T] t[R][t]_[+2][+] _[ ¬∑ ¬∑ ¬∑]_[ +] _[ Œ≥][k][‚àí]_[1] _[‚á¢][T] t[R][t]_[+] _[k]_[+] _[ ¬∑ ¬∑ ¬∑]_[ +] _[ Œ≥][T][ ‚àí][t][‚àí]_[1] _[‚á¢][T] t[R][T]_  \n",
      "Per-reward Importance Sampling ‚ùê = 1 Another way of reducing variance, even if ùú∏ ‚ùê Uses the fact that the return is a _sum of rewards_  \n",
      "> _[[T]] t[R][t]_[+1][+] _[ Œ≥‚á¢][T] t[R][t]_[+2][+] _[ ¬∑ ¬∑ ¬∑]_[ +] _[ Œ≥][k][‚àí]_[1] _[‚á¢][T] t_  \n",
      "> _[[T]]_[+] _[ ¬∑ ¬∑ ¬∑]_[ +] _[ Œ≥][T][ ‚àí][t][‚àí]_[1] _[‚á¢][T] t[R][t]_[+] _[k] t_  \n",
      "_‚á¢[T] t[G][t]_[=] _[ ‚á¢][[T]] t_  \n",
      "> _[[T]] t[R][T]_  \n",
      "‚ùê where  \n",
      "> _[‚á°]_[(] _[A][t][|][S][t]_[)] _‚á¢[T] t[R][t]_[+] _[k]_[=]  \n",
      "_‚á° A S_ ( _t_ +1 _| t_ +1) _A S[¬∑ ¬∑ ¬∑][ ‚á°]_[[(]] _A[[A][t]][[t]]_[[+]] _[[k][|]][[|]] S[[S][t]][[t]]_[[+]] _[[k]]_[[)]] _¬µ_ ( _t_ +1 _| t_ +1) _¬µ_ ( _t_ + _k|| t_ + _k_ )  \n",
      "> [[(]] _A[[A][t]][[t]]_[[+]] _[[k][|]][[|]] S[[S][t]][[t]]_[[+]] _[[k]]_[[)]] _[¬∑ ¬∑ ¬∑][‚á°]_[(] _A[A][T][ ‚àí]_[1] _[|] S[S][T][ ‚àí]_[1][)] _¬µ_ ( _t_ + _k|| t_ + _k_ ) _¬µ_ ( _T ‚àí ‚àí_ 1 _| T ‚àí ‚àí_ 1))  \n",
      "> _[[ ‚àí]][[ ‚àí]] A S ¬µ_ ( _T ‚àí ‚àí_ 1 _| T ‚àí ‚àí_ 1)) _[R][t]_[+] _[k]_  \n",
      "_A S ¬µ_ ( _t| t_ )  \n",
      "= ‚à¥E E _‚á¢[T] t[R][t]_[+] _[k]_ ‚á• ‚á§  \n",
      "_‚á¢[t] t_[+] _[k]_  \n",
      "_R t_ + _k_  \n",
      "## Per-reward Importance Sampling  \n",
      "## ‚ùê = 1 Another way of reducing variance, even if ùú∏ ‚ùê Uses the fact that the return is a _sum of rewards_  \n",
      "\\begin{array}{c}{{\\rho_{t}^{T}G_{t}=\\rho_{t}^{T}R_{t+1}+\\gamma\\rho_{t}^{T}R_{t+2}+\\cdots+\\gamma^{k-1}\\rho_{t}^{T}R_{t+2}+\\cdots+\\gamma^{T-t-1}\\rho_{t}^{T}R_{T}}}\\\\ {{\\mathrm{(\\bf{\\bf{\\tilde{t}}}}\\Re_{t+k}=\\frac{\\pi(A_{t}|S_{t})\\pi\\bigl(A_{t+1}|S_{t+k}\\bigr)-\\cdots}{\\mu(A_{t+k}|S_{t+k})}\\cdots\\frac{\\pi(A_{t-1}|S_{T-1})}{\\mu(A_{t+1}|S_{t+k})}R_{t+k}}}\\end{array}  \n",
      "**----- Start of picture text -----**<br>\n",
      "‚á¢ [T] t [G][t] [=] [ ‚á¢][T] t [R][t] [+1] [+] [ Œ≥‚á¢][T] t [R][t] [+2] [+] [ ¬∑ ¬∑ ¬∑] [ +] [ Œ≥][k][‚àí] [1] [‚á¢][T] t [R][t] [+] [k] [+] [ ¬∑ ¬∑ ¬∑] [ +] [ Œ≥][T][ ‚àí][t][‚àí] [1] [‚á¢][T] t [R][T]<br>‚ùê where<br>‚á° A S<br>( t +1 | t +1)<br>[‚á°] [(] [A][t][|][S][t] [)] [‚á°] [(] [A][T][ ‚àí] [1] [|][S][T][ ‚àí] [1][)]<br>‚á¢ [T] t [R][t] [+] [k] [=]<br>A S A S [¬∑ ¬∑ ¬∑][ ‚á°] [(] A [A][t] [+] [k][|] S [S][t] [+] [k] [)] [¬∑ ¬∑ ¬∑] A S<br>¬µ ( t| t ) ¬µ ( t +1 | t +1) ¬µ ( t + k| t + k ) ¬µ ( T ‚àí 1 | T ‚àí 1) [R][t] [+] [k]<br>**----- End of picture text -----**<br>  \n",
      "\\begin{array}{l}{{\\mathrm{:{\\Xi}\\left[\\rho_{t}^{T}R_{t+k}\\right]=\\mathbb{E}\\left[\\rho_{t}^{t+k}R_{t+1}+\\gamma\\rho_{t}^{t+2}R_{t+3}+\\cdots+\\gamma^{T-t-1}\\rho_{t}^{T}R_{T}}}\\\\ {{\\mathrm{:{\\bf{\\Xi}}\\left[\\rho_{t}^{T}G_{t}\\right]=\\mathbb{E}\\left[\\rho_{t}^{t+1}R_{t+1}+\\gamma\\rho_{t}^{t+2}R_{t+3}+\\cdots+\\gamma^{T-t-1}\\rho_{t}^{T}R_{T}\\right]}}\\\\ {{\\mathrm{{\\cal{T}}\\left[\\rho_{t}^{T}G_{t}\\right]\\Delta}}\\end{array}  \n",
      "**----- Start of picture text -----**<br>\n",
      "=<br>‚à¥E  E R<br>‚á¢ [T] t [R][t] [+] [k] ‚á¢ [t] t [+] [k] t + k<br>‚á• ‚á§ ‚á• ‚á§<br>=<br>‚à¥ E ‚á¢ [T] t [G][t]  E ‚á¢ [t] t [+1] Rt +1 +  Œ≥‚á¢ [t] t [+2] Rt +2 +  Œ≥ [2] ‚á¢ [t] t [+3] Rt +3 +  ¬∑ ¬∑ ¬∑  +  Œ≥ [T][ ‚àí][t][‚àí] [1] ‚á¢ [T] t [R][T]<br>‚á• ‚á§ ‚á• ‚á§<br>| }<br>{ z<br>Àú<br>G<br>t<br>‚ùê Per-reward ordinary IS:<br>P t2 T s [G] [Àú] [t]<br>( )<br>V  ( s ) ,<br>T s<br>| ( ) |<br>**----- End of picture text -----**<br>  \n",
      "## Summary  \n",
      "‚ùê MC has several advantages over DP:  \n",
      "- Can learn directly from interaction with environment  \n",
      "- No need for full models  \n",
      " Less harmed by violating Markov property (later in book)  \n",
      "‚ùê MC methods provide an alternate policy evaluation process ‚ùê One issue to watch for: maintaining sufficient exploration  \n",
      "- exploring starts, soft policies  \n",
      "- ‚ùê Introduced distinction between _on-policy_ and _off-policy_ methods ‚ùê Introduced _importance sampling_ for off-policy learning ‚ùê Introduced distinction between _ordinary_ and _weighted_ IS  \n",
      "## Summary  \n",
      "‚ùê MC has several advantages over DP:  \n",
      " Can learn directly from interaction with environment  \n",
      "- No need for full models  \n",
      " Less harmed by violating Markov property (later in book)  \n",
      "‚ùê MC methods provide an alternate policy evaluation process ‚ùê One issue to watch for: maintaining sufficient exploration  \n",
      "- exploring starts, soft policies  \n",
      "‚ùê Introduced distinction between _on-policy_ and _off-policy_ methods ‚ùê Introduced _importance sampling_ for off-policy learning  \n",
      "- ‚ùê Introduced distinction between _ordinary_ and _weighted_ IS ‚ùê Introduced two _return-specific_ ideas for reducing IS variance  \n",
      "- _discounting-aware_ and _per-reward_ IS  \n",
      "R. S. Sutton and A. G. Barto: Reinforcement Learning: An Introduction  \n",
      "## Paths to a policy  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Model<br>Direct<br>Environmental<br>Experience planning<br>interaction<br>Direct RL<br>methods Value<br>function<br>Greedification<br>Policy<br>**----- End of picture text -----**<br>  \n",
      "## Paths to a policy  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Model<br>Direct<br>Environmental Simulation<br>Experience planning<br>interaction<br>Direct RL<br>methods Value<br>function<br>Greedification<br>Simulation-based RL<br>Policy<br>**----- End of picture text -----**<br>  \n",
      "## Paths to a policy  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Model<br>Model<br>learning<br>Direct<br>Environmental Simulation<br>Experience planning<br>interaction<br>Direct RL<br>methods Value<br>function<br>Greedification<br>Policy<br>**----- End of picture text -----**<br>  \n",
      "## Conventional Model-based RL  \n",
      "## Paths to a policy  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Model<br>Model<br>learning<br>Direct<br>Environmental Simulation<br>Experience planning<br>interaction<br>Direct RL<br>methods Value<br>function<br>Greedification<br>Policy<br>**----- End of picture text -----**<br>  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Dyna<br>Model-based RL<br>**----- End of picture text -----**<br>\n",
      "\n",
      "Source: {'header': '', 'chunk_number': 0, 'course': 'AI_ALGORITHMS', 'source': 'data/AI_ALGORITHMS/sbchap5.pdf'}\n",
      "Content: ## **Chapter 5**  \n",
      "## **Monte Carlo Methods**  \n",
      "In this chapter we consider our first learning methods for estimating value functions and discovering optimal policies. Unlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only _experience_ ‚Äîsample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Learning from _actual_ experience is striking because it requires no prior knowledge of the environment‚Äôs dynamics, yet can still attain optimal behavior. Learning from _simulated_ experience is also powerful. Although a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP). In surprisingly many cases it is easy to generate experience sampled according to the desired probability distributions, but infeasible to obtain the distributions in explicit form.  \n",
      "Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected. Only on the completion of an episode are value estimates and policies changed. Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step (online) sense. The term ‚ÄúMonte Carlo‚Äù is often used more broadly for any estimation method whose operation involves a significant random component. Here we use it specifically for methods based on averaging complete returns (as opposed to methods that learn from partial returns, considered in the next chapter).  \n",
      "Monte Carlo methods sample and average _returns_ for each state‚Äìaction pair much like the bandit methods we explored in Chapter 2 sample and average  \n",
      "_rewards_ for each action. The main di‚Üµerence is that now there are multiple states, each acting like a di‚Üµerent bandit problem (like an associative-search or contextual bandit) and that the di‚Üµerent bandit problems are interrelated. That is, the return after taking an action in one state depends on the actions taken in later states in the same episode. Because all the action selections are undergoing learning, the problem becomes nonstationary from the point of view of the earlier state.  \n",
      "To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in Chapter 4 for DP. Whereas there we _computed_ value functions from knowledge of the MDP, here we _learn_ value functions from sample returns with the MDP. The value functions and corresponding policies still interact to attain optimality in essentially the same way (GPI). As in the DP chapter, first we consider the prediction problem (the computation of _v‚á°_ and _q‚á°_ for a fixed arbitrary policy _‚á°_ ) then policy improvement, and, finally, the control problem and its solution by GPI. Each of these ideas taken from DP is extended to the Monte Carlo case in which only sample experience is available.  \n",
      "## **5.1 Monte Carlo Prediction**  \n",
      "We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return‚Äîexpected cumulative future discounted reward‚Äîstarting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods.  \n",
      "In particular, suppose we wish to estimate _v‚á°_ ( _s_ ), the value of a state _s_ under policy _‚á°_ , given a set of episodes obtained by following _‚á°_ and passing through _s_ . Each occurrence of state _s_ in an episode is called a _visit_ to _s_ . Of course, _s_ may be visited multiple times in the same episode; let us call the first time it is visited in an episode the _first visit_ to _s_ . The _first-visit MC method_ estimates _v‚á°_ ( _s_ ) as the average of the returns following first visits to _s_ , whereas the _every-visit MC method_ averages the returns following all visits to _s_ . These two Monte Carlo (MC) methods are very similar but have slightly di‚Üµerent theoretical properties. First-visit MC has been most widely studied, dating back to the 1940s, and is the one we focus on in this chapter. Every-visit MC extends more naturally to function approximation and eligibility traces, as discussed in Chapters 9 and 7. First-visit MC is shown in procedural form in Figure 5.1.  \n",
      "Initialize: _‚á°_ policy to be evaluated _V_ an arbitrary state-value function _Returns_ ( _s_ ) an empty list, for all _s 2_ S Repeat forever: Generate an episode using _‚á°_ For each state _s_ appearing in the episode: _G_ return following the first occurrence of _s_ Append _G_ to _Returns_ ( _s_ ) _V_ ( _s_ ) average( _Returns_ ( _s_ ))  \n",
      "Figure 5.1: The first-visit MC method for estimating _v‚á°_ . Note that we use a capital letter _V_ for the approximate value function because, after initialization, it soon becomes a random variable.  \n",
      "Both first-visit MC and every-visit MC converge to _v‚á°_ ( _s_ ) as the number of visits (or first visits) to _s_ goes to infinity. This is easy to see for the case of first-visit MC. In this case each return is an independent, identically distributed estimate of _v‚á°_ ( _s_ ) with finite variance. By the law of large numbers the sequence of averages of these estimates converges to their expected value. Each average is itself an unbiased estimate, and the standard deviation of its error falls as 1 _/[p]_ ~~_n_ ,~~ where _n_ is the number of returns averaged. Every-visit MC is less straightforward, but its estimates also converge asymptotically to _v‚á°_ ( _s_ ) (Singh and Sutton, 1996).  \n",
      "The use of Monte Carlo methods is best illustrated through an example.  \n",
      "**Example 5.1: Blackjack** The object of the popular casino card game of _blackjack_ is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer‚Äôs cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a _natural_ . He then wins unless the dealer also has a natural, in which case the game is a draw. If the player does not have a natural, then he can request additional cards, one by one ( _hits_ ), until he either stops ( _sticks_ ) or exceeds 21 ( _goes bust_ ). If he goes bust, he loses; if he sticks, then it becomes the dealer‚Äôs turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome‚Äîwin, lose, or draw‚Äîis determined by whose final sum is closer to 21.  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "After 10,000 episodes After 500,000 episodes<br>Usable +1<br>ace<br>!1<br>No<br>usable<br>ace<br>Dealer showing<br>A<br>10<br>12<br>Player sum<br>21<br>**----- End of picture text -----**<br>  \n",
      "Figure 5.2: Approximate state-value functions for the blackjack policy that sticks only on 20 or 21, computed by Monte Carlo policy evaluation.  \n",
      "Playing blackjack is naturally formulated as an episodic finite MDP. Each game of blackjack is an episode. Rewards of +1, _‚àí_ 1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount ( _Œ≥_ = 1); therefore these terminal rewards are also the returns. The player‚Äôs actions are to hit or to stick. The states depend on the player‚Äôs cards and the dealer‚Äôs showing card. We assume that cards are dealt from an infinite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt. If the player holds an ace that he could count as 11 without going bust, then the ace is said to be _usable_ . In this case it is always counted as 11 because counting it as 1 would make the sum 11 or less, in which case there is no decision to be made because, obviously, the player should always hit. Thus, the player makes decisions on the basis of three variables: his current sum (12‚Äì21), the dealer‚Äôs one showing card (ace‚Äì10), and whether or not he holds a usable ace. This makes for a total of 200 states.  \n",
      "Consider the policy that sticks if the player‚Äôs sum is 20 or 21, and otherwise hits. To find the state-value function for this policy by a Monte Carlo approach, one simulates many blackjack games using the policy and averages the returns following each state. Note that in this task the same state never recurs within one episode, so there is no di‚Üµerence between first-visit and every-visit MC methods. In this way, we obtained the estimates of the statevalue function shown in Figure 5.2. The estimates for states with a usable ace are less certain and less regular because these states are less common. In any event, after 500,000 games the value function is very well approximated.  \n",
      "## _5.1. MONTE CARLO PREDICTION_  \n",
      "Although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events‚Äîin particular, they require the quantities _p_ ( _s[0] , r|s, a_ )‚Äîand it is not easy to determine these for blackjack. For example, suppose the player‚Äôs sum is 14 and he chooses to stick. What is his expected reward as a function of the dealer‚Äôs showing card? All of these expected rewards and transition probabilities must be computed _before_ DP can be applied, and such computations are often complex and error-prone. In contrast, generating the sample games required by Monte Carlo methods is easy. This is the case surprisingly often; the ability of Monte Carlo methods to work with sample episodes alone can be a significant advantage even when one has complete knowledge of the environment‚Äôs dynamics.  \n",
      "Can we generalize the idea of backup diagrams to Monte Carlo algorithms? The general idea of a backup diagram is to show at the top the root node to be updated and to show below all the transitions and leaf nodes whose rewards and estimated values contribute to the update. For Monte Carlo estimation of _v‚á°_ , the root is a state node, and below it is the entire trajectory of transitions along a particular single episode, ending at the terminal state, as in Figure 5.3. Whereas the DP diagram (Figure 3.4a) shows all possible transitions, the Monte Carlo diagram shows only those sampled on the one episode. Whereas the DP diagram includes only one-step transitions, the Monte Carlo diagram goes all the way to the end of the episode. These di‚Üµerences in the diagrams accurately reflect the fundamental di‚Üµerences between the algorithms.  \n",
      "An important fact about Monte Carlo methods is that the estimates for each state are independent. The estimate for one state does not build upon the estimate of any other state, as is the case in DP. In other words, Monte Carlo methods do not _bootstrap_ as we defined it in the previous chapter.  \n",
      "In particular, note that the computational expense of estimating the value of a single state is independent of the number of states. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states. One can generate many sample episodes starting from the states of interest, averaging returns from only these states ignoring all others. This is a third advantage Monte Carlo methods can have over DP methods (after the ability to learn from actual experience and from simulated experience).  \n",
      "terminal state  \n",
      "Figure 5.3: The backup diagram for Monte Carlo estimation of _v‚á°_ .  \n",
      "## **Example 5.2: Soap Bubble**  \n",
      "Suppose a wire frame forming a closed loop is dunked in soapy water to form a soap surface or bubble conforming at its edges to the wire frame. If the geometry of the wire frame is irregular but known, how can you compute the shape of the surface? The shape has the property that the total force on each point exerted by neighborA bubble on a wire loop ing points is zero (or else the shape would change). This means that the surface‚Äôs height at any point is the average of its heights at points in a small circle around that point. In addition, the surface must meet at its boundaries with the wire frame. The usual approach to problems of this kind is to put a grid over the area covered by the surface and solve for its height at the grid points by an iterative computation. Grid points at the boundary are forced to the wire frame, and all others are adjusted toward the average of the heights of their four nearest neighbors. This process then iterates, much like DP‚Äôs iterative policy evaluation, and ultimately converges to a close approximation to the desired surface.  \n",
      "This is similar to the kind of problem for which Monte Carlo methods were originally designed. Instead of the iterative computation described above, imagine standing on the surface and taking a random walk, stepping randomly from grid point to neighboring grid point, with equal probability, until you  \n",
      "reach the boundary. It turns out that the expected value of the height at the boundary is a close approximation to the height of the desired surface at the starting point (in fact, it is exactly the value computed by the iterative method described above). Thus, one can closely approximate the height of the surface at a point by simply averaging the boundary heights of many walks started at the point. If one is interested in only the value at one point, or any fixed small set of points, then this Monte Carlo method can be far more efficient than the iterative method based on local consistency.  \n",
      "## **5.2 Monte Carlo Estimation of Action Values**  \n",
      "If a model is not available, then it is particularly useful to estimate _action_ values (the values of state‚Äìaction pairs) rather than _state_ values. With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP. Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for Monte Carlo methods is to estimate _q‚á§_ . To achieve this, we first consider the policy evaluation problem for action values.  \n",
      "The policy evaluation problem for action values is to estimate _q‚á°_ ( _s, a_ ), the expected return when starting in state _s_ , taking action _a_ , and thereafter following policy _‚á°_ . The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state‚Äì action pair rather than to a state. A state‚Äìaction pair _s, a_ is said to be visited in an episode if ever the state _s_ is visited and action _a_ is taken in it. The everyvisit MC method estimates the value of a state‚Äìaction pair as the average of the returns that have followed visits all the visits to it. The first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected. These methods converge quadratically, as before, to the true expected values as the number of visits to each state‚Äìaction pair approaches infinity.  \n",
      "The only complication is that many state‚Äìaction pairs may never be visited. If _‚á°_ is a deterministic policy, then in following _‚á°_ one will observe returns only for one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. This is a serious problem because the purpose of learning action values is to help in choosing among the actions available in each state. To compare alternatives we need to estimate the value of _all_ the actions from each state, not just the one we currently favor.  \n",
      "This is the general problem of _maintaining exploration_ , as discussed in the context of the _n_ -armed bandit problem in Chapter 2. For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes _start in a state‚Äìaction pair_ , and that every pair has a nonzero probability of being selected as the start. This guarantees that all state‚Äìaction pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of _exploring starts_ .  \n",
      "The assumption of exploring starts is sometimes useful, but of course it cannot be relied upon in general, particularly when learning directly from actual interaction with an environment. In that case the starting conditions are unlikely to be so helpful. The most common alternative approach to assuring that all state‚Äìaction pairs are encountered is to consider only policies that are stochastic with a nonzero probability of selecting all actions in each state. We discuss two important variants of this approach in later sections. For now, we retain the assumption of exploring starts and complete the presentation of a full Monte Carlo control method.  \n",
      "## **5.3 Monte Carlo Control**  \n",
      "We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. The overall idea is to proceed according to the same pattern as in the DP chapter, that is, according to the idea of generalized policy iteration (GPI). In GPI one maintains both an approximate policy and an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function:  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "evaluation<br>q ‚Üí  q œÄ<br>œÄ q<br>œÄ ‚Üígreedy( q )<br>improvement<br>**----- End of picture text -----**<br>  \n",
      "These two kinds of changes work against each other to some extent, as each creates a moving target for the other, but together they cause both policy and  \n",
      "value function to approach optimality.  \n",
      "To begin, let us consider a Monte Carlo version of classical policy iteration. In this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with an arbitrary policy _‚á°_ 0 and ending with the optimal policy and optimal action-value function:  \n",
      "\\pi_{0}\\stackrel{\\mathrm{E}}{\\longrightarrow}q_{\\pi_{0}}\\stackrel{1}{\\longrightarrow}\\pi_{1}\\stackrel{\\mathrm{E}}{\\longrightarrow}q_{\\pi_{1}}\\stackrel{1}{\\longrightarrow}\\pi_{2}\\stackrel{\\mathrm{E}}{\\longrightarrow}\\cdots\\stackrel{1}{\\longrightarrow}\\pi_{*}\\stackrel{\\mathrm{E}}{\\longrightarrow}\\stackrel{\\mathrm{E}}{\\longrightarrow}\\sum\\alpha_{*}\\stackrel{\\mathrm{E}}{\\longrightarrow}\\bar{q}_{\\star}\\stackrel{\\mathrm{E}}{\\longrightarrow}q_{*},  \n",
      "where _‚àí!_ E denotes a complete policy evaluation and _‚àí!_ I denotes a complete policy improvement. Policy evaluation is done exactly as described in the preceding section. Many episodes are experienced, with the approximate actionvalue function approaching the true function asymptotically. For the moment, let us assume that we do indeed observe an infinite number of episodes and that, in addition, the episodes are generated with exploring starts. Under these assumptions, the Monte Carlo methods will compute each _q‚á°k_ exactly, for arbitrary _‚á°k_ .  \n",
      "Policy improvement is done by making the policy greedy with respect to the current value function. In this case we have an _action_ -value function, and therefore no model is needed to construct the greedy policy. For any actionvalue function _q_ , the corresponding greedy policy is the one that, for each _s 2_ S, deterministically chooses an action with maximal action-value:  \n",
      "\\pi(s)=\\arg\\operatorname*{max}_{a}{q(s,a)}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(5.1)  \n",
      "Policy improvement then can be done by constructing each _‚á°k_ +1 as the greedy policy with respect to _q‚á°k_ . The policy improvement theorem (Section 4.2) then applies to _‚á°k_ and _‚á°k_ +1 because, for all _s 2_ S,  \n",
      "\\begin{array}{l l l}{{q_{\\pi_{k}}(s,\\pi_{k+1}(s))}}&{{=}}&{{q_{\\pi_{k}}(s,\\arg\\operatorname{arg\\,max}\\,q_{\\pi_{k}}(s,a))}}\\\\ {{}}&{{}}&{{=}}&{{\\operatorname*{max}\\,q_{\\pi_{k}}(s,a)}}\\\\ {{}}&{{}}&{{\\geq}}&{{\\eta_{\\pi_{k}}(s,\\pi_{k}(s))}}\\\\ {{}}&{{}}&{{=}}&{{v_{\\pi_{k}}(s).}}\\end{array}  \n",
      "As we discussed in the previous chapter, the theorem assures us that each _‚á°k_ +1 is uniformly better than _‚á°k_ , or just as good as _‚á°k_ , in which case they are both optimal policies. This in turn assures us that the overall process converges to the optimal policy and optimal value function. In this way Monte Carlo methods can be used to find optimal policies given only sample episodes and no other knowledge of the environment‚Äôs dynamics.  \n",
      "We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method. One was that the  \n",
      "episodes have exploring starts, and the other was that policy evaluation could be done with an infinite number of episodes. To obtain a practical algorithm we will have to remove both assumptions. We postpone consideration of the first assumption until later in this chapter.  \n",
      "For now we focus on the assumption that policy evaluation operates on an infinite number of episodes. This assumption is relatively easy to remove. In fact, the same issue arises even in classical DP methods such as iterative policy evaluation, which also converge only asymptotically to the true value function. In both DP and Monte Carlo cases there are two ways to solve the problem. One is to hold firm to the idea of approximating _q‚á°k_ in each policy evaluation. Measurements and assumptions are made to obtain bounds on the magnitude and probability of error in the estimates, and then sufficient steps are taken during each policy evaluation to assure that these bounds are sufficiently small. This approach can probably be made completely satisfactory in the sense of guaranteeing correct convergence up to some level of approximation. However, it is also likely to require far too many episodes to be useful in practice on any but the smallest problems.  \n",
      "The second approach to avoiding the infinite number of episodes nominally required for policy evaluation is to forgo trying to complete policy evaluation before returning to policy improvement. On each evaluation step we move the value function _toward q‚á°k_ , but we do not expect to actually get close except over many steps. We used this idea when we first introduced the idea of GPI in Section 4.6. One extreme form of the idea is value iteration, in which only one iteration of iterative policy evaluation is performed between each step of policy improvement. The in-place version of value iteration is even more extreme; there we alternate between improvement and evaluation steps for single states.  \n",
      "For Monte Carlo policy evaluation it is natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation, and then the policy is improved at all the states visited in the episode. A complete simple algorithm along these lines is given in Figure 5.4. We call this algorithm _Monte Carlo ES_ , for Monte Carlo with Exploring Starts.  \n",
      "In Monte Carlo ES, all the returns for each state‚Äìaction pair are accumulated and averaged, irrespective of what policy was in force when they were observed. It is easy to see that Monte Carlo ES cannot converge to any suboptimal policy. If it did, then the value function would eventually converge to the value function for that policy, and that in turn would cause the policy to change. Stability is achieved only when both the policy and the value function are optimal. Convergence to this optimal fixed point seems inevitable as the changes to the action-value function decrease over time, but has not yet  \n",
      "\\begin{array}{l}{{\\stackrel{n}{\\underset{\\mathrm{n}}{\\underbrace{}}}}}\\\\ {{\\frac{n}{\\omega}}}\\end{array}  \n",
      "**----- Start of picture text -----**<br>\n",
      "Initialize, for all  s 2  S,  a 2  A( s ):<br>Q ( s, a ) arbitrary<br>‚á° ( s ) arbitrary<br>Returns ( s, a ) empty list<br>Repeat forever:<br>Choose  S 0  2  S and  A 0  2  A( S 0) s.t. all pairs have probability  >  0<br>Generate an episode starting from  S 0 , A 0, following  ‚á°<br>For each pair  s, a  appearing in the episode:<br>G return following the first occurrence of  s, a<br>Append  G  to  Returns ( s, a )<br>Q ( s, a ) average( Returns ( s, a ))<br>For each  s  in the episode:<br>‚á° ( s ) argmax a Q ( s, a )<br>**----- End of picture text -----**<br>  \n",
      "Figure 5.4: Monte Carlo ES: A Monte Carlo control algorithm assuming exploring starts and that episodes always terminate for all policies.  \n",
      "been formally proved. In our opinion, this is one of the most fundamental open theoretical questions in reinforcement learning (for a partial solution, see Tsitsiklis, 2002).  \n",
      "**Example 5.3: Solving Blackjack** It is straightforward to apply Monte Carlo ES to blackjack. Since the episodes are all simulated games, it is easy to arrange for exploring starts that include all possibilities. In this case one simply picks the dealer‚Äôs cards, the player‚Äôs sum, and whether or not the player has a usable ace, all at random with equal probability. As the initial policy we use the policy evaluated in the previous blackjack example, that which sticks only on 20 or 21. The initial action-value function can be zero for all state‚Äìaction pairs. Figure 5.5 shows the optimal policy for blackjack found by Monte Carlo ES. This policy is the same as the ‚Äúbasic‚Äù strategy of Thorp (1966) with the sole exception of the leftmost notch in the policy for a usable ace, which is not present in Thorp‚Äôs strategy. We are uncertain of the reason for this discrepancy, but confident that what is shown here is indeed the optimal policy for the version of blackjack we have described.  \n",
      "## _CHAPTER 5. MONTE CARLO METHODS_  \n",
      "124  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "! [*] * Vv * [*] [*]<br>21<br>STICK 20<br>19<br>Usable 18 +1<br>17<br>ace 16<br>15 \"1<br>HIT 14<br>13<br>12<br>11<br>A 2 3 4 5 6 7 8 9 10<br>21<br>20<br>STICK 19<br>No 1817 +1<br>usable 16<br>15 \"1<br>ace HIT 14<br>13<br>12<br>11<br>A 2 3 4 5 6 7 8 9 10<br>Dealer showing Dealer showing<br>Dealer showing<br>Dealer showing<br>AA<br>AA<br>0<br>1010<br>1010<br>1212<br>1212<br>2121<br>2121<br>Player sum<br>Player sum<br>Player sum<br>Player sum<br>**----- End of picture text -----**<br>  \n",
      "Figure 5.5: The optimal policy and state-value function for blackjack, found by Monte Carlo ES (Figure 5.4). The state-value function shown was computed from the action-value function found by Monte Carlo ES.  \n",
      "##  \n",
      "## **5.4 Monte Carlo Control without Exploring Starts**  \n",
      "How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call _on-policy_ methods and _o‚Üµ-policy_ methods. Onpolicy methods attempt to evaluate or improve the policy that is used to make decisions, whereas o‚Üµ-policy methods evaluate or improve a policy di‚Üµerent from that used to generate the data. The Monte Carlo ES method developed above is an example of an on-policy method. In this section we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. O‚Üµ-policy methods are considered in the next section.  \n",
      "In on-policy control methods the policy is generally _soft_ , meaning that _‚á°_ ( _a|s_ ) _>_ 0 for all _s 2_ S and all _a 2_ A( _s_ ), but gradually shifted closer and closer to a deterministic optimal policy. Many of the methods discussed in Chapter 2 provide mechanisms for this. The on-policy method we present in this section uses _\"-greedy_ policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability _\"_  \n",
      "## _5.4. MONTE CARLO CONTROL WITHOUT EXPLORING STARTS_ 125  \n",
      "they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, _|_ A( _‚úès_ ) _|_[, and the remaining bulk of the] _‚úè_ probability, 1 _‚àí \"_ + _|_ A( _s_ ) _|_[, is given to the greedy action. The] _[ \"]_[-greedy policies] are examples of _\"-soft_ policies, defined as policies for which _‚á°_ ( _a|s_ ) _‚â• |_ A( _‚úès_ ) _|_[for] all states and actions, for some _\" >_ 0. Among _\"_ -soft policies, _\"_ -greedy policies are in some sense those that are closest to greedy.  \n",
      "The overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte Carlo ES, we use first-visit MC methods to estimate the action-value function for the current policy. Without the assumption of exploring starts, however, we cannot simply improve the policy by making it greedy with respect to the current value function, because that would prevent further exploration of nongreedy actions. Fortunately, GPI does not require that the policy be taken all the way to a greedy policy, only that it be moved _toward_ a greedy policy. In our on-policy method we will move it only to an _\"_ -greedy policy. For any _\"_ -soft policy, _‚á°_ , any _\"_ -greedy policy with respect to _q‚á°_ is guaranteed to be better than or equal to _‚á°_ .  \n",
      "That any _\"_ -greedy policy with respect to _q‚á°_ is an improvement over any _\"_ -soft policy _‚á°_ is assured by the policy improvement theorem. Let _‚á°[0]_ be the _\"_ -greedy policy. The conditions of the policy improvement theorem apply because for any _s 2_ S:  \n",
      "\\begin{array}{l l l}{{q_{\\pi}(s,\\pi^{\\prime}(s))}}&{{=}}&{{\\displaystyle\\sum_{a}\\pi^{\\prime}(a|s)q_{\\pi}(s,a)}}\\\\ {{}}&{{=}}&{{\\displaystyle\\frac{e}{|A(s)|}\\displaystyle\\sum_{a}q_{\\pi}(s,a)}}&{{\\displaystyle\\left(1-\\varepsilon\\right)\\prod_{a}\\frac{\\pi(a|s)-\\frac{e}{|\\vec{k}(s)|}q_{\\pi}(s,a)}}{{1-\\varepsilon}}}q_{\\pi}(s,a)}\\end{array}  \n",
      "\\begin{array}{l l}{{\\displaystyle(\\ t{\\mathrm{the~sum~is~a~weighted~anage~with~nonmmesed}}}}\\\\ {{\\mathrm{as~such~it~must~be~than~or~equal~to~the~unumber~averaged}}}\\\\ {{\\displaystyle=\\displaystyle\\frac{\\epsilon\\epsilon}{\\left|\\mathcal{A}(s)\\right|}\\displaystyle\\sum_{a}^{}q_{\\pi}(s,a)}}&{{\\displaystyle\\frac{\\alpha}{\\left|\\mathcal{A}(s)\\right|}\\displaystyle\\sum_{a}^{}q_{\\pi}(s,a)}}\\\\ {{\\displaystyle=\\displaystyle\\frac{\\epsilon}{\\left|\\mathcal{A}(s)}\\right|}\\displaystyle\\sum_{a}^{}q_{\\pi}(s,a)}}&{{\\displaystyle\\frac{\\Gamma}{\\epsilon}(s,a)}}\\end{array}  \n",
      "Thus, by the policy improvement theorem, _‚á°[0] ‚â• ‚á°_ (i.e., _v‚á°0_ ( _s_ ) _‚â• v‚á°_ ( _s_ ), for all _s 2_ S). We now prove that equality can hold only when both _‚á°[0]_ and _‚á°_ are optimal among the _\"_ -soft policies, that is, when they are better than or equal to all other _\"_ -soft policies.  \n",
      "Consider a new environment that is just like the original environment, except with the requirement that policies be _\"_ -soft ‚Äúmoved inside‚Äù the environment. The new environment has the same action and state set as the original  \n",
      "and behaves as follows. If in state _s_ and taking action _a_ , then with probability 1 _‚àí \"_ the new environment behaves exactly like the old environment. With probability _\"_ it repicks the action at random, with equal probabilities, and then behaves like the old environment with the new, random action. The best one can do in this new environment with general policies is the same as the best one could do in the original environment with _\"_ -soft policies. Let e _v‚á§_ and e _q‚á§_ denote the optimal value functions for the new environment. Then a policy _‚á°_ is optimal among _\"_ -soft policies if and only if _v‚á°_ = e _v‚á§_ . From the definition of e _v‚á§_ we know that it is the unique solution to  \n",
      "\\begin{array}{r c l}{{\\tilde{v}_{*}(s)}}&{{=}}&{{(1-\\varepsilon)\\operatorname*{max}\\tilde{q_{*}}(s,a)+\\frac{\\epsilon}{|\\mathcal{A}(s)|}\\sum_{a}\\tilde{q_{*}}(s,a)}}\\\\ {{}}&{{=}}&{{(1-\\varepsilon)\\operatorname*{max}\\sum_{a}p(s^{\\prime},r|s,a)\\Big[r+\\gamma\\tilde{v_{*}}(s^{\\prime})\\Big].}}\\end{array}  \n",
      "When equality holds and the _\"_ -soft policy _‚á°_ is no longer improved, then we also know, from (5.2), that  \n",
      "\\begin{array}{r c l}{{v_{\\pi}(s)}}&{{=}}&{{(1-\\varepsilon)\\operatorname*{max}q_{\\pi}(s,a)+\\frac{\\epsilon}{|\\mathcal{A}(s)|}\\sum_{a}q_{\\pi}(s,a)}}\\\\ {{}}&{{}}&{{}}\\\\ {{}}&{{=}}&{{(1-\\varepsilon)\\operatorname*{max}\\sum_{a}p(s^{\\prime},r|s,a)\\Big[r+\\gamma v_{\\pi}(s^{\\prime})\\Big].}}\\end{array}  \n",
      "However, this equation is the same as the previous one, except for the substitution of _v‚á°_ for e _v‚á§_ . Since e _v‚á§_ is the unique solution, it must be that _v‚á°_ = e _v‚á§_ .  \n",
      "In essence, we have shown in the last few pages that policy iteration works for _\"_ -soft policies. Using the natural notion of greedy policy for _\"_ -soft policies, one is assured of improvement on every step, except when the best policy has been found among the _\"_ -soft policies. This analysis is independent of how the action-value functions are determined at each stage, but it does assume that they are computed exactly. This brings us to roughly the same point as in the previous section. Now we only achieve the best policy among the _\"_ -soft policies, but on the other hand, we have eliminated the assumption of exploring starts. The complete algorithm is given in Figure 5.6.  \n",
      "OMITTED MATH EQUATION  \n",
      "**----- Start of picture text -----**<br>\n",
      "Initialize, for all  s 2  S,  a 2  A( s ):<br>Q ( s, a ) arbitrary<br>Returns ( s, a ) empty list<br>‚á° ( a|s ) an arbitrary  \" -soft policy<br>Repeat forever:<br>(a) Generate an episode using  ‚á°<br>(b) For each pair  s, a  appearing in the episode:<br>G return following the first occurrence of  s, a<br>Append  G  to  Returns ( s, a )<br>Q ( s, a ) average( Returns ( s, a ))<br>(c) For each  s  in the episode:<br>a [‚á§] arg max a Q ( s, a )<br>For all  a 2  A( s ):<br>1  ‚àí \"  +  \"/| A( s ) | if  a  =  a [‚á§]<br>‚á° ( a|s ) ‚á¢ \"/| A( s ) | if  a 6 =  a [‚á§]<br>**----- End of picture text -----**<br>  \n",
      "Figure 5.6: An on-policy first-visit MC control algorithm for _\"_ -soft policies.  \n",
      "## **5.5 O‚Üµ-policy Prediction via Importance Sampling**  \n",
      "So far we have considered methods for estimating the value functions for a policy given an infinite supply of episodes generated using that policy. Suppose now that all we have are episodes generated from a _di‚Üµerent_ policy. That is, suppose we wish to estimate _v‚á°_ or _q‚á°_ , but all we have are episodes following another policy _¬µ_ , where _¬µ 6_ = _‚á°_ . We call _‚á°_ the _target policy_ because learning its value function is the target of the learning process, and we call _¬µ_ the _behavior policy_ because it is the policy controlling the agent and generating behavior. The overall problem is called _o‚Üµ-policy learning_ because it is learning about a policy given only experience ‚Äúo‚Üµ‚Äù (not following) that policy.  \n",
      "In order to use episodes from _¬µ_ to estimate values for _‚á°_ , we must require that every action taken under _‚á°_ is also taken, at least occasionally, under _¬µ_ . That is, we require that _‚á°_ ( _a|s_ ) _>_ 0 implies _¬µ_ ( _a|s_ ) _>_ 0. This is called the assumption of _coverage_ . It follows from coverage that _¬µ_ must be stochastic in states where it is not identical to _‚á°_ . The target policy _‚á°_ , on the other hand, may be deterministic, and, in fact, this is a case of particular interest. Typically the target policy is the deterministic greedy policy with respect to the current action-value function estimate. This policy we hope becomes a deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an _\"_ -greedy policy.  \n",
      "Importance sampling is a general technique for estimating expected values under one distribution given samples from another. We apply this technique to o‚Üµ-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the _importance-sampling ratio_ . Given a starting state _St_ , the probability of the subsequent state‚Äìaction trajectory, _At, St_ +1 _, At_ +1 _, . . . , ST_ , occurring under any policy _‚á°_ is  \n",
      "\\prod_{k=t}^{T-1}\\pi(A_{k}|S_{k})p(S_{k+1}^{\\phantom{T}}|S_{k},A_{k}),  \n",
      "where _p_ is the state-transition probability function defined by (3.8). Thus, the relative probability of the trajectory under the target and behavior policies (the importance-sampling ratio) is  \n",
      "Q _T ‚àí_ 1  \n",
      "\\rho_{t}^{T}=\\prod_{k=t}^{T-1}\\pi(A_{k}|S_{k})p(S_{k+1}|S_{k},A_{k})}{=\\prod_{k=t}^{T-1}\\frac{\\pi(A_{k}|S_{k})}{\\mu(A_{k}|S_{k})}.\\qquad\\qquad(5.3)  \n",
      "Note that although the trajectory probabilities depend on the MDP‚Äôs transition probabilities, which are generally unknown, all the transition probabilities cancel and drop out. The importance sampling ratio ends up depending only on the two policies and not at all on the MDP.  \n",
      "Now we are ready to give a Monte Carto algorithm that uses a batch of observed episodes following policy _¬µ_ to estimate _v‚á°_ ( _s_ ). It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time _t_ = 101. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state _s_ is visited, denoted T( _s_ ). This is for an every-visit method; for a first-visit method, T( _s_ ) would only include time steps that were first visits to _s_ within their episode. Also, let _T_ ( _t_ ) denote the first time of termination following time _t_ , and _Gt_ denote the return after _t_ up through _T_ ( _t_ ). Then _{Gt}t2_ T( _s_ ) are the returns that pertain to state _s_ , and _{‚á¢[T] t_[(] _[t]_[)] _}t2_ T( _s_ ) are the corresponding importance-sampling ratios. To estimate _v‚á°_ ( _s_ ), we simply scale the returns by the ratios and average the results:  \n",
      "\\hookrightarrow  \n",
      "V(s)=\\frac{\\sum_{t\\in\\mathcal{V}(s)}\\rho_{t}^{T(t)}G_{t}}{|\\mathcal{V}(s)|}.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(5.4)  \n",
      "When importance sampling is done as a simple average in this way it is called _ordinary importance sampling_ .  \n",
      "An important alternative is _weighted importance sampling_ , which uses a _weighted_ average, defined as  \n",
      "V(s)=\\frac{\\sum_{t\\in\\mathcal{V}(s)}\\rho_{t}^{T(t)}G_{t}}{\\sum_{t\\in\\mathcal{V}(s)}\\rho_{t}^{T(t)}},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(5.5)  \n",
      "or zero if the denominator is zero. To understand these two varieties of importance sampling, consider their estimates after observing a single return. In the weighted-average estimate, the ratio _‚á¢[T] t_[(] _[t]_[)] for the single return cancels in the numerator and denominator, so that the estimate is equal to the observed return independent of the ratio (assuming the ratio is nonzero). Given that this return was the only one observed, this is a reasonable estimate, but of course its expectation is _v¬µ_ ( _s_ ) rather than _v‚á°_ ( _s_ ), and in this statistical sense it is biased. In contrast, the simple average (5.4) is always _v‚á°_ ( _s_ ) in expectation (it is unbiased), but it can be extreme. Suppose the ratio were ten, indicating that the trajectory observed is ten times as likely under the target policy as under the behavior policy. In this case the ordinary importance-sampling estimate would be _ten times_ the observed return. That is, it would be quite far from the observed return even though the episode‚Äôs trajectory is considered very representative of the target policy.  \n",
      "Formally, the di‚Üµerence between the two kinds of importance sampling is expressed in their variances. The variance of the ordinary importancesampling estimator is in general unbounded because the variance of the ratios is unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred. A complete every-visit MC algorithm for o‚Üµ-policy policy evaluation using weighted importance sampling is given at the end of the next section in Figure 5.9.  \n",
      "**Example 5.4: O‚Üµ-policy Estimation of a Blackjack State Value** We applied both ordinary and weighted importance-sampling methods to estimate the value of a single blackjack state from o‚Üµ-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimates for any other states. In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player‚Äôs cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generated by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21, as in Example 5.1. The value of this state under the target policy is approximately _‚àí_ 0 _._ 27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both o‚Üµ-policy methods closely approximated this value after 1000 o‚Üµ-policy episodes using the random policy. Figure 5.7 shows the mean squared error (estimated from 100 independent runs) for each method as a function of number of episodes. The weighted importance-sampling method has much lower overall error in this example, as is typical in practice.  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "4<br>Ordinary<br>Mean importance<br>sampling<br>square<br>2<br>error<br>(average over<br>100 runs)<br>Weighted importance sampling<br>0<br>0 10 100 1000 10,000<br>Episodes (log scale)<br>**----- End of picture text -----**<br>  \n",
      "Figure 5.7: Weighted importance sampling produces lower error estimates of the value of a single blackjack state from o‚Üµ-policy episodes (see Example 5.4).  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "R  = +1<br>‚á° (back |s ) = 1<br>0.1<br>back s<br>0.9 end ¬µ (back |s ) = [1]<br>2<br>2<br>Monte-Carlo<br>estimate of<br>          with  vœÄ ( s )<br>ordinary<br>importance  1<br>sampling<br>(ten runs)<br>0<br>1 10 100 1000 10,000 100,000 1,000,000 10,000,000 100,000,000<br>Episodes (log scale)<br>**----- End of picture text -----**<br>  \n",
      "Figure 5.8: Ordinary importance sampling produces surprisingly unstable estimates on the one-state MDP shown inset (Example 5.5). The correct estimate here is 1, and, even though this is the expected value of a sample return (after importance sampling), the variance of the samples is infinite, and the estimates do not convergence to this value. These results are for o‚Üµ-policy first-visit MC.  \n",
      "## **Example 5.5: Infinite Variance**  \n",
      "The estimates of ordinary importance sampling will typically have infinite variance, and thus unsatisfactory convergence properties, whenever the scaled returns have infinite variance‚Äîand this can easily happen in o‚Üµ-policy learning when trajectories contain loops. A simple example is shown inset in Figure 5.8. There is only one nonterminal state _s_ and two actions, end and back. The end action causes a deterministic transition to termination, whereas the back action transitions, with probability 0.9, back to _s_ or, with probability 0.1, on to termination. The rewards are +1 on the latter transition and otherwise zero. Consider the target policy that always selects back. All episodes under this policy consist of some number (possibly zero) of transitions back to _s_ followed by termination with a reward and return of +1. Thus the value of _s_ under the target policy is thus 1. Suppose we are estimating this value from o‚Üµ-policy data using the behavior policy that selects end and back with equal probability. The lower part of Figure 5.8 shows ten independent runs of the first-visit MC algorithm using ordinary importance sampling. Even after millions of episodes, the estimates fail to converge to the correct value of 1. In contrast, the weighted importance-sampling algorithm would give an estimate of exactly 1 everafter the first episode that was consistent with the target policy (i.e., that ended with the back action). This is clear because  \n",
      "that algorithm produces a weighted average of the returns consistent with the target policy, all of which would be exactly 1.  \n",
      "We can verify that the variance of the importance-sampling-scaled returns is infinite in this example by a simple calculation. The variance of any random variable _X_ is the expected value of the deviation from its mean _X_[¬Ø] , which can be written  \n",
      "\\stackrel{\\mathrm{event}}{\\mathrm{Var}}\\!\\left[X\\right]=\\mathbb{E}\\left[\\left(X-{\\bar{X}}\\right)^{2}\\right]=\\mathbb{E}\\left[X^{2}-2X{\\bar{X}}+{\\bar{X}}^{2}\\right]=\\mathbb{E}\\left[X^{2}\\right]-{\\bar{X}}^{2}.  \n",
      "Thus, if the mean is finite, as it is in our case, the variance is infinite if and only if the expectation of the square of the random variable is infinite. Thus, we need only show that the expected square of the importance-sampling-scaled return is infinite:  \n",
      "{}_{\\mathbb{Q}}\\ 2\\uparrow  \n",
      "\\mathbb{E}\\left[\\left(\\prod_{t=0}^{T-1}{\\frac{\\pi(A_{t}|S_{t})}{\\mu(A_{t}|S_{t})}}G_{0}\\right)^{2}\\right].  \n",
      "To compute this expectation, we break it down into cases based on episode length and termination. First note that, for any episode ending with the end action, the importance sampling ratio is zero, because the target policy would never take this action; these episodes thus contribute nothing to the expectation (the quantity in parenthesis will be zero) and can be ignored. We need only consider episodes that involve some number (possibly zero) of back actions that transition back to the nonterminal state, followed by a back action transitioning to termination. All of these episodes have a return of 1, so the _G_ 0 factor can be ignored. To get the expected square we need only consider each length of episode, multiplying the probability of the episode‚Äôs occurrence by the square of its importance-sampling ratio, and add these up:  \n",
      "\\mathbf{\\theta}\\lor1  \n",
      "\\setminus\\ {2}  \n",
      "\\frac{\\lambda^{\\mathrm{i}}}{\\lambda}\\mathrm{u}}{\\bigg|}_{1}^{\\frac{i}{2}}\\mathrm{u}}  \n",
      "## **5.6 Incremental Implementation**  \n",
      "Monte Carlo prediction methods can be implemented incrementally, on an episode-by-episode basis, using extensions of the techniques described in Chapter 2. Whereas in Chapter 2 we averaged _rewards_ , in Monte Carlo methods we average _returns_ . In all other respects exactly the same methods as used in Chapter 2 can be used for _on-policy_ Monte Carlo methods. For _o‚Üµ-policy_ Monte Carlo methods, we need to separately consider those that use _ordinary_ importance sampling and those that use _weighted_ importance sampling.  \n",
      "In ordinary importance sampling, the returns are scaled by the importance sampling ratio _‚á¢[T] t_[(] _[t]_[)] (5.3), then simply averaged. For these methods we can again use the incremental methods of Chapter 2, but using the scaled returns in place of the rewards of that chapter. This leaves the case of o‚Üµ-policy methods using _weighted_ importance sampling. Here we have to form a weighted average of the returns, and a slightly di‚Üµerent incremental algorithm is required.  \n",
      "Suppose we have a sequence of returns _G_ 1 _, G_ 2 _, . . . , Gn‚àí_ 1, all starting in the same state and each with a corresponding random weight _Wi_ (e.g., _Wi_ = _‚á¢[T] t_[(] _[t]_[)] ). We wish to form the estimate  \n",
      "P _n‚àí_ 1  \n",
      "V_{n}=\\frac{\\sum_{k=1}^{n-1}W_{k}G_{k}}{\\sum_{k=1}^{n-1}W_{k}},\\qquad n\\geq2,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad  \n",
      "and keep it up-to-date as we obtain a single additional return _Gn_ . In addition to keeping track of _Vn_ , we must maintain for each state the cumulative sum _Cn_ of the weights given to the first _n_ returns. The update rule for _Vn_ is  \n",
      "{\\Gamma}  \n",
      "V_{n+1}=V_{n}+{\\frac{W_{n}}{C_{n}}}\\left[G_{n}-V_{n}\\right],\\qquad n\\geq1,\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(5.7)  \n",
      "and  \n",
      "C_{n+1}=C_{n}+W_{n+1},  \n",
      "where _C_ 0 = 0 (and _V_ 1 is arbitrary and thus need not be specified). Figure 5.9 gives a complete episode-by-episode incremental algorithm for Monte Carlo policy evaluation. The algorithm is nominally for the o‚Üµ-policy case, using weighted importance sampling, but applies as well to the on-policy case just by choosing the target and behavior policies as the same.  \n",
      "Initialize, for all _s 2_ S, _a 2_ A( _s_ ): _Q_ ( _s, a_ ) arbitrary _C_ ( _s, a_ ) 0 _¬µ_ ( _a|s_ ) an arbitrary soft behavior policy _‚á°_ ( _a|s_ ) an arbitrary target policy Repeat forever: Generate an episode using _¬µ_ : _S_ 0 _, A_ 0 _, R_ 1 _, . . . , ST ‚àí_ 1 _, AT ‚àí_ 1 _, RT , ST G_ 0 _W_ 1 For _t_ = _T ‚àí_ 1 _, T ‚àí_ 2 _, . . ._ downto 0: _G Œ≥G_ + _Rt_ +1 _C_ ( _St, At_ ) _C_ ( _St, At_ ) + _W Q_ ( _St, At_ ) _Q_ ( _St, At_ ) + _C_ ( _SWt,At_ )[[] _[G][ ‚àí][Q]_[(] _[S][t][, A][t]_[)]] _W W[‚á°]_[(] _[A][t][|][S][t]_[)] _¬µ_ ( _At|St_ ) If _W_ = 0 then ExitForLoop  \n",
      "Figure 5.9: An incremental every-visit MC policy-evaluation algorithm, using weighted importance sampling. The approximation _Q_ converges to _q‚á°_ (for all encountered state‚Äìaction pairs) even though all actions are selected according to a potentially di‚Üµerent policy, _¬µ_ . In the on-policy case ( _‚á°_ = _¬µ_ ), _W_ is always 1.  \n",
      "## **5.7 O‚Üµ-Policy Monte Carlo Control**  \n",
      "We are now ready to present an example of the second class of learning control methods we consider in this book: o‚Üµ-policy methods. Recall that the distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control. In o‚Üµ-policy methods these two functions are separated. The policy used to generate behavior, called the _behavior_ policy, may in fact be unrelated to the policy that is evaluated and improved, called the _target_ policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions.  \n",
      "O‚Üµ-policy Monte Carlo control methods use one of the techniques presented in the preceding two sections. They follow the behavior policy while learning about and improving the target policy. These techniques requires that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage). To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability).  \n",
      "Figure 5.10 shows an o‚Üµ-policy Monte Carlo method, based on GPI and weighted importance sampling, for estimating _q‚á§_ . The target policy _‚á°_ is the greedy policy with respect to _Q_ , which is an estimate of _q‚á°_ . The behavior policy _¬µ_ can be anything, but in order to assure convergence of _‚á°_ to the optimal policy, an infinite number of returns must be obtained for each pair of state and action. This can be assured by choosing _¬µ_ to be _\"_ -soft.  \n",
      "A potential problem is that this method learns only from the _tails_ of episodes, after the last nongreedy action. If nongreedy actions are frequent, then learning will be slow, particularly for states appearing in the early portions of long episodes. Potentially, this could greatly slow learning. There has been insufficient experience with o‚Üµ-policy Monte Carlo methods to assess how serious this problem is. If it is serious, the most important way to address it is probably by incorporating temporal-di‚Üµerence learning, the algorithmic idea developed in the next chapter. Alternatively, if _Œ≥_ is less than 1, then the idea developed in the next section may also help significantly.  \n",
      "136  \n",
      "Initialize, for all _s 2_ S, _a 2_ A( _s_ ): _Q_ ( _s, a_ ) arbitrary _C_ ( _s, a_ ) 0 _‚á°_ ( _s_ ) a deterministic policy that is greedy with respect to _Q_ Repeat forever: Generate an episode using any soft policy _¬µ_ : _S_ 0 _, A_ 0 _, R_ 1 _, . . . , ST ‚àí_ 1 _, AT ‚àí_ 1 _, RT , ST G_ 0 _W_ 1 For _t_ = _T ‚àí_ 1 _, T ‚àí_ 2 _, . . ._ downto 0: _G Œ≥G_ + _Rt_ +1 _C_ ( _St, At_ ) _C_ ( _St, At_ ) + _W Q_ ( _St, At_ ) _Q_ ( _St, At_ ) + _C_ ( _SWt,At_ )[[] _[G][ ‚àí][Q]_[(] _[S][t][, A][t]_[)]] _‚á°_ ( _St_ ) argmax _a Q_ ( _St, a_ ) (with ties broken arbitrarily) _W W_ 1 _¬µ_ ( _At|St_ ) If _W_ = 0 then ExitForLoop  \n",
      "Figure 5.10: An o‚Üµ-policy every-visit MC control algorithm, using weighted importance sampling. The policy _‚á°_ converges to optimal at all encountered states even though actions are selected according to a di‚Üµerent soft policy _¬µ_ , which may change between or even within episodes.  \n",
      "## _‚á§_ **5.8 Importance Sampling on Truncated Re-**  \n",
      "## **turns**  \n",
      "So far our o‚Üµ-policy methods have formed importance-sampling ratios for returns considered as unitary wholes. This is clearly the right thing for a Monte Carlo method to do in the absence of discounting (i.e., if _Œ≥_ = 1), but if _Œ≥ <_ 1 then there may be something better. Consider the case where episodes are long and _Œ≥_ is significantly less than 1. For concreteness, say that episodes last 100 steps and that _Œ≥_ = 0. The return from time 0 will then be _G_ 0 = _R_ 1, and its importance sampling ratio will be a product of 100 factors, _‚á°_ ( _A_ 0 _|S_ 0) _‚á°_ ( _A_ 1 _|S_ 1) _¬µ_ ( _A_ 0 _|S_ 0) _¬µ_ ( _A_ 1 _|S_ 1) _[¬∑ ¬∑ ¬∑][ ‚á°] ¬µ_ ([(] _A[A]_ 99[99] _|[|] S[S]_ 99[99] )[)][. In ordinary importance sampling, the return will] be scaled by the entire product, but it is really only necessary to scale by the first factor, by _[‚á°] ¬µ_ ([(] _A[A]_ 0[0] _|[|] S[S]_ 0[0] )[)][. The other 99 factors] _[ ‚á°] ¬µ_ ([(] _A[A]_ 1[1] _|[|] S[S]_ 1[1] )[)] _[¬∑ ¬∑ ¬∑][ ‚á°] ¬µ_ ([(] _A[A]_ 99[99] _|[|] S[S]_ 99[99] )[)][are irrelevant] because after the first reward the return has already been determined. These later factors are all independent of the return and of expected value 1; they do not change the expected update, but they add enormously to its variance. In some cases they could even make the variance infinite. Let us now consider an idea for avoiding this large extraneous variance.  \n",
      "The essence of the idea is to think of discounting as determining a probability of termination or, equivalently, a _degree_ of partial termination. For any _Œ≥ 2_ [0 _,_ 1), we can think of the return _G_ 0 as partly terminating in one step, to the degree 1 _‚àí Œ≥_ , producing a return of just the first reward, _R_ 1, and as partly terminating after two steps, to the degree (1 _‚àí Œ≥_ ) _Œ≥_ , producing a return of _R_ 1 + _R_ 2, and so on. The latter degree corresponds to terminating on the second step, 1 _‚àí Œ≥_ , and not having already terminated on the first step, _Œ≥_ . The _‚àí_ degree of termination on the third step is thus (1 _Œ≥_ ) _Œ≥_[2] , with the _Œ≥_[2] reflecting that termination did not occur on either of the first two steps. The partial returns here are called _flat partial returns_ :  \n",
      "\\bar{G}_{t}^{h}=R_{t+1}+R_{t+2}+\\cdot\\cdot\\cdot\\cdot+R_{h},\\qquad0\\le t<h\\le T,  \n",
      "where ‚Äúflat‚Äù denotes the absence of discounting, and ‚Äúpartial‚Äù denotes that these returns do not extend all the way to termination but instead stop at _h_ , called the _horizon_ (and _T_ is the time of termination of the episode). The conventional full return _Gt_ can be viewed as a sum of flat partial returns as suggested above as follows:  \n",
      "\\scriptstyle{\\begin{array}{c}{\\scriptstyle{_\\mathrm{e}}_{1}^{n}|x_{1}^{n}\\rangle}\\\\ {\\scriptstyle{_\\mathrm{i}}^{n}}_{\\mathrm{k}}^{n}|x_{1}^{n}}\\\\ {\\scriptstyle{_{\\mathrm{j}}^{n}|x_{1}^{n}|x_{1}^{n}}}\\\\ {\\scriptstyle{_{\\mathrm{j}}^{n}|x_{1}^{n}|x_{1}^{n}}}\\end{array}}  \n",
      "Now we need to scale the flat partial returns by an importance sampling ratio that is similarly truncated. As _G[h] t_[only involves rewards up to a horizon] _h_ , we only need the ratio of the probabilities up to _h_ . We define an ordinary importance-sampling estimator, analogous to (5.4), as  \n",
      "V(s)=\\frac{\\sum_{t\\in\\mathcal{V}(s)}\\left(\\gamma^{T(t)-t-1}\\rho_{t}^{T(t)}\\bar{G}_{t}^{T(t)}+(1-\\gamma)\\,\\sum_{h=t+1}^{T(t)-1}\\gamma^{h-t-1}\\rho_{t}^{h}\\bar{G}_{t}^{h}\\right)}{|\\mathrm{J}(s)|},\\;(5.8)  \n",
      "and a weighted importance-sampling estimator, analogous to (5.5), as  \n",
      "\\mathfrak{C}  \n",
      "V(s)=\\frac{\\sum_{t\\in\\mathcal{V}(s)}\\left(\\gamma^{T(t)-t-1}\\rho_{t}^{T(t)}\\bar{G}_{t}^{T(t)}+(1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\gamma^{h-t-1}\\rho_{t}^{h}\\bar{G}_{t}^{h}\\right)}{\\sum_{t\\in\\mathcal{V}(s)}\\left(\\gamma^{T(t)-t-1}\\rho_{t}^{T(t)}+(1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\gamma^{h-t-1}\\rho_{t}^{h}\\right)}.  \n",
      "## **5.9 Summary**  \n",
      "The Monte Carlo methods presented in this chapter learn value functions and optimal policies from experience in the form of _sample episodes_ . This gives them at least three kinds of advantages over DP methods. First, they can be used to learn optimal behavior directly from interaction with the environment, with no model of the environment‚Äôs dynamics. Second, they can be used with simulation or _sample models_ . For surprisingly many applications it is easy to simulate sample episodes even though it is difficult to construct the kind of explicit model of transition probabilities required by DP methods. Third, it is easy and efficient to _focus_ Monte Carlo methods on a small subset of the states. A region of special interest can be accurately evaluated without going to the expense of accurately evaluating the rest of the state set (we explore this further in Chapter 8).  \n",
      "A fourth advantage of Monte Carlo methods, which we discuss later in the book, is that they may be less harmed by violations of the Markov property. This is because they do not update their value estimates on the basis of the value estimates of successor states. In other words, it is because they do not bootstrap.  \n",
      "In designing Monte Carlo control methods we have followed the overall schema of _generalized policy iteration_ (GPI) introduced in Chapter 4. GPI involves interacting processes of policy evaluation and policy improvement. Monte Carlo methods provide an alternative policy evaluation process. Rather than use a model to compute the value of each state, they simply average many returns that start in the state. Because a state‚Äôs value is the expected return, this average can become a good approximation to the value. In control methods we are particularly interested in approximating action-value functions, because these can be used to improve the policy without requiring a model of the environment‚Äôs transition dynamics. Monte Carlo methods intermix policy evaluation and policy improvement steps on an episode-by-episode basis, and can be incrementally implemented on an episode-by-episode basis.  \n",
      "Maintaining _sufficient exploration_ is an issue in Monte Carlo control methods. It is not enough just to select the actions currently estimated to be best, because then no returns will be obtained for alternative actions, and it may never be learned that they are actually better. One approach is to ignore this problem by assuming that episodes begin with state‚Äìaction pairs randomly selected to cover all possibilities. Such _exploring starts_ can sometimes be arranged in applications with simulated episodes, but are unlikely in learning from real experience. In _on-policy_ methods, the agent commits to always exploring and tries to find the best policy that still explores. In _o‚Üµ-policy_ methods, the agent also explores, but learns a deterministic optimal policy  \n",
      "that may be unrelated to the policy followed.  \n",
      "_O‚Üµ-policy Monte Carlo prediction_ refers to learning the value function of a _target policy_ from data generated by a di‚Üµerent _behavior policy_ . Such learning methods are all based on some form of _importance sampling_ , that is, on weighting returns by the ratio of the probabilities of taking the observed actions under the two policies. _Ordinary importance sampling_ uses a simple average of the weighted returns, whereas _weighted importance sampling_ uses a weighted average. Ordinary importance sampling produces unbiased estimates, but has larger, possibly infinite, variance, whereas weighted importance sampling always has finite variance and are preferred in practice. Despite their conceptual simplicity, o‚Üµ-policy Monte Carlo methods for both prediction and control remain unsettled and a subject of ongoing research.  \n",
      "The Monte Carlo methods treated in this chapter di‚Üµer from the DP methods treated in the previous chapter in two major ways. First, they operate on sample experience, and thus can be used for direct learning without a model. Second, they do not bootstrap. That is, they do not update their value estimates on the basis of other value estimates. These two di‚Üµerences are not tightly linked, and can be separated. In the next chapter we consider methods that learn from experience, like Monte Carlo methods, but also bootstrap, like DP methods.  \n",
      "## **Bibliographical and Historical Remarks**  \n",
      "The term ‚ÄúMonte Carlo‚Äù dates from the 1940s, when physicists at Los Alamos devised games of chance that they could study to help understand complex physical phenomena relating to the atom bomb. Coverage of Monte Carlo methods in this sense can be found in several textbooks (e.g., Kalos and Whitlock, 1986; Rubinstein, 1981).  \n",
      "An early use of Monte Carlo methods to estimate action values in a reinforcement learning context was by Michie and Chambers (1968). In pole balancing (Example 3.4), they used averages of episode durations to assess the worth (expected balancing ‚Äúlife‚Äù) of each possible action in each state, and then used these assessments to control action selections. Their method is similar in spirit to Monte Carlo ES with every-visit MC estimates. Narendra and Wheeler (1986) studied a Monte Carlo method for ergodic finite Markov chains that used the return accumulated from one visit to a state to the next as a reward for adjusting a learning automaton‚Äôs action probabilities.  \n",
      "Barto and Du‚Üµ(1994) discussed policy evaluation in the context of classical Monte Carlo algorithms for solving systems of linear equations. They used  \n",
      "the analysis of Curtiss (1954) to point out the computational advantages of Monte Carlo policy evaluation for large problems. Singh and Sutton (1996) distinguished between every-visit and first-visit MC methods and proved results relating these methods to reinforcement learning algorithms.  \n",
      "The blackjack example is based on an example used by Widrow, Gupta, and Maitra (1973). The soap bubble example is a classical Dirichlet problem whose Monte Carlo solution was first proposed by Kakutani (1945; see Hersh and Griego, 1969; Doyle and Snell, 1984). The racetrack exercise is adapted from Barto, Bradtke, and Singh (1995), and from Gardner (1973).  \n",
      "Monte Carlo ES was introduced in the 1998 edition of this book. That may have been the first explicit connection between Monte Carlo estimation and control methods based on policy iteration.  \n",
      "Efficient o‚Üµ-policy learning has become recognized as an important challenge that arises in several fields. For example, it is closely related to the idea of ‚Äúinterventions‚Äù and ‚Äúcounterfactuals‚Äù in probabalistic graphical (Bayesian) models (e.g., Pearl, 1995; Balke and Pearl, 1994). O‚Üµ-policy methods using importance sampling have a long history and yet still are not well understood. Weighted importance sampling, which is also sometimes called normalized importance sampling (e.g., Koller and Friedman, 2009), is discussed by, for example, Rubinstein (1981), Hesterberg (1988), Shelton (2001), and Liu (2001). Combining o‚Üµ-policy learning with temporal-di‚Üµerence learning and approximation methods introduces subtle issues that we consider in later chapters.  \n",
      "The target policy in o‚Üµ-policy learning is sometimes referred to in the literature as the ‚Äúestimation‚Äù policy, as it was in the first edition of this book.  \n",
      "Our treatment of the idea of importance sampling based on truncated returns is based on the analysis and ‚Äúforward view‚Äù of Sutton, Mahmood, Precup, and van Hasselt (2014). A related idea is that of per-decision importance sampling (Precup, Sutton and Singh, 2000).  \n",
      "## **Exercises**  \n",
      "**Exercise 5.1** Consider the diagrams on the right in Figure 5.2. Why does the estimated value function jump up for the last two rows in the rear? Why does it drop o‚Üµfor the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower?  \n",
      "**Exercise 5.2** What is the backup diagram for Monte Carlo estimation of _q‚á°_ ?  \n",
      "**Exercise 5.3** What is the Monte Carlo estimate analogous to (5.5) for _action_  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Finish<br>line<br>Finish<br>line<br>Starting line Starting line<br>**----- End of picture text -----**<br>  \n",
      "Figure 5.11: A couple of right turns for the racetrack task.  \n",
      "values, given returns generated using _¬µ_ ?  \n",
      "**Exercise 5.4** What is the equation analogous to (5.5) for _action_ values _Q_ ( _s, a_ ) instead of state values _V_ ( _s_ )?  \n",
      "**Exercise 5.5** In learning curves such as those shown in Figure 5.7 error generally decreases with training, as indeed happened for the ordinary importancesampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?  \n",
      "**Exercise 5.6** The results with Example 5.5 and shown in Figure 5.8 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?  \n",
      "**Exercise 5.7** Modify the algorithm for first-visit MC policy evaluation (Figure 5.1) to use the incremental implementation for sample averages described in Section 2.4.  \n",
      "**Exercise 5.8** Derive the weighted-average update rule (5.7) from (5.6). Follow the pattern of the derivation of the unweighted rule (2.3).  \n",
      "**Exercise 5.9: Racetrack (programming)** Consider driving a race car around a turn like those shown in Figure 5.11. You want to go as fast as possible, but not so fast as to run o‚Üµthe track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, _‚àí_ 1, or 0 in one step, for a total of nine actions.  \n",
      "Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero. Each episode begins in one of the randomly selected start states and ends when the car crosses the finish line. The rewards are _‚àí_ 1 for each step that stays on the track, and _‚àí_ 5 if the agent tries to drive o‚Üµthe track. Actually leaving the track is not allowed, but the position is always advanced by at least one cell along either the horizontal or vertical axes. With these restrictions and considering only right turns, such as shown in the figure, all episodes are guaranteed to terminate, yet the optimal policy is unlikely to be excluded. To make the task more challenging, we assume that on half of the time steps the position is displaced forward or to the right by one additional cell beyond that specified by the velocity. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy.  \n",
      "> _‚á§_ **Exercise 5.10** Modify the algorithm for o‚Üµ-policy Monte Carlo control (Figure 5.10) to use the idea of the truncated weighted-average estimator (5.9). Note that you will first need to convert this equation to action values.\n",
      "\n",
      "Source: {'source': 'data/AI_ALGORITHMS/sbfinalweek.pdf', 'chunk_number': 0, 'course': 'AI_ALGORITHMS', 'header': ''}\n",
      "Content: ## **Chapter 7**  \n",
      "## **Eligibility Traces**  \n",
      "Eligibility traces are one of the basic mechanisms of reinforcement learning. For example, in the popular TD( _Œª_ ) algorithm, the _Œª_ refers to the use of an eligibility trace. Almost any temporal-di‚Üµerence (TD) method, such as Q- learning or Sarsa, can be combined with eligibility traces to obtain a more general method that may learn more efficiently.  \n",
      "There are two ways to view eligibility traces. The more theoretical view, which we emphasize here, is that they are a bridge from TD to Monte Carlo methods. When TD methods are augmented with eligibility traces, they produce a family of methods spanning a spectrum that has Monte Carlo methods at one end and one-step TD methods at the other. In between are intermediate methods that are often better than either extreme method. In this sense eligibility traces unify TD and Monte Carlo methods in a valuable and revealing way.  \n",
      "The other way to view eligibility traces is more mechanistic. From this perspective, an eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action. The trace marks the memory parameters associated with the event as eligible for undergoing learning changes. When a TD error occurs, only the eligible states or actions are assigned credit or blame for the error. Thus, eligibility traces help bridge the gap between events and training information. Like TD methods themselves, eligibility traces are a basic mechanism for temporal credit assignment.  \n",
      "For reasons that will become apparent shortly, the more theoretical view of eligibility traces is called the _forward_ view, and the more mechanistic view is called the _backward_ view. The forward view is most useful for understanding _what_ is computed by methods using eligibility traces, whereas the backward view is more appropriate for developing intuition about the algorithms them-  \n",
      "## _CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "selves. In this chapter we present both views and then establish senses in which they are equivalent, that is, in which they describe the same algorithms from two points of view. As usual, we first consider the prediction problem and then the control problem. That is, we first consider how eligibility traces are used to help in predicting returns as a function of state for a fixed policy (i.e., in estimating _v‚á°_ ). Only after exploring the two views of eligibility traces within this prediction setting do we extend the ideas to action values and control methods.  \n",
      "## **7.1** _n_ **-Step TD Prediction**  \n",
      "What is the space of methods lying between Monte Carlo and TD methods? Consider estimating _v‚á°_ from sample episodes generated using _‚á°_ . Monte Carlo methods perform a backup for each state based on the entire sequence of observed rewards from that state until the end of the episode. The backup of simple TD methods, on the other hand, is based on just the one next reward, using the value of the state one step later as a proxy for the remaining rewards. One kind of intermediate method, then, would perform a backup based on an intermediate number of rewards: more than one, but less than all of them until termination. For example, a two-step backup would be based on the first two rewards and the estimated value of the state two steps later. Similarly, we could have three-step backups, four-step backups, and so on. Figure 7.1 diagrams the spectrum of _n-step backups_ for _v‚á°_ , with the one-step, simple TD backup on the left and the up-until-termination Monte Carlo backup on the right.  \n",
      "The methods that use _n_ -step backups are still TD methods because they still change an earlier estimate based on how it di‚Üµers from a later estimate. Now the later estimate is not one step later, but _n_ steps later. Methods in which the temporal di‚Üµerence extends over _n_ steps are called _n-step TD methods_ . The TD methods introduced in the previous chapter all use one-step backups, and henceforth we call them _one-step TD methods_ .  \n",
      "More formally, consider the backup applied to state _St_ as a result of the state‚Äìreward sequence, _St, Rt_ +1 _, St_ +1 _, Rt_ +2 _, . . . , RT , ST_ (omitting the actions for simplicity). We know that in Monte Carlo backups the estimate of _v‚á°_ ( _St_ ) is updated in the direction of the complete return:  \n",
      "G_{t}=R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}R_{t+3}+\\cdot\\cdot\\cdot+\\gamma^{T-t-1}R_{T},  \n",
      "where _T_ is the last time step of the episode. Let us call this quantity the _target_ of the backup. Whereas in Monte Carlo backups the target is the return, in  \n",
      "## _7.1. N -STEP TD PREDICTION_  \n",
      "\\mathrm{TO-lzstep}\\qquad\\ 2{\\mathrm{-step}}\\qquad\\qquad p{\\mathrm{-step}}\\qquad\\qquad\\mathrm{Monte}\\,{\\mathrm{Gaulo}}  \n",
      "Figure 7.1: The spectrum ranging from the one-step backups of simple TD methods to the up-until-termination backups of Monte Carlo methods. In between are the _n_ -step backups, based on _n_ steps of real rewards and the estimated value of the _n_ th next state, all appropriately discounted.  \n",
      "one-step backups the target is the first reward plus the discounted estimated value of the next state:  \n",
      "R_{t+1}+\\gamma V_{t}(S_{t+1}),  \n",
      "where _Vt_ : S _!_ R here is the estimate at time _t_ of _v‚á°_ , in which case it makes sense that _Œ≥Vt_ ( _St_ +1) should take the place of the remaining terms _Œ≥Rt_ +2 + _Œ≥_[2] _Rt_ +3 + _¬∑ ¬∑ ¬∑_ + _Œ≥[T][‚àí][t][‚àí]_[1] _RT_ , as we discussed in the previous chapter. Our point now is that this idea makes just as much sense after two steps as it does after one. The target for a two-step backup might be  \n",
      "R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}V_{t}(S_{t+2}),  \n",
      "where now _Œ≥_[2] _Vt_ ( _St_ +2) corrects for the absence of the terms _Œ≥_[2] _Rt_ +3 + _Œ≥_[3] _Rt_ +4 + _¬∑ ¬∑ ¬∑_ + _Œ≥[T][‚àí][t][‚àí]_[1] _RT_ . Similarly, the target for an arbitrary _n_ -step backup might be  \n",
      "R_{t+1}+\\gamma R_{t+2}+\\gamma^{2}+\\cdots+\\gamma^{n-1}R_{t+n}+\\gamma^{n}V_{t}(S_{t+n}),\\quad\\forall n\\geq1.  \n",
      "All of these can be considered approximate returns, truncated after _n_ steps and then corrected for the remaining missing terms, in the above case by _Vt_ ( _St_ + _n_ ). Notationally, it is useful to retain full generality for the correction term. We define the general _n-step return_ as  \n",
      "G_{t}^{t+n}(c)=R_{t+1}+\\gamma R_{t+2}+\\cdot\\cdot\\cdot\\cdot+\\gamma^{n-1}R_{h}+\\gamma^{n}c,  \n",
      "170  \n",
      "for any _n ‚â•_ 1 and any scalar correction _c 2_ R. The time _h_ = _t_ + _n_ is called the _horizon_ of the _n_ -step return.  \n",
      "If the episode ends before the horizon is reached, then the truncation in an _n_ -step return e‚Üµectively occurs at the episode‚Äôs end, resulting in the conventional complete return. In other words, if _h ‚â• T_ , then _G[h] t_[(] _[c]_[) =] _[ G][t]_[. Thus,] the last _n n_ -step returns of an episode are always complete returns, and an infinite-step return is always a complete return. This definition enables us to treat Monte Carlo methods as the special case of infinite-step targets. All of this is consistent with the tricks for treating episodic and continuing tasks equivalently that we introduced in Section 3.4. There we chose to treat the terminal state as a state that always transitions to itself with zero reward. Under this trick, all _n_ -step returns that last up to or past termination have the same value as the complete return.  \n",
      "An _n-step backup_ is defined to be a backup toward the _n_ -step return. In the tabular, state-value case, the _n_ -step backup at time _t_ produces the following increment ‚àÜ _t_ ( _St_ ) in the estimated value _V_ ( _St_ ):  \n",
      "\\Delta_{t}(S_{t})=\\alpha\\left[G_{t}^{t+n}(V_{t}(S_{t+n}))-V_{t}(S_{t})\\right],\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(7.2)  \n",
      "where _‚Üµ_ is a positive step-size parameter, as usual. The increments to the estimated values of the other states are defined to be zero (‚àÜ _t_ ( _s_ ) = 0 _, 8s 6_ = _St_ ).  \n",
      "We define the _n_ -step backup in terms of an increment, rather than as a direct update rule as we did in the previous chapter, in order to allow di‚Üµerent ways of making the updates. In _on-line updating_ , the updates are made during the episode, as soon as the increment is computed. In this case we write  \n",
      "V_{t+1}(s)=V_{t}(s)+\\Delta_{t}(s),\\qquad\\forall s\\in\\vartheta.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad  \n",
      "On-line updating is what we have implicitly assumed in most of the previous two chapters. In _o‚Üµ-line updating_ , on the other hand, the increments are accumulated ‚Äúon the side‚Äù and are not used to change value estimates until the end of the episode. In this case, the approximate values _Vt_ ( _s_ ) _, 8s 2_ S, do not change during an episode and can be denoted simpty _V_ ( _s_ ). At the end of the episode, the new value (for the next episode) is obtained by summing all the increments during the episode. That is, for an episode starting at time step 0 and terminating at step _T_ , for all _s 2_ S:  \n",
      "\\begin{array}{c c}{{V_{t+1}(s)=V_{t}(s),~~~~~~~\\forall t<T,~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \n",
      "with of course _V_ 0 of the next episode being the _VT_ of this one. You may recall how in Section 6.3 we carried this idea one step further, deferring the  \n",
      "increments until they could be summed over a whole set of episodes, in _batch updating_ .  \n",
      "For any value function _v_ : S _!_ R, the expected value of the _n_ -step return using _v_ is guaranteed to be a better estimate of _v‚á°_ than _v_ is, in a worst-state sense. That is, the worst error under the new estimate is guaranteed to be less than or equal to _Œ≥[n]_ times the worst error under _v_ :  \n",
      "ÔøΩÔøΩE _‚á°_  \n",
      "ÔøΩÔøΩ _St_ = _s_  \n",
      "\\begin{array}{l c c l}{{-\\ v_{\\pi}(s)|}}&{{\\leq}}&{{\\gamma^{n}\\ \\mathrm{max}\\ |v(s)-v_{\\pi}(s)|\\mathrm{~,~}\\left(\\mathbb{7}.\\mathbb{P}\\right)}}\\end{array}  \n",
      "\\operatorname{In}_{s}\\!\\mathop{\\mathrm{\\tiny{III}}}_{s}\\!\\subset\\!\\left|{\\overline{{\\mathbb{N}}}}_{t}\\!\\right|\\!\\left({\\frac{{\\mathcal{S}}_{t}\\!+\\!n}{\\theta}}\\!\\left({\\mathcal{S}}_{t+n}\\right)\\!\\right)  \n",
      "for all _n ‚â•_ 1. This is called the _error reduction property_ of _n_ -step returns. Because of the error reduction property, one can show formally that on-line and o‚Üµ-line TD prediction methods using _n_ -step backups converge to the correct predictions under appropriate technical conditions. The _n_ -step TD methods thus form a family of valid methods, with one-step TD methods and Monte Carlo methods as extreme members.  \n",
      "Nevertheless, _n_ -step TD methods are rarely used because they are inconvenient to implement. Computing _n_ -step returns requires waiting _n_ steps to observe the resultant rewards and states. For large _n_ , this can become problematic, particularly in control applications. The significance of _n_ -step TD methods is primarily for theory and for understanding related methods that are more conveniently implemented. In the next few sections we use the idea of _n_ -step TD methods to explain and justify eligibility trace methods.  \n",
      "**Example 7.1:** _n_ **-step TD Methods on the Random Walk** Consider using _n_ -step TD methods on the random walk task described in Example 6.2 and shown in Figure 6.5. Suppose the first episode progressed directly from the center state, C, to the right, through D and E, and then terminated on the right with a return of 1. Recall that the estimated values of all the states started at an intermediate value, _V_ 0( _s_ ) = 0 _._ 5. As a result of this experience, a onestep method would change only the estimate for the last state, _V_ (E), which would be incremented toward 1, the observed return. A two-step method, on the other hand, would increment the values of the two states preceding termination: _V_ (D) and _V_ (E) both would be incremented toward 1. A threestep method, or any _n_ -step method for _n >_ 2, would increment the values of all three of the visited states toward 1, all by the same amount. Which _n_ is better? Figure 7.2 shows the results of a simple empirical assessment for a larger random walk process, with 19 states (and with a _‚àí_ 1 outcome on the left, all values initialized to 0). Results are shown for on-line and o‚Üµ-line _n_ -step TD methods with a range of values for _n_ and _‚Üµ_ . The performance measure for each algorithm and parameter setting, shown on the vertical axis, is the square-root of the average squared error between its predictions at the end of the episodenfor the 19 states and their true values, then averaged over  \n",
      "## _CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "On-line n-step TD methods Off-line n-step TD methods<br>512256128 n=64 n=32 512 256 128 n=64<br>n=1<br>RMS error n=64<br>over first n=3<br>n=2<br>10 episodes n=32<br>n=32 n=1<br>n=4<br>n=16<br>n=8<br>n=16<br>n=8 n=2<br>n=4<br>‚Üµ ‚Üµ<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.2: Performance of _n_ -step TD methods as a function of _‚Üµ_ , for various values of _n_ , on a 19-state random walk task (Example 7.1).  \n",
      "the first 10 episodes and 100 repetitions of the whole experiment (the same sets of walks were used for all methods). First note that the on-line methods generally worked best on this task, both reaching lower levels of absolute error and doing so over a larger range of the step-size parameter _‚Üµ_ (in fact, all the o‚Üµ-line methods were unstable for _‚Üµ_ much above 0.3). Second, note that methods with an intermediate value of _n_ worked best. This illustrates how the generalization of TD and Monte Carlo methods to _n_ -step methods can potentially perform better than either of the two extreme methods.  \n",
      "## **7.2 The Forward View of TD(** _Œª_ **)**  \n",
      "Backups can be done not just toward any _n_ -step return, but toward any _average_ of _n_ -step returns. For example, a backup can be done toward a target that is half of a two-step return and half of a four-step return: 12 _[G] t[t]_[+2] ( _Vt_ ( _St_ +2)) + 21 _[G] t[t]_[+4] ( _Vt_ ( _St_ +4)). Any set of returns can be averaged in this way, even an infinite set, as long as the weights on the component returns are positive and sum to 1. The composite return possesses an error reduction property similar to that of individual _n_ -step returns (7.5) and thus can be used to construct backups with guaranteed convergence properties. Averaging produces a substantial new range of algorithms. For example, one could average one-step and infinite-step returns to obtain another way of interrelating TD and Monte Carlo methods. In principle, one could even average experience-based backups with DP backups to get a simple combination of experience-based and model-based methods (see Chapter 8).  \n",
      "## _7.2. THE FORWARD VIEW OF TD(Œª)_  \n",
      "A backup that averages simpler component backups is called a _complex backup_ . The backup diagram for a complex backup consists of the backup diagrams for each of the component backups with a horizontal line above them and the weighting fractions below. For example, the complex backup for the case mentioned at the start of this section, mixing half of a two-step backup and half of a four-step backup, has the diagram:  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "The TD( _Œª_ ) algorithm can be understood as one particular way of averaging _n_ -step backups. This average contains all the _n_ -step backups, each weighted proportional to _Œª[n][‚àí]_[1] , where _Œª 2_ [0 _,_ 1], and normalized by a factor of 1 _‚àí Œª_ to ensure that the weights sum to 1 (see Figure 7.3). The resulting backup is toward a return, called the _Œª-return_ , defined by  \n",
      "{\\cal L}_{t}=(1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_{t}^{t+n}(V_{t}(S_{t+n})).  \n",
      "Figure 7.4 further illustrates the weighting on the sequence of _n_ -step returns in the _Œª_ -return. The one-step return is given the largest weight, 1 _‚àí Œª_ ; the two-step return is given the next largest weight, (1 _‚àíŒª_ ) _Œª_ ; the three-step return is given the weight (1 _‚àí Œª_ ) _Œª_[2] ; and so on. The weight fades by _Œª_ with each additional step. After a terminal state has been reached, all subsequent _n_ -step returns are equal to _Gt_ . If we want, we can separate these post-termination terms from the main sum, yielding  \n",
      "{\\cal L}_{t}\\ \\ =\\ \\ (1-\\lambda)\\sum_{n=1}^{T-t-1}\\lambda^{n-1}G_{t}^{t+n}(V_{t}(S_{t+n}))\\ \\ +\\ \\ \\lambda^{T-t-1}G_{t},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (7.6)  \n",
      "as indicated in the figures. This equation makes it clearer what happens when _Œª_ = 1. In this case the main sum goes to zero, and the remaining term reduces to the conventional return, _Gt_ . Thus, for _Œª_ = 1, backing up according to the  \n",
      "_CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "TD(\"), \"-return<br>1!\"<br>(1!\") \"<br>(1!\") \" [2]<br># [= ][1]<br>\" [T-t-] [1]<br>Figure 7.3: The backup digram for TD( Œª ). If  Œª  = 0, then the overall backup<br>reduces to its first component, the one-step TD backup, whereas if  Œª  = 1, then<br>the overall backup reduces to its last component, the Monte Carlo backup.<br>Œª -return is the same as the Monte Carlo algorithm that we called constant- ‚Üµ<br>MC (6.1) in the previous chapter. On the other hand, if  Œª  = 0, then the<br>Œª -return reduces to  G [t] t [+1] ( Vt ( St +1)), the one-step return. Thus, for  Œª  = 0,<br>backing up according to the  Œª -return is the same as the one-step TD method,<br>TD(0) from the previous chapter (6.2).<br>We define the  Œª-return algorithm  as the method that performs backups<br>towards the  Œª -return as target. On each step,  t , it computes an increment,<br>weight given to<br>total area = 1<br>the 3-step return<br>is (1  ‚àí Œª ) Œª [2]<br>decay by \"<br>Weight 1!\" weight given to<br>actual, final return<br>is  Œª [T][ ‚àí][t][‚àí] [1]<br>t T<br>Time<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.4: Weighting given in the _Œª_ -return to each of the _n_ -step returns.  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "RrT<br>Rrt +3 Stst+3 +3<br>Rrt +2 Stst+2 +2<br>Rrt +1 s<br>Stt +1+1<br>s<br>Stt<br>Time<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.5: The forward or theoretical view. We decide how to update each state by looking forward to future rewards and states.  \n",
      "‚àÜ _t_ ( _St_ ), to the value of the state occurring on that step:  \n",
      "\\Delta_{t}(S_{t})=\\alpha\\biggl[L_{t}-V_{t}(S_{t})\\biggr].\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(7.7)  \n",
      "(The increments for other states are of course ‚àÜ _t_ ( _s_ ) = 0, for all _s 6_ = _St_ .) As with _n_ -step TD methods, the updating can be either on-line or o‚Üµ-line. The upper row of Figure 7.6 shows the performance of the on-line and o‚Üµline _Œª_ -return algorithms on the 19-state random walk task (Example 7.1). The experiment was just as in the _n_ -step case (Figure 7.2) except that here we varied _Œª_ instead of _n_ . Note that overall performance of the _Œª_ -return algorithms is comparable to that of the _n_ -step algorithms. In both cases we get best performance with an intermediate value of the truncation parameter, _n_ or _Œª_ .  \n",
      "The approach that we have been taking so far is what we call the theoretical, or _forward_ , view of a learning algorithm. For each state visited, we look forward in time to all the future rewards and decide how best to combine them. We might imagine ourselves riding the stream of states, looking forward from each state to determine its update, as suggested by Figure 7.5. After looking forward from and updating one state, we move on to the next and never have to work with the preceding state again. Future states, on the other hand, are viewed and processed repeatedly, once from each vantage point preceding them.  \n",
      "The _Œª_ -return algorithm is the basis for the forward view of eligibility traces as used in the TD( _Œª_ ) method. In fact, we show in a later section that, in the o‚Üµ-line case, the _Œª_ -return algorithm _is_ the TD( _Œª_ ) algorithm. The _Œª_ -return and TD( _Œª_ ) methods use the _Œª_ parameter to shift from one-step TD methods to Monte Carlo methods. The specific way this shift is done is interesting, but not obviously better or worse than the way it is done with simple _n_ -step methods by varying _n_ . Ultimately, the most compelling motivation for the _Œª_ way of mixing _n_ -step backups is that there in a simple algorithm‚ÄîTD( _Œª_ )‚Äîfor achieving it. This is a mechanism issue rather than a theoretical one. In the  \n",
      "## _CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "On-line Œª-return Off-line Œª-return<br>= off-line TD(Œª), accumulating traces<br>Œª=1<br>Œª=1 Œª=.99<br>Œª=.99<br>Œª=.975<br>Œª=0<br>Œª=.95<br>Œª=.99<br>Œª=.4<br>Œª=.975<br>Œª=.8<br>Œª=0 Œª=.95<br>Œª=.95<br>Œª=.9<br>Œª=.9<br>Œª=.4<br>Œª=.8<br>On-line TD(Œª), dutch traces On-line TD(Œª), accumulating traces<br>Œª=1 1 .99 .975<br>Œª=.95<br>Œª=.9<br>Œª=.99 Œª=.8<br>Œª=.975<br>Œª=.95<br>Œª=0 Œª=0<br>Œª=.95<br>Œª=.4<br>Œª=.9<br>Œª=.4<br>Œª=.8<br>True on-line TD(Œª) On-line TD(Œª), replacing traces<br>= real-time Œª-return<br>Œª=1 Œª=1 Œª=.99<br>Œª=.99 Œª=.975<br>Œª=.975<br>Œª=.95<br>Œª=0 Œª=.975<br>Œª=0<br>Œª=.95<br>Œª=.95<br>Œª=.9<br>Œª=.4 Œª=.9 Œª=.4<br>Œª=.8 Œª=.8<br>‚Üµ ‚Üµ<br>RMS error over first 10 episodes on 19-state random walk<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.6: Performance of all _Œª_ -based algorithms on the 19-state random walk (Example 7.1). The _Œª_ = 0 line is the same for all five on-line algorithms.  \n",
      "## _7.3. THE BACKWARD VIEW OF TD(Œª)_  \n",
      "next few sections we develop the mechanistic, or backward, view of eligibility traces as used in TD( _Œª_ ).  \n",
      "## **7.3 The Backward View of TD(** _Œª_ **)**  \n",
      "In the previous section we presented the forward or theoretical view of the tabular TD( _Œª_ ) algorithm as a way of mixing backups that parametrically shifts from a TD method to a Monte Carlo method. In this section we instead define TD( _Œª_ ) mechanistically and show that it can closely approximate the forward view. The mechanistic, or _backward_ , view of TD( _Œª_ ) is useful because it is simple conceptually and computationally. In particular, the forward view itself is not directly implementable because it is _acausal_ , using at each step knowledge of what will happen many steps later. The backward view provides a causal, incremental mechanism for approximating the forward view and, in the o‚Üµ-line case, for achieving it exactly.  \n",
      "In the backward view of TD( _Œª_ ), there is an additional memory variable associated with each state, its _eligibility trace_ . The eligibility trace for state _s_ at time _t_ is a random variable denoted _Et_ ( _s_ ) _2_ R[+] . On each step, the eligibility traces of all non-visited states decay by _Œ≥Œª_ :  \n",
      "E_{t}(s)=\\gamma\\lambda E_{t-1}(s),\\qquad\\forall s\\in\\vartheta,s\\neq S_{t},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad  \n",
      "where _Œ≥_ is the discount rate and _Œª_ is the parameter introduced in the previous section. Henceforth we refer to _Œª_ as the _trace-decay parameter_ . What about the trace for _St_ , the one state visited at time _t_ ? The classical eligibility trace for _St_ decays just like for any state, but is then incremented by 1:  \n",
      "E_{t}(S_{t})=\\gamma\\lambda E_{t-1}(S_{t})+1.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(7.9)  \n",
      "This kind of eligibility trace is called an _accumulating_ trace because it accumulates each time the state is visited, then fades away gradually when the state is not visited, as illustrated as illustrated below.  \n",
      "OMITTED IMAGE  \n",
      "accumulating eligibility trace times of visits to a state  \n",
      "Eligibility traces keep a simple record of which states have recently been visited, where ‚Äúrecently‚Äù is defined in terms of _Œ≥Œª_ . The traces are said to indicate the degree to which each state is _eligible_ for undergoing learning changes should a reinforcing event occur. The reinforcing events we are concerned with  \n",
      "178  \n",
      "## _CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "are the moment-by-moment one-step TD errors. For example, the TD error for state-value prediction is  \n",
      "\\delta_{t}\\ =\\ R_{t+1}+\\gamma V_{t}(S_{t+1})-V_{t}(S_{t}).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(7.10)  \n",
      "In the backward view of TD( _Œª_ ), the global TD error signal triggers proportional updates to all recently visited states, as signaled by their nonzero traces:  \n",
      "\\Delta V_{t}(s)=\\alpha\\delta_{t}E_{t}(s).\\qquad{\\mathrm{for~all~}}s\\in8.  \n",
      "As always, these increments could be done on each step to form an on-line algorithm, or saved until the end of the episode to produce an o‚Üµ-line algorithm. In either case, equations (7.8‚Äì7.11) provide the mechanistic definition of the TD( _Œª_ ) algorithm. A complete algorithm for on-line TD( _Œª_ ) is given in Figure 7.7.  \n",
      "The backward view of TD( _Œª_ ) is oriented backward in time. At each moment we look at the current TD error and assign it backward to each prior state according to the state‚Äôs eligibility trace at that time. We might imagine ourselves riding along the stream of states, computing TD errors, and shouting them back to the previously visited states, as suggested by Figure 7.8. Where the TD error and traces come together, we get the update given by (7.11).  \n",
      "To better understand the backward view, consider what happens at various values of _Œª_ . If _Œª_ = 0, then by (7.9) all traces are zero at _t_ except for the trace corresponding to _St_ . Thus the TD( _Œª_ ) update (7.11) reduces to the simple TD  \n",
      "Initialize _V_ ( _s_ ) arbitrarily (but set to 0 if _s_ is terminal) Repeat (for each episode): Initialize _E_ ( _s_ ) = 0, for all _s 2_ S Initialize _S_ Repeat (for each step of episode): _A_ action given by _‚á°_ for _S_ Take action _A_ , observe reward, _R_ , and next state, _S[0] Œ¥ R_ + _Œ≥V_ ( _S[0]_ ) _‚àí V_ ( _S_ ) _E_ ( _S_ ) _E_ ( _S_ ) + 1 (accumulating traces) or _E_ ( _S_ ) (1 _‚àí ‚Üµ_ ) _E_ ( _S_ ) + 1 (dutch traces) or _E_ ( _S_ ) 1 (replacing traces) For all _s 2_ S: _V_ ( _s_ ) _V_ ( _s_ ) + _‚ÜµŒ¥E_ ( _s_ ) _E_ ( _s_ ) _Œ≥ŒªE_ ( _s_ ) _S S[0]_ until _S_ is terminal  \n",
      "Figure 7.7: On-line tabular TD( _Œª_ ).  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "!<br>t<br>Eett Ìõø t<br>Sstt -3-3 StEset -2 t -2 t EStsett -1 t -1 Eett<br>s<br>Stt<br>s<br>Time Stt +1+1<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.8: The backward or mechanistic view. Each update depends on the current TD error combined with eligibility traces of past events.  \n",
      "rule (6.2), which we henceforth call TD(0). In terms of Figure 7.8, TD(0) is the case in which only the one state preceding the current one is changed by the TD error. For larger values of _Œª_ , but still _Œª <_ 1, more of the preceding states are changed, but each more temporally distant state is changed less because its eligibility trace is smaller, as suggested in the figure. We say that the earlier states are given less _credit_ for the TD error.  \n",
      "If _Œª_ = 1, then the credit given to earlier states falls only by _Œ≥_ per step. This turns out to be just the right thing to do to achieve Monte Carlo behavior. For example, remember that the TD error, _Œ¥t_ , includes an undiscounted term of _Rt_ +1. In passing this back _k_ steps it needs to be discounted, like any reward in a return, by _Œ≥[k]_ , which is just what the falling eligibility trace achieves. If _Œª_ = 1 and _Œ≥_ = 1, then the eligibility traces do not decay at all with time. In this case the method behaves like a Monte Carlo method for an undiscounted, episodic task. If _Œª_ = 1, the algorithm is also known as TD(1).  \n",
      "TD(1) is a way of implementing Monte Carlo algorithms that is more general than those presented earlier and that significantly increases their range of applicability. Whereas the earlier Monte Carlo methods were limited to episodic tasks, TD(1) can be applied to discounted continuing tasks as well. Moreover, TD(1) can be performed incrementally and on-line. One disadvantage of Monte Carlo methods is that they learn nothing from an episode until it is over. For example, if a Monte Carlo control method does something that produces a very poor reward but does not end the episode, then the agent‚Äôs tendency to do that will be undiminished during the episode. On-line TD(1), on the other hand, learns in an _n_ -step TD way from the incomplete ongoing episode, where the _n_ steps are all the way up to the current step. If something unusually good or bad happens during an episode, control methods based on  \n",
      "TD(1) can learn immediately and alter their behavior on that same episode.  \n",
      "It is revealing to revisit the 19-state random walk example (Example 7.1) to see how well the backward-view TD( _Œª_ ) algorithm does in approximating the ideal of the forward-view _Œª_ -return algorithm. The performances of o‚Üµ-line and on-line TD( _Œª_ ) with accumulating traces are shown in the upper-right and middle-right panels of Figure 7.6. In the o‚Üµ-line case it has been proven that the _Œª_ -return algorithm and TD( _Œª_ ) are identical in their overall updates at the end of the episode. Thus, the one set of results in the upper-right panel is sufficient for both of these algorithms. However, recall that the o‚Üµ-line case is not our main focus, as all of its performance levels are generally lower and obtained over a narrower range of parameter values than can be obtained with on-line methods, as we saw earlier for _n_ -step methods in Figure 7.2 and for _Œª_ -return methods in the upper two panels of Figure 7.6.  \n",
      "In the on-line case, the performances of TD( _Œª_ ) with accumulating traces (middle-right panel) are indeed much better and closer to that of the on-line _Œª_ -return algorithm (upper-left panel). If _Œª_ = 0, then in fact it is the identical algorithm at all _‚Üµ_ , and if _‚Üµ_ is small, then for all _Œª_ it is a close approximation to the _Œª_ -return algorithm by the end of each episode. However, if both parameters are larger, for example _Œª >_ 0 _._ 9 and _‚Üµ>_ 0 _._ 5, then the algorithms perform substantially di‚Üµerently: the _Œª_ -return algorithm performs a little less well whereas TD( _Œª_ ) is likely to be unstable. This is not a terrible problem, as these parameter values are higher than one would want to use anyway, but it is a weakness of the method.  \n",
      "Two alternative variations of eligibility traces have been proposed to address these limitations of the accumulating trace. On each step, all three trace types decay the traces of the non-visited states in the same way, that is, according to (7.8), but they di‚Üµer in how the visited state is incremented. The first trace variation is the _replacing trace_ . Suppose a state is visited and then revisited before the trace due to the first visit has fully decayed to zero. With accumulating traces the revisit causes a further increment in the trace (7.9), driving it greater than 1, whereas, with replacing traces, the trace is simply reset to 1:  \n",
      "E_{t t}(S_{t})=1.\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(7.12)  \n",
      "In the special case of _Œª_ = 1, TD( _Œª_ ) with replacing traces is closely related to first-visit Monte Carlo methods. The second trace variation, called the _dutch trace_ , is sort of intermediate between accumulating and replacing traces, depending on the step-size parameter _‚Üµ_ . Dutch traces are defined by  \n",
      "E_{t}(S_{t})=(1-\\alpha)\\gamma\\lambda E_{t-1}(S_{t})+1.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \n",
      "Note that as _‚Üµ_ approaches zero, the dutch trace becomes the accumulating  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "times of state visits<br>accumulating traces<br>dutch traces (Œ± = 0.5)<br>replacing traces<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.9: The three di‚Üµerent kinds of traces. Accumulating traces add up each time a state is visited, whereas replacing traces are reset to one, and dutch traces do something inbetween, depending on _‚Üµ_ (here we show them for _‚Üµ_ = 0 _._ 5). In all cases the traces decay at a rate of _Œ≥Œª_ per step; here we show _Œ≥Œª_ = 0 _._ 8 such that the traces have a time constant of approximately 5 steps (the last four visits are on successive steps).  \n",
      "trace, and, if _‚Üµ_ = 1, the dutch trace becomes the replacing trace. Figure 7.9 contrasts the three kinds of traces, showing the behavior of the dutch trace for _‚Üµ_ = 1 _/_ 2. The performances of TD( _Œª_ ) with these two kinds of traces are shown as additional panels in (7.6). In both cases, performance is more robust to the parameter values than it is with accumulating traces. The performance with dutch traces in particular achieves our goal of an on-line causal algorithm that closely approximates the _Œª_ -return algorithm.  \n",
      "## **7.4 Equivalences of Forward and Backward Views**  \n",
      "It is sometimes possible to prove that two learning methods originating in di‚Üµerent ways are in fact equivalent in the strong sense that the value functions they produce are exactly the same on every time step. A simple case of this is that one-step methods and all _Œª_ -based methods are equivalent if _Œª_ = 0. This follows immediately from the fact that their backup targets are all the same. Another easy-to-see example is the equivalence at _Œª_ = 1 of o‚Üµ-line TD( _Œª_ ) and the constant- _‚Üµ_ MC methods, as noted in the previous section. Of particular interest are equivalences between forward-view algorithms, which are often more intuitive and clearer conceptually, and backward-view algorithms that are efficient and causal. The best example of this that we have encountered so far is the equivalence at all _Œª_ of the o‚Üµ-line _Œª_ -return algorithm (forward view) and o‚Üµ-line TD( _Œª_ ) with accumulating traces (backward view). That was an equivalence of value functions at the end of episodes and, because they are o‚Üµline methods which don‚Äôt change values within an episode, it is a step-by-step equivalence as well. This equivalence was proved formally in the first edition  \n",
      "of this book, and was verified empirically here on the 19-state random-walk example in producing the upper-left panel of Figure 7.6.  \n",
      "For on-line methods (and _Œª >_ 0) the first edition of this book established only _approximate_ episode-by-episode equivalences between the _Œª_ -return algorithm and TD( _Œª_ ). In the random-walk problem, at the end of episodes, TD( _Œª_ ) with accumulating traces is almost the same as the _Œª_ -return algorithm, but only for small _‚Üµ_ and _Œª_ . With dutch traces the approximation is closer, but it is still not exact even on an episode-by-episode basis (compare the upper-left and middle left panels of Figure 7.6). Only recently has an interesting exact equivalence been established between a _Œª_ -based forward view and an efficient backward-view implementation, in particular, between a ‚Äúreal-time‚Äù _Œª_ -return algorithm and the ‚Äútrue online TD( _Œª_ )‚Äù algorithm (van Seijen and Sutton, 2014). This is a striking and revealing result, but a little technical. The best way to present it is using the notation of linear function approximation, which we develop in Chapter 9. We postpone development of the real-time _Œª_ -return algorithm until then and present here only the backward-view algorithm.  \n",
      "_True online TD(Œª)_ is defined by the dutch trace (Eqs. 7.13 and 7.8) and the following value function update:  \n",
      "V_{t+1}(s)=V_{t}(s)+\\alpha\\left[\\delta_{t}+V_{t}(S_{t})-V_{t-1}(S_{t})\\right]E_{t}(s)-\\alpha I_{s S_{t}}\\left[V_{t}(S_{t})-V_{t-1}(S_{t})\\right],  \n",
      "for all _s 2_ S, where _Ixy_ is an identity-indicator function, equal to 1 if _x_ = _y_ and 0 otherwise. An efficient implementation is given as a boxed algorithm in Figure 7.10.  \n",
      "Results on the 19-state random-walk example for true online TD( _Œª_ ) are given in the lower-left panel of Figure 7.6. We see that in this example true on-line TD( _Œª_ ) appears to perform slightly better than the on-line _Œª_ -return algorithm, but not necessarly better than TD( _Œª_ ) with dutch traces; most of the performance improvement seems to come from the dutch traces rather than the slightly di‚Üµerent or extra terms in the equations above. Of course, this is just one example; benefits of the exact equivalence may appear on other problems. One thing we can say in that these slight di‚Üµerences enable true on-line TD( _Œª_ ) with _Œª_ = 1 to be exactly equivalent by the end of the episode to the constant- _‚Üµ_ MC method, while making updates on-line and in real-time. The same cannot be said for any of the other methods.  \n",
      "Initialize _V_ ( _s_ ) arbitrarily (but set to 0 if _s_ is terminal) _V_ old 0 Repeat (for each episode): Initialize _E_ ( _s_ ) = 0, for all _s 2_ S Initialize _S_ Repeat (for each step of episode): _A_ action given by _‚á°_ for _S_ Take action _A_ , observe reward, _R_ , and next state, _S[0]_ ‚àÜ _V_ ( _S_ ) _‚àí V_ old _V_ old _V_ ( _S[0]_ ) _Œ¥ R_ + _Œ≥V_ ( _S[0]_ ) _‚àí V_ ( _S_ ) _E_ ( _S_ ) (1 _‚àí ‚Üµ_ ) _E_ ( _S_ ) + 1 For all _s 2_ S: _V_ ( _s_ ) _V_ ( _s_ ) + _‚Üµ_ ( _Œ¥_ + ‚àÜ) _E_ ( _s_ ) _E_ ( _s_ ) _Œ≥ŒªE_ ( _s_ ) _V_ ( _S_ ) _V_ ( _S_ ) _‚àí ‚Üµ_ ‚àÜ _S S[0]_ until _S_ is terminal  \n",
      "## Figure 7.10: Tabular true on-line TD( _Œª_ ).  \n",
      "## **7.5 Sarsa(** _Œª_ **)**  \n",
      "How can eligibility traces be used not just for prediction, as in TD( _Œª_ ), but for control? As usual, the main idea of one popular approach is simply to learn action values, _Qt_ ( _s, a_ ), rather than state values, _Vt_ ( _s_ ). In this section we show how eligibility traces can be combined with Sarsa in a straightforward way to produce an on-policy TD control method. The eligibility trace version of Sarsa we call _Sarsa_ ( _Œª_ ), and the original version presented in the previous chapter we henceforth call _one-step Sarsa_ .  \n",
      "The idea in Sarsa( _Œª_ ) is to apply the TD( _Œª_ ) prediction method to state‚Äì action pairs rather than to states. Obviously, then, we need a trace not just for each state, but for each state‚Äìaction pair. Let _Et_ ( _s, a_ ) denote the trace for state‚Äìaction pair _s, a_ . The traces can be any of the three types‚Äîaccumulating, replace, or dutch‚Äîand are updated in essentially the same way as before except of course being triggered by visiting the state‚Äìaction pair (here given using the identity-indicator notation):  \n",
      "_Et_ ( _s, a_ ) = _Œ≥ŒªEt‚àí_ 1( _s, a_ ) + _IsStIaAt_ (accumulating) _Et_ ( _s, a_ ) = (1 _‚àí ‚Üµ_ ) _Œ≥ŒªEt‚àí_ 1( _s, a_ ) + _IsStIaAt_ (dutch) _Et_ ( _s, a_ ) = (1 _‚àí IsStIaAt_ ) _Œ≥ŒªEt‚àí_ 1( _s, a_ ) + _IsStIaAt_ (replacing) for all _s 2_ S _, a 2_ A. Otherwise Sarsa( _Œª_ ) is just like TD( _Œª_ ), substituting  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Sarsa(Œª)<br>s , aStt ,  Att<br>1‚àíŒª<br>(1‚àíŒª) Œª<br>(1‚àíŒª) Œª [2]<br>sSTT<br>Œ£ [= ] [1]<br>Œª [T-t-] [1]<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.11: Sarsa( _Œª_ )‚Äôs backup diagram.  \n",
      "state‚Äìaction variables for state variables‚Äî _Qt_ ( _s, a_ ) for _Vt_ ( _s_ ) and _Et_ ( _s, a_ ) for _Et_ ( _s_ ):  \n",
      "_Qt_ +1( _s, a_ ) = _Qt_ ( _s, a_ ) + _‚ÜµŒ¥t Et_ ( _s, a_ ) _,_ for all _s, a_  \n",
      "where  \n",
      "\\delta_{t}=R_{t+1}+\\gamma Q_{t}(S_{t+1},A_{t+1})-Q_{t}(S_{t},A_{t}).  \n",
      "Figure 7.11 shows the backup diagram for Sarsa( _Œª_ ). Notice the similarity to the diagram of the TD( _Œª_ ) algorithm (Figure 7.3). The first backup looks ahead one full step, to the next state‚Äìaction pair, the second looks ahead two steps, and so on. A final backup is based on the complete return. The weighting of each backup is just as in TD( _Œª_ ) and the _Œª_ -return algorithm.  \n",
      "One-step Sarsa and Sarsa( _Œª_ ) are on-policy algorithms, meaning that they approximate _q‚á°_ ( _s, a_ ), the action values for the current policy, _‚á°_ , then improve the policy gradually based on the approximate values for the current policy. The policy improvement can be done in many di‚Üµerent ways, as we have seen throughout this book. For example, the simplest approach is to use the _\"_ - greedy policy with respect to the current action-value estimates. Figure 7.12 shows the complete Sarsa( _Œª_ ) algorithm for this case.  \n",
      "**Example 7.2: Traces in Gridworld** The use of eligibility traces can substantially increase the efficiency of control algorithms. The reason for this  \n",
      "Initialize _Q_ ( _s, a_ ) arbitrarily, for all _s 2_ S _, a 2_ A( _s_ ) Repeat (for each episode): _E_ ( _s, a_ ) = 0, for all _s 2_ S _, a 2_ A( _s_ ) Initialize _S_ , _A_ Repeat (for each step of episode): Take action _A_ , observe _R_ , _S[0]_ Choose _A[0]_ from _S[0]_ using policy derived from _Q_ (e.g., _\"_ -greedy) _Œ¥ R_ + _Œ≥Q_ ( _S[0] , A[0]_ ) _‚àí Q_ ( _S, A_ ) _E_ ( _S, A_ ) _E_ ( _S, A_ ) + 1 (accumulating traces) or _E_ ( _S, A_ ) (1 _‚àí ‚Üµ_ ) _E_ ( _S, A_ ) + 1 (dutch traces) or _E_ ( _S, A_ ) 1 (replacing traces) For all _s 2_ S _, a 2_ A( _s_ ): _Q_ ( _s, a_ ) _Q_ ( _s, a_ ) + _‚ÜµŒ¥E_ ( _s, a_ ) _E_ ( _s, a_ ) _Œ≥ŒªE_ ( _s, a_ ) _S S[0]_ ; _A A[0]_ until _S_ is terminal  \n",
      "Figure 7.12: Tabular Sarsa( _Œª_ ).  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Action values increased Action values increased<br>Path taken<br>by one-step Sarsa by Sarsa(!) with !=0.9<br>**----- End of picture text -----**<br>  \n",
      "OMITTED IMAGE  \n",
      "Figure 7.13: Gridworld example of the speedup of policy learning due to the use of eligibility traces.  \n",
      "186  \n",
      "## _CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "is illustrated by the gridworld example in Figure 7.13. The first panel shows the path taken by an agent in a single episode, ending at a location of high reward, marked by the *. In this example the values were all initially 0, and all rewards were zero except for a positive reward at the * location. The arrows in the other two panels show which action values were strengthened as a result of this path by one-step Sarsa and Sarsa( _Œª_ ) methods. The one-step method strengthens only the last action of the sequence of actions that led to the high reward, whereas the trace method strengthens many actions of the sequence. The degree of strengthening (indicated by the size of the arrows) falls o‚Üµ(according to _Œ≥Œª_ or (1 _‚àí ‚Üµ_ ) _Œ≥Œª_ ) with steps from the reward. In this example, the fall o‚Üµis 0.9 per step.  \n",
      "## **7.6 Watkins‚Äôs Q(** _Œª_ **)**  \n",
      "When Chris Watkins (1989) first proposed Q-learning, he also proposed a simple way to combine it with eligibility traces. Recall that Q-learning is an o‚Üµ-policy method, meaning that the policy learned about need not be the same as the one used to select actions. In particular, Q-learning learns about the greedy policy while it typically follows a policy involving exploratory actions‚Äî occasional selections of actions that are suboptimal according to _Q_ . Because of this, special care is required when introducing eligibility traces.  \n",
      "Suppose we are backing up the state‚Äìaction pair _St, At_ at time _t_ . Suppose that on the next two time steps the agent selects the greedy action, but on the third, at time _t_ + 3, the agent selects an exploratory, nongreedy action. In learning about the value of the greedy policy at _St, At_ we can use subsequent experience only as long as the greedy policy is being followed. Thus, we can use the one-step and two-step returns, but not, in this case, the three-step return. The _n_ -step returns for all _n ‚â•_ 3 no longer have any necessary relationship to the greedy policy.  \n",
      "Thus, unlike TD( _Œª_ ) or Sarsa( _Œª_ ), Watkins‚Äôs Q( _Œª_ ) does not look ahead all the way to the end of the episode in its backup. It only looks ahead as far as the next exploratory action. Aside from this di‚Üµerence, however, Watkins‚Äôs Q( _Œª_ ) is much like TD( _Œª_ ) and Sarsa( _Œª_ ). Their lookahead stops at episode‚Äôs end, whereas Q( _Œª_ )‚Äôs lookahead stops at the first exploratory action, or at episode‚Äôs end if there are no exploratory actions before that. Actually, to be more precise, one-step Q-learning and Watkins‚Äôs Q( _Œª_ ) both look one action _past_ the first exploration, using their knowledge of the action values. For example, suppose the first action, _At_ +1, is exploratory. Watkins‚Äôs Q( _Œª_ ) would still do the one-step update of _Qt_ ( _St, At_ ) toward _Rt_ +1 + _Œ≥_ max _a Qt_ ( _St_ +1 _, a_ ). In general,  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Watkins's Q(\")<br>s , aStt ,  Att<br>1!\"<br>OR<br>(1!\") \"<br>(1!\") \" [2]<br>stS + t-nn<br>\" [T-t-] [1] first<br>non-greedy<br>\" [n] [!1] action<br>**----- End of picture text -----**<br>  \n",
      "Figure 7.14: The backup diagram for Watkins‚Äôs Q( _Œª_ ). The series of component backups ends either with the end of the episode or with the first nongreedy action, whichever comes first.  \n",
      "if _At_ + _n_ is the first exploratory action, then the longest backup is toward  \n",
      "R_{t+1}+\\gamma R_{t+2}+\\cdot\\cdot\\cdot+\\gamma^{n-1}R_{t+n}+\\gamma^{n}\\operatorname*{max}Q_{t}(S_{t+n},a),  \n",
      "where we assume o‚Üµ-line updating. The backup diagram in Figure 7.14 illustrates the forward view of Watkins‚Äôs Q( _Œª_ ), showing all the component backups.  \n",
      "The mechanistic or backward view of Watkins‚Äôs Q( _Œª_ ) is also very simple. Eligibility traces are used just as in Sarsa( _Œª_ ), except that they are set to zero whenever an exploratory (nongreedy) action is taken. The trace update is best thought of as occurring in two steps. First, the traces for all state‚Äìaction pairs are either decayed by _Œ≥Œª_ or, if an exploratory action was taken, set to 0. Second, the trace corresponding to the current state and action is incremented by 1. The overall result is  \n",
      "\\left\\{\\begin{array}{c}{{\\gamma\\lambda E_{t-1}(s,a)+I_{s S_{t}}\\cdot I_{a A_{t}}}}\\end{array}\\right.  \n",
      "E_{t}(s,a)={\\left\\{\\begin{array}{l l}{\\gamma\\lambda E_{t-1}(s,a)+I_{s S_{t}}\\cdot I_{a A_{t}}}&{{\\mathrm{if~}}Q_{t-1}(S_{t},A_{t})=\\operatorname*{max}_{a}Q_{t-1}(S_{t},a);}\\\\ {I_{s S_{t}}\\cdot I_{a A_{t}}}&{{\\mathrm{otherwise.}}}\\end{array}\\right.}  \n",
      "One could also use analogous dutch or replacing traces here. The rest of the algorithm is defined by  \n",
      "Q_{t+1}(s,a)=Q_{t}(s,a)+\\alpha\\delta_{t}E_{t}(s,a),\\quad\\forall s\\in\\S,a\\in{\\mathcal{A}}(s)  \n",
      "188  \n",
      "## _CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "Initialize _Q_ ( _s, a_ ) arbitrarily, for all _s 2_ S _, a 2_ A( _s_ ) Repeat (for each episode): _E_ ( _s, a_ ) = 0, for all _s 2_ S _, a 2_ A( _s_ ) Initialize _S_ , _A_ Repeat (for each step of episode): Take action _A_ , observe _R_ , _S[0]_ Choose _A[0]_ from _S[0]_ using policy derived from _Q_ (e.g., _\"_ -greedy) _A[‚á§]_ argmax _a Q_ ( _S[0] , a_ ) (if _A[0]_ ties for the max, then _A[‚á§] A[0]_ ) _Œ¥ R_ + _Œ≥Q_ ( _S[0] , A[‚á§]_ ) _‚àí Q_ ( _S, A_ ) _E_ ( _S, A_ ) _E_ ( _S, A_ ) + 1 (accumulating traces) or _E_ ( _S, A_ ) (1 _‚àí ‚Üµ_ ) _E_ ( _S, A_ ) + 1 (dutch traces) or _E_ ( _S, A_ ) 1 (replacing traces) For all _s 2_ S _, a 2_ A( _s_ ): _Q_ ( _s, a_ ) _Q_ ( _s, a_ ) + _‚ÜµŒ¥E_ ( _s, a_ ) If _A[0]_ = _A[‚á§]_ , then _E_ ( _s, a_ ) _Œ≥ŒªE_ ( _s, a_ ) else _E_ ( _s, a_ ) 0 _S S[0]_ ; _A A[0]_ until _S_ is terminal  \n",
      "Figure 7.15: Tabular version of Watkins‚Äôs Q( _Œª_ ) algorithm.  \n",
      "where  \n",
      "\\delta_{t}=R_{t+1}+\\gamma\\operatorname*{max}_{a^{\\prime}}Q_{t}(S_{t+1},a^{\\prime})-Q_{t}(S_{t},A_{t}).  \n",
      "Figure 7.15 shows the complete algorithm in pseudocode.  \n",
      "Unfortunately, cutting o‚Üµtraces every time an exploratory action is taken loses much of the advantage of using eligibility traces. If exploratory actions are frequent, as they often are early in learning, then only rarely will backups of more than one or two steps be done, and learning may be little faster than one-step Q-learning.  \n",
      "## **7.7 O‚Üµ-policy Eligibility Traces using Importance Sampling**  \n",
      "The eligibility traces in Watkins‚Äôs Q( _Œª_ ) are a crude way to deal with o‚Üµpolicy training. First, they treat the o‚Üµ-policy aspect as binary; either the target policy is followed and traces continue normally, or it is deviated from and traces are cut o‚Üµcompletely; there is nothing inbetween. But the target policy may take di‚Üµerent actions with di‚Üµerent positive probabilities, as may the behavior policy, in which case following and deviating will be a matter of  \n",
      "## _7.8. IMPLEMENTATION ISSUES_  \n",
      "degree. In Chapter 5 we saw how to use the ratio of the two probabilities of taking the action to more precisely assign credit to a single action, and the product of ratios to assign credit to a sequence.  \n",
      "Second, Watkins‚Äôs Q( _Œª_ ) confounds bootstrapping and o‚Üµ-policy deviation. Bootstrapping refers to the degree to which an algorithm builds its estimates from other estimates, like TD and DP, or does not, like MC methods. In TD( _Œª_ ) and Sarsa( _Œª_ ), the _Œª_ parameter controls the degree of bootstrapping, with the value _Œª_ = 1 denoting no bootstrapping, turning these TD methods into MC methods. But the same cannot be said for Q( _Œª_ ). As soon as there is a deviation from the target policy Q( _Œª_ ) cuts the trace and uses its value estimate rather than waiting for the actual rewards‚Äîit bootstraps even if _Œª_ = 1. Ideally we would like to totally de-couple bootstrapping from the o‚Üµ-policy aspect, to use _Œª_ to specify the degree of bootstrapping while using importance sampling to correct independently for the degree of o‚Üµ-policy deviation.  \n",
      "## **7.8 Implementation Issues**  \n",
      "It might at first appear that methods using eligibility traces are much more complex than one-step methods. A naive implementation would require every state (or state‚Äìaction pair) to update both its value estimate and its eligibility trace on every time step. This would not be a problem for implementations on single-instruction, multiple-data parallel computers or in plausible neural implementations, but it is a problem for implementations on conventional serial computers. Fortunately, for typical values of _Œª_ and _Œ≥_ the eligibility traces of almost all states are almost always nearly zero; only those that have recently been visited will have traces significantly greater than zero. Only these few states really need to be updated because the updates at the others will have essentially no e‚Üµect.  \n",
      "In practice, then, implementations on conventional computers keep track of and update only the few states with nonzero traces. Using this trick, the computational expense of using traces is typically a few times that of a onestep method. The exact multiple of course depends on _Œª_ and _Œ≥_ and on the expense of the other computations. Cichosz (1995) has demonstrated a further implementation technique that further reduces complexity to a constant independent of _Œª_ and _Œ≥_ . Finally, it should be noted that the tabular case is in some sense a worst case for the computational complexity of traces. When function approximation is used (Chapter 9), the computational advantages of not using traces generally decrease. For example, if artificial neural networks and backpropagation are used, then traces generally cause only a doubling of the required memory and computation per step.  \n",
      "190 _6_  \n",
      "## _‚á§_ **7.9 Variable** _Œª 6_  \n",
      "The _Œª_ -return can be significantly generalized beyond what we have described so far by allowing _Œª_ to vary from step to step, that is, by redefining the trace update as _6_  \n",
      "_Œ≥Œªt Et‚àí_ 1( _s_ ) _6_  \n",
      "E_{t}(s)=\\left\\{\\begin{array}{l l}{{\\gamma\\slash{\\lambda}_{t}E_{t-1}\\left(s\\right)}}&{{\\mathrm{i}\\int s\\llap/\\slash{\\gamma}\\partial_{t}\\xi}}\\\\ {{\\gamma\\partial_{t}E_{t-1}\\left(s\\right)+\\mathrm{i}}}&{{\\mathrm{i}\\mathrm{~}s=S_{t},}}\\end{array}\\right.  \n",
      "_6_ where _Œªt_ denotes the value of _Œª_ at time _t_ . This is an advanced topic because the added generality has never been used in practical applications, but it is interesting theoretically and may yet prove useful. For example, one idea is to vary _Œª_ as a function of state: _Œªt_ = _Œª_ ( _St_ ). If a state‚Äôs value estimate is believed to be known with high certainty, then it makes sense to use that estimate fully, ignoring whatever states and rewards are received after it. This corresponds to cutting o‚Üµall the traces once this state has been reached, that is, to choosing the _Œª_ for the certain state to be zero or very small. Similarly, states whose value estimates are highly uncertain, perhaps because even the state estimate is unreliable, can be given _Œª_ s near 1. This causes their estimated values to have little e‚Üµect on any updates. They are ‚Äúskipped over‚Äù until a state that is known better is encountered. Some of these ideas were explored formally by Sutton and Singh (1994).  \n",
      "_6_ The eligibility trace equation above is the backward view of variable _Œª_ s. The corresponding forward view is a more general definition of the _Œª_ -return:  \n",
      "\\begin{array}{l l l}{{G_{t}^{\\lambda}}}&{{=}}&{{\\displaystyle\\sum_{n=t+1}^{\\infty}G_{t}^{(n)}\\left(1-\\lambda_{t+n}\\right)\\prod_{i=t+1}^{t+n-1}\\lambda_{i}\\qquad}}&{{\\qquad\\qquad}}&{{\\qquad\\qquad}}\\\\ {{}}&{{}}&{{}}&{{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad}}&{{\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\times\\qquad\\qquad\\quad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\quad\\quad\\qquad\\quad\\qquad\\qquad\\qquad\\qquad\\quad\\qquad\\qquad  \n",
      "## _6_ **7.10 Conclusions**  \n",
      "_6_ Eligibility traces in conjunction with TD errors provide an efficient, incremental way of shifting and choosing between Monte Carlo and TD methods. Traces can be used without TD errors to achieve a similar e‚Üµect, but only awkwardly. A method such as TD( _Œª_ ) enables this to be done from partial experiences and with little memory and little nonmeaningful variation in predictions.  \n",
      "_6_ As we mentioned in Chapter 5, Monte Carlo methods may have advantages in non-Markov tasks because they do not bootstrap. Because eligibility  \n",
      "traces make TD methods more like Monte Carlo methods, they also can have advantages in these cases. If one wants to use TD methods because of their other advantages, but the task is at least partially non-Markov, then the use of an eligibility trace method is indicated. Eligibility traces are the first line of defense against both long-delayed rewards and non-Markov tasks.  \n",
      "By adjusting _Œª_ , we can place eligibility trace methods anywhere along a continuum from Monte Carlo to one-step TD methods. Where shall we place them? We do not yet have a good theoretical answer to this question, but a clear empirical answer appears to be emerging. On tasks with many steps per episode, or many steps within the half-life of discounting, it appears significantly better to use eligibility traces than not to (e.g., see Figure 9.12). On the other hand, if the traces are so long as to produce a pure Monte Carlo method, or nearly so, then performance degrades sharply. An intermediate mixture appears to be the best choice. Eligibility traces should be used to bring us toward Monte Carlo methods, but not all the way there. In the future it may be possible to vary the trade-o‚Üµbetween TD and Monte Carlo methods more finely by using variable _Œª_ , but at present it is not clear how this can be done reliably and usefully.  \n",
      "Methods using eligibility traces require more computation than one-step methods, but in return they o‚Üµer significantly faster learning, particularly when rewards are delayed by many steps. Thus it often makes sense to use eligibility traces when data are scarce and cannot be repeatedly processed, as is often the case in on-line applications. On the other hand, in o‚Üµ-line applications in which data can be generated cheaply, perhaps from an inexpensive simulation, then it often does not pay to use eligibility traces. In these cases the objective is not to get more out of a limited amount of data, but simply to process as much data as possible as quickly as possible. In these cases the speedup per datum due to traces is typically not worth their computational cost, and one-step methods are favored.  \n",
      "## **Bibliographical and Historical Remarks**  \n",
      "- **7.1‚Äì2** The forward view of eligibility traces in terms of _n_ -step returns and the _Œª_ -return is due to Watkins (1989), who also first discussed the error reduction property of _n_ -step returns. Our presentation is based on the slightly modified treatment by Jaakkola, Jordan, and Singh (1994). The results in the random walk examples were made for this text based on work of Sutton (1988) and Singh and Sutton (1996). The use of backup diagrams to describe these and other algorithms in this chapter is new, as are the terms ‚Äúforward view‚Äù and ‚Äúbackward view.‚Äù  \n",
      "TD( _Œª_ ) was proved to converge in the mean by Dayan (1992), and with probability 1 by many researchers, including Peng (1993), Dayan and Sejnowski (1994), and Tsitsiklis (1994). Jaakkola, Jordan, and Singh (1994), in addition, first proved convergence of TD( _Œª_ ) under on-line updating. Gurvits, Lin, and Hanson (1994) proved convergence of a more general class of eligibility trace methods.  \n",
      "- **7.3** TD( _Œª_ ) with accumulating traces was introduced by Sutton (1988, 1984). Replacing traces are due to Singh and Sutton (1996). Dutch traces are due to van Seijen and Sutton (2014, in prep).  \n",
      "- Eligibility traces came into reinforcement learning via the fecund ideas of Klopf (1972). Our use of eligibility traces was based on Klopf‚Äôs work (Sutton, 1978a, 1978b, 1978c; Barto and Sutton, 1981a, 1981b; Sutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983; Sutton, 1984). We may have been the first to use the term ‚Äúeligibility trace‚Äù (Sutton and Barto, 1981). The idea that stimuli produce aftere‚Üµects in the nervous system that are important for learning is very old. See Section 14.??.  \n",
      "- **7.4** The episode-by-episode equivalence of forward and backward views, and the relationships to Monte Carlo methods, were proved by Sutton (1988) for undiscounted episodic tasks, then extended to the general case in the first edition of this book (1989). We see these as now superceded by the analyses and step-by-step equivalences in Section 9.??.  \n",
      "- **7.5** Sarsa( _Œª_ ) was first explored as a control method by Rummery and Niranjan (1994; Rummery, 1995). Our presentation of replacing traces omits a subtlety which is sometimes found to be beneficial: clearing (setting to zero) the traces of all the actions _not_ taken in the state that is visited, as suggested by Singh and Sutton (1996). This can also be done in Q( _Œª_ ). Nowadays we would recommend just using dutch traces, which generalize better to function approximation.  \n",
      "- **7.6** Watkins‚Äôs Q( _Œª_ ) is due to Watkins (1989). Peng‚Äôs Q( _Œª_ ) is due to Peng and Williams (Peng, 1993; Peng and Williams, 1994, 1996). Rummery (1995) made extensive comparative studies of these algorithms.  \n",
      "- Convergence has still not been proved for any control method for 0 _< Œª <_ 1.  \n",
      "## _7.10. CONCLUSIONS_  \n",
      "- **7.8-9** The ideas in these two sections were generally known for many years, but beyond what is in the sources cited in the sections themselves, this text may be the first place they have been described. Perhaps the first published discussion of variable _Œª_ was by Watkins (1989), who pointed out that the cutting o‚Üµof the backup sequence (Figure 7.14) in his Q( _Œª_ ) when a nongreedy action was selected could be implemented by temporarily setting _Œª_ to 0.  \n",
      "## **Exercises**  \n",
      "**Exercise 7.1** Why do you think a larger random walk task (19 states instead of 5) was used in the examples of this chapter? Would a smaller walk have shifted the advantage to a di‚Üµerent value of _n_ ? How about the change in leftside outcome from 0 to _‚àí_ 1? Would that have made any di‚Üµerence in the best value of _n_ ?  \n",
      "**Exercise 7.2** Why do you think on-line methods worked better than o‚Üµ-line methods on the example task?  \n",
      "> _‚á§_ **Exercise 7.3** In the lower part of Figure 7.2, notice that the plot for _n_ = 3 is di‚Üµerent from the others, dropping to low performance at a much lower value of _‚Üµ_ than similar methods. In fact, the same was observed for _n_ = 5, _n_ = 7, and _n_ = 9. Can you explain why this might have been so? In fact, we are not sure ourselves. See `http://www.cs.utexas.edu/~ikarpov/Classes/RL/RandomWalk/` for an attempt at a thorough answer by Igor Karpov.  \n",
      "**Exercise 7.4** The parameter _Œª_ characterizes how fast the exponential weighting in Figure 7.4 falls o‚Üµ, and thus how far into the future the _Œª_ -return algorithm looks in determining its backup. But a rate factor such as _Œª_ is sometimes an awkward way of characterizing the speed of the decay. For some purposes it is better to specify a time constant, or half-life. What is the equation relating _Œª_ and the half-life, _‚åßŒª_ , the time by which the weighting sequence will have fallen to half of its initial value?  \n",
      "**Exercise 7.5 (programming)** Draw a backup diagram for Sarsa( _Œª_ ) with replacing traces.  \n",
      "**Exercise 7.6** Write pseudocode for an implementation of TD( _Œª_ ) that updates only value estimates for states whose traces are greater than some small positive constant.  \n",
      "**Exercise 7.7** Write equations or pseudocode for Sarsa( _Œª_ ) and/or Q( _Œª_ ) with dutch traces. Do the same for a true-on-line version.  \n",
      "_CHAPTER 7. ELIGIBILITY TRACES_  \n",
      "## **Chapter 8**  \n",
      "## **Planning and Learning with Tabular Methods**  \n",
      "In this chapter we develop a unified view of methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporaldi‚Üµerence methods. We think of the former as _planning_ methods and of the latter as _learning_ methods. Although there are real di‚Üµerences between these two kinds of methods, there are also great similarities. In particular, the heart of both kinds of methods is the computation of value functions. Moreover, all the methods are based on looking ahead to future events, computing a backedup value, and then using it to update an approximate value function. Earlier in this book we presented Monte Carlo and temporal-di‚Üµerence methods as distinct alternatives, then showed how they can be seamlessly integrated by using eligibility traces such as in TD( _Œª_ ). Our goal in this chapter is a similar integration of planning and learning methods. Having established these as distinct in earlier chapters, we now explore the extent to which they can be intermixed.  \n",
      "## **8.1 Models and Planning**  \n",
      "By a _model_ of the environment we mean anything that an agent can use to predict how the environment will respond to its actions. Given a state and an action, a model produces a prediction of the resultant next state and next reward. If the model is stochastic, then there are several possible next states and next rewards, each with some probability of occurring. Some models produce a description of all possibilities and their probabilities; these we call _distribution models_ . Other models produce just one of the possibilities, sampled  \n",
      "## 196 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "according to the probabilities; these we call _sample models_ . For example, consider modeling the sum of a dozen dice. A distribution model would produce all possible sums and their probabilities of occurring, whereas a sample model would produce an individual sum drawn according to this probability distribution. The kind of model assumed in dynamic programming‚Äîestimates of the state transition probabilities and expected rewards, _p_ ( _s[0] |s, a_ ) and _r_ ( _s, a, s[0]_ )‚Äî is a distribution model. The kind of model used in the blackjack example in Chapter 5 is a sample model. Distribution models are stronger than sample models in that they can always be used to produce samples. However, in surprisingly many applications it is much easier to obtain sample models than distribution models.  \n",
      "Models can be used to mimic or simulate experience. Given a starting state and action, a sample model produces a possible transition, and a distribution model generates all possible transitions weighted by their probabilities of occurring. Given a starting state and a policy, a sample model could produce an entire episode, and a distribution model could generate all possible episodes and their probabilities. In either case, we say the model is used to _simulate_ the environment and produce _simulated experience_ .  \n",
      "The word _planning_ is used in several di‚Üµerent ways in di‚Üµerent fields. We use the term to refer to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment:  \n",
      "## planning model policy  \n",
      "Within artificial intelligence, there are two distinct approaches to planning according to our definition. In _state-space planning_ , which includes the approach we take in this book, planning is viewed primarily as a search through the state space for an optimal policy or path to a goal. Actions cause transitions from state to state, and value functions are computed over states. In what we call _plan-space planning_ , planning is instead viewed as a search through the space of plans. Operators transform one plan into another, and value functions, if any, are defined over the space of plans. Plan-space planning includes evolutionary methods and _partial-order planning_ , a popular kind of planning in artificial intelligence in which the ordering of steps is not completely determined at all stages of planning. Plan-space methods are difficult to apply efficiently to the stochastic optimal control problems that are the focus in reinforcement learning, and we do not consider them further (but see Section 15.6 for one application of reinforcement learning within plan-space planning).  \n",
      "The unified view we present in this chapter is that all state-space planning methods share a common structure, a structure that is also present in the  \n",
      "learning methods presented in this book. It takes the rest of the chapter to develop this view, but there are two basic ideas: (1) all state-space planning methods involve computing value functions as a key intermediate step toward improving the policy, and (2) they compute their value functions by backup operations applied to simulated experience. This common structure can be diagrammed as follows:  \n",
      "{\\mathrm{nodel~}}~{\\cfrac{\\longrightarrow}{\\longrightarrow}}~{\\underset{\\mathrm{experience}}{\\longrightarrow}}~{\\cfrac{\\mathrm{butated~}}{\\longrightarrow}}~{\\cfrac{\\mathrm{\\boldmath~\\patanys}}{\\longrightarrow}}~\\underbrace{\\cfrac{\\mathrm{\\boldmath~\\palues~}}}{\\longrightarrow}~~{\\cfrac{\\mathrm{\\boldmath~\\policy~}}}{\\ =}~~~~{\\mathrm{\\boldmath~\\polic~}}  \n",
      "Dynamic programming methods clearly fit this structure: they make sweeps through the space of states, generating for each state the distribution of possible transitions. Each distribution is then used to compute a backed-up value and update the state‚Äôs estimated value. In this chapter we argue that various other state-space planning methods also fit this structure, with individual methods di‚Üµering only in the kinds of backups they do, the order in which they do them, and in how long the backed-up information is retained.  \n",
      "Viewing planning methods in this way emphasizes their relationship to the learning methods that we have described in this book. The heart of both learning and planning methods is the estimation of value functions by backup operations. The di‚Üµerence is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment. Of course this di‚Üµerence leads to a number of other di‚Üµerences, for example, in how performance is assessed and in how flexibly experience can be generated. But the common structure means that many ideas and algorithms can be transferred between planning and learning. In particular, in many cases a learning algorithm can be substituted for the key backup step of a planning method. Learning methods require only experience as input, and in many cases they can be applied to simulated experience just as well as to real experience. Figure 8.1 shows a simple example of a planning method based on one-step tabular Q-learning and on random samples from a sample model. This method, which we call _random-sample one-step tabular Q-planning_ , converges to the optimal policy for the model under the same conditions that one-step tabular Q-learning converges to the optimal policy for the real environment (each state‚Äìaction pair must be selected an infinite number of times in Step 1, and _‚Üµ_ must decrease appropriately over time).  \n",
      "In addition to the unified view of planning and learning methods, a second theme in this chapter is the benefits of planning in small, incremental steps. This enables planning to be interrupted or redirected at any time with little wasted computation, which appears to be a key requirement for efficiently intermixing planning with acting and with learning of the model. More surprisingly, later in this chapter we present evidence that planning in very small  \n",
      "198 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "|Do|forever:||\n",
      "|---|---|---|\n",
      "||1. Select a state,_ S 2_ S, and an action,_ A 2_ A(_s_), at random||\n",
      "||2. Send_ S, A_ to a sample model, and obtain||\n",
      "||a sample next reward,_ R_, and a sample next state,_ S0_||\n",
      "||3. Apply one-step tabular Q-learning to_ S, A, R, S0_:||\n",
      "||_Q_(_S, A_) _Q_(_S, A_) +_ ‚Üµ_<br>‚á•<br>_R_ +_ Œ≥_ max_a Q_(_S0, a_)_ ‚àíQ_(_S, A_)|‚á§|  \n",
      "Figure 8.1: Random-sample one-step tabular Q-planning  \n",
      "steps may be the most efficient approach even on pure planning problems if the problem is too large to be solved exactly.  \n",
      "## **8.2 Integrating Planning, Acting, and Learning**  \n",
      "When planning is done on-line, while interacting with the environment, a number of interesting issues arise. New information gained from the interaction may change the model and thereby interact with planning. It may be desirable to customize the planning process in some way to the states or decisions currently under consideration, or expected in the near future. If decision-making and model-learning are both computation-intensive processes, then the available computational resources may need to be divided between them. To begin exploring these issues, in this section we present Dyna-Q, a simple architecture integrating the major functions needed in an on-line planning agent. Each function appears in Dyna-Q in a simple, almost trivial, form. In subsequent sections we elaborate some of the alternate ways of achieving each function and the trade-o‚Üµs between them. For now, we seek merely to illustrate the ideas and stimulate your intuition.  \n",
      "Within a planning agent, there are at least two roles for real experience: it can be used to improve the model (to make it more accurately match the real environment) and it can be used to directly improve the value function and policy using the kinds of reinforcement learning methods we have discussed in previous chapters. The former we call _model-learning_ , and the latter we call _direct reinforcement learning_ (direct RL). The possible relationships between experience, model, values, and policy are summarized in Figure 8.2. Each arrow shows a relationship of influence and presumed improvement. Note how experience can improve value and policy functions either directly or indirectly via the model. It is the latter, which is sometimes called _indirect reinforcement learning_ , that is involved in planning.  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "value/policy<br>acting<br>planning direct<br>RL<br>model experience<br>model<br>learning<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.2: Relationships among learning, planning, and acting.  \n",
      "Both direct and indirect methods have advantages and disadvantages. Indirect methods often make fuller use of a limited amount of experience and thus achieve a better policy with fewer environmental interactions. On the other hand, direct methods are much simpler and are not a‚Üµected by biases in the design of the model. Some have argued that indirect methods are always superior to direct ones, while others have argued that direct methods are responsible for most human and animal learning. Related debates in psychology and AI concern the relative importance of cognition as opposed to trial-and-error learning, and of deliberative planning as opposed to reactive decision-making. Our view is that the contrast between the alternatives in all these debates has been exaggerated, that more insight can be gained by recognizing the similarities between these two sides than by opposing them. For example, in this book we have emphasized the deep similarities between dynamic programming and temporal-di‚Üµerence methods, even though one was designed for planning and the other for modelfree learning.  \n",
      "Dyna-Q includes all of the processes shown in Figure 8.2‚Äîplanning, acting, model-learning, and direct RL‚Äîall occurring continually. The planning method is the random-sample one-step tabular Q-planning method given in Figure 8.1. The direct RL method is one-step tabular Q-learning. The modellearning method is also table-based and assumes the world is deterministic. After each transition _St, At Rt_ +1 _, St_ +1, the model records in its table entry for _St, At_ the prediction that _Rt_ +1 _, St_ +1 will deterministically follow. Thus, if the model is queried with a state‚Äìaction pair that has been experienced before, it simply returns the last-observed next state and next reward as its prediction. During planning, the Q-planning algorithm randomly samples only from state‚Äìaction pairs that have previously been experienced (in Step 1), so the model is never queried with a pair about which it has no information.  \n",
      "The overall architecture of Dyna agents, of which the Dyna-Q algorithm is  \n",
      "## 200 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Policy/value functions<br>planning update<br>direct RL simulated<br>update experience<br>real<br>experience<br>search<br>model<br>control<br>learning<br>Model<br>Environment<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.3: The general Dyna Architecture  \n",
      "one example, is shown in Figure 8.3. The central column represents the basic interaction between agent and environment, giving rise to a trajectory of real experience. The arrow on the left of the figure represents direct reinforcement learning operating on real experience to improve the value function and the policy. On the right are model-based processes. The model is learned from real experience and gives rise to simulated experience. We use the term _search control_ to refer to the process that selects the starting states and actions for the simulated experiences generated by the model. Finally, planning is achieved by applying reinforcement learning methods to the simulated experiences just as if they had really happened. Typically, as in Dyna-Q, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience. The reinforcement learning method is thus the ‚Äúfinal common path‚Äù for both learning and planning. Learning and planning are deeply integrated in the sense that they share almost all the same machinery, di‚Üµering only in the source of their experience.  \n",
      "Conceptually, planning, acting, model-learning, and direct RL occur simultaneously and in parallel in Dyna agents. For concreteness and implementation on a serial computer, however, we fully specify the order in which they occur within a time step. In Dyna-Q, the acting, model-learning, and direct RL processes require little computation, and we assume they consume just a fraction of the time. The remaining time in each step can be devoted to the planning process, which is inherently computation-intensive. Let us assume that there is time in each step, after acting, model-learning, and direct RL, to complete _n_ iterations (Steps 1‚Äì3) of the Q-planning algorithm. Figure 8.4 shows the complete algorithm for Dyna-Q.  \n",
      "Initialize _Q_ ( _s, a_ ) and _Model_ ( _s, a_ ) for all _s 2_ S and _a 2_ A( _s_ ) Do forever: (a) _S_ current (nonterminal) state (b) _A ‚úè_ -greedy( _S, Q_ ) (c) Execute action _A_ ; observe resultant reward, _R_ , and state, _S[0]_ (d) _Q_ ( _S, A_ ) _Q_ ( _S, A_ ) + _‚Üµ_ ‚á• _R_ + _Œ≥_ max _a Q_ ( _S[0] , a_ ) _‚àí Q_ ( _S, A_ )‚á§ (e) _Model_ ( _S, A_ ) _R, S[0]_ (assuming deterministic environment) (f) Repeat _n_ times: _S_ random previously observed state _A_ random action previously taken in _S R, S[0] Model_ ( _S, A_ ) _Q_ ( _S, A_ ) _Q_ ( _S, A_ ) + _‚Üµ_ ‚á• _R_ + _Œ≥_ max _a Q_ ( _S[0] , a_ ) _‚àí Q_ ( _S, A_ )‚á§  \n",
      "Figure 8.4: Dyna-Q Algorithm. _Model_ ( _s, a_ ) denotes the contents of the model (predicted next state and reward) for state‚Äìaction pair _s, a_ . Direct reinforcement learning, model-learning, and planning are implemented by steps (d), (e), and (f), respectively. If (e) and (f) were omitted, the remaining algorithm would be one-step tabular Q-learning.  \n",
      "**Example 8.1: Dyna Maze** Consider the simple maze shown inset in Figure 8.5. In each of the 47 states there are four actions, `up` , `down` , `right` , and `left` , which take the agent deterministically to the corresponding neighboring states, except when movement is blocked by an obstacle or the edge of the maze, in which case the agent remains where it is. Reward is zero on all transitions, except those into the goal state, on which it is +1. After reaching the goal state (G), the agent returns to the start state (S) to begin a new episode. This is a discounted, episodic task with _Œ≥_ = 0 _._ 95.  \n",
      "The main part of Figure 8.5 shows average learning curves from an experiment in which Dyna-Q agents were applied to the maze task. The initial action values were zero, the step-size parameter was _‚Üµ_ = 0 _._ 1, and the exploration parameter was _‚úè_ = 0 _._ 1. When selecting greedily among actions, ties were broken randomly. The agents varied in the number of planning steps, _n_ , they performed per real step. For each _n_ , the curves show the number of steps taken by the agent in each episode, averaged over 30 repetitions of the experiment. In each repetition, the initial seed for the random number generator was held constant across algorithms. Because of this, the first episode was exactly the same (about 1700 steps) for all values of _n_ , and its data are not shown in the figure. After the first episode, performance improved for all values of _n_ , but much more rapidly for larger values. Recall that the _n_ = 0 agent is a nonplanning agent, utilizing only direct reinforcement learning (onestep tabular Q-learning). This was by far the slowest agent on this problem, despite the fact that the parameter values ( _‚Üµ_ and _\"_ ) were optimized for it. The  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "G<br>800<br>S<br>600 actions<br>Steps 0 planning steps<br>per 400 (direct RL only)<br>episode<br>5 planning steps<br>50 planning steps<br>200<br>14<br>2 10 20 30 40 50<br>Episodes<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.5: A simple maze (inset) and the average learning curves for Dyna-Q agents varying in their number of planning steps ( _n_ ) per real step. The task is to travel from S to G as quickly as possible.  \n",
      "nonplanning agent took about 25 episodes to reach ( _\"_ -)optimal performance, whereas the _n_ = 5 agent took about five episodes, and the _n_ = 50 agent took only three episodes.  \n",
      "Figure 8.6 shows why the planning agents found the solution so much faster than the nonplanning agent. Shown are the policies found by the _n_ = 0 and _n_ = 50 agents halfway through the second episode. Without planning ( _n_ = 0), each episode adds only one additional step to the policy, and so only one step (the last) has been learned so far. With planning, again only one step is learned during the first episode, but here during the second episode an extensive policy has been developed that by the episode‚Äôs end will reach almost back to the start state. This policy is built by the planning process while the agent is still wandering near the start state. By the end of the third episode a complete optimal policy will have been found and perfect performance attained.  \n",
      "In Dyna-Q, learning and planning are accomplished by exactly the same algorithm, operating on real experience for learning and on simulated experience for planning. Because planning proceeds incrementally, it is trivial to intermix planning and acting. Both proceed as fast as they can. The agent is always reactive and always deliberative, responding instantly to the latest sensory information and yet always planning in the background. Also ongoing in the  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "WITHOUT PLANNING ( Nn =0) WITH PLANNING ( Nn =50)<br>G G<br>S S<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.6: Policies found by planning and nonplanning Dyna-Q agents halfway through the second episode. The arrows indicate the greedy action in each state; no arrow is shown for a state if all of its action values are equal. The black square indicates the location of the agent. background is the model-learning process. As new information is gained, the model is updated to better match reality. As the model changes, the ongoing planning process will gradually compute a di‚Üµerent way of behaving to match the new model.  \n",
      "## **8.3 When the Model Is Wrong**  \n",
      "In the maze example presented in the previous section, the changes in the model were relatively modest. The model started out empty, and was then filled only with exactly correct information. In general, we cannot expect to be so fortunate. Models may be incorrect because the environment is stochastic and only a limited number of samples have been observed, because the model was learned using function approximation that has generalized imperfectly, or simply because the environment has changed and its new behavior has not yet been observed. When the model is incorrect, the planning process will compute a suboptimal policy.  \n",
      "In some cases, the suboptimal policy computed by planning quickly leads to the discovery and correction of the modeling error. This tends to happen when the model is optimistic in the sense of predicting greater reward or better state transitions than are actually possible. The planned policy attempts to exploit these opportunities and in doing so discovers that they do not exist.  \n",
      "**Example 8.2: Blocking Maze** A maze example illustrating this relatively minor kind of modeling error and recovery from it is shown in Figure 8.7. Initially, there is a short path from start to goal, to the right of the barrier, as shown in the upper left of the figure. After 1000 time steps, the short path is ‚Äúblocked,‚Äù and a longer path is opened up along the left-hand side of  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "G G<br>S S<br>150<br>Dyna-Q+<br>Dyna-Q<br>Cumulative<br>reward<br>Dyna-AC<br>0<br>0 1000 2000 3000<br>Time steps<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.7: Average performance of Dyna agents on a blocking task. The left environment was used for the first 1000 steps, the right environment for the rest. Dyna-Q+ is Dyna-Q with an exploration bonus that encourages exploration. Dyna-AC is a Dyna agent that uses an actor‚Äìcritic learning method instead of Q-learning.  \n",
      "the barrier, as shown in upper right of the figure. The graph shows average cumulative reward for Dyna-Q and two other Dyna agents. The first part of the graph shows that all three Dyna agents found the short path within 1000 steps. When the environment changed, the graphs become flat, indicating a period during which the agents obtained no reward because they were wandering around behind the barrier. After a while, however, they were able to find the new opening and the new optimal behavior.  \n",
      "Greater difficulties arise when the environment changes to become _better_ than it was before, and yet the formerly correct policy does not reveal the improvement. In these cases the modeling error may not be detected for a long time, if ever, as we see in the next example.  \n",
      "**Example 8.3: Shortcut Maze** The problem caused by this kind of environmental change is illustrated by the maze example shown in Figure 8.8. Initially, the optimal path is to go around the left side of the barrier (upper left). After 3000 steps, however, a shorter path is opened up along the right side, without disturbing the longer path (upper right). The graph shows that two of the three Dyna agents never switched to the shortcut. In fact, they never realized that it existed. Their models said that there was no shortcut, so the more they planned, the less likely they were to step to the right  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "G G<br>S S<br>400<br>Dyna-Q<br>Dyna-Q+<br>Cumulative Dyna-AC<br>reward<br>0<br>0 3000 6000<br>Time steps<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.8: Average performance of Dyna agents on a shortcut task. The left environment was used for the first 3000 steps, the right environment for the rest.  \n",
      "and discover it. Even with an _\"_ -greedy policy, it is very unlikely that an agent will take so many exploratory actions as to discover the shortcut. The general problem here is another version of the conflict between exploration and exploitation. In a planning context, exploration means trying actions that improve the model, whereas exploitation means behaving in the optimal way given the current model. We want the agent to explore to find changes in the environment, but not so much that performance is greatly degraded. As in the earlier exploration/exploitation conflict, there probably is no solution that is both perfect and practical, but simple heuristics are often e‚Üµective.  \n",
      "The Dyna-Q+ agent that did solve the shortcut maze uses one such heuristic. This agent keeps track for each state‚Äìaction pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment. The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect. To encourage behavior that tests long-untried actions, a special ‚Äúbonus reward‚Äù is given on simulated experiences involving these actions. In particular, if the modeled reward for a transition is _R_ , and the transition has not been tried in _‚åß_ time steps, then planning backups are done as if that transition produced a reward of _R_ + _[p]_ ~~_‚åß_ ,~~ for some small __ . This encourages the agent to keep testing all accessible state transitions and even to plan  \n",
      "long sequences of actions in order to carry out such tests.[1] Of course all this testing has its cost, but in many cases, as in the shortcut maze, this kind of computational curiosity is well worth the extra exploration.  \n",
      "## **8.4 Prioritized Sweeping**  \n",
      "In the Dyna agents presented in the preceding sections, simulated transitions are started in state‚Äìaction pairs selected uniformly at random from all previously experienced pairs. But a uniform selection is usually not the best; planning can be much more efficient if simulated transitions and backups are focused on particular state‚Äìaction pairs. For example, consider what happens during the second episode of the first maze task (Figure 8.6). At the beginning of the second episode, only the state‚Äìaction pair leading directly into the goal has a positive value; the values of all other pairs are still zero. This means that it is pointless to back up along almost all transitions, because they take the agent from one zero-valued state to another, and thus the backups would have no e‚Üµect. Only a backup along a transition into the state just prior to the goal, or from it into the goal, will change any values. If simulated transitions are generated uniformly, then many wasteful backups will be made before stumbling onto one of the two useful ones. As planning progresses, the region of useful backups grows, but planning is still far less efficient than it would be if focused where it would do the most good. In the much larger problems that are our real objective, the number of states is so large that an unfocused search would be extremely inefficient.  \n",
      "This example suggests that search might be usefully focused by working _backward_ from goal states. Of course, we do not really want to use any methods specific to the idea of ‚Äúgoal state.‚Äù We want methods that work for general reward functions. Goal states are just a special case, convenient for stimulating intuition. In general, we want to work back not just from goal states but from any state whose value has changed. Assume that the values are initially correct given the model, as they were in the maze example prior to discovering the goal. Suppose now that the agent discovers a change in the environment and changes its estimated value of one state. Typically, this will imply that the values of many other states should also be changed, but the only useful onestep backups are those of actions that lead directly into the one state whose value has already been changed. If the values of these actions are updated,  \n",
      "> 1The Dyna-Q+ agent was changed in two other ways as well. First, actions that had never before been tried before from a state were allowed to be considered in the planning step (f) of Figure 8.4. Second, the initial model for such actions was that they would lead back to the same state with a reward of zero.  \n",
      "then the values of the predecessor states may change in turn. If so, then actions leading into them need to be backed up, and then _their_ predecessor states may have changed. In this way one can work backward from arbitrary states that have changed in value, either performing useful backups or terminating the propagation.  \n",
      "As the frontier of useful backups propagates backward, it often grows rapidly, producing many state‚Äìaction pairs that could usefully be backed up. But not all of these will be equally useful. The values of some states may have changed a lot, whereas others have changed little. The predecessor pairs of those that have changed a lot are more likely to also change a lot. In a stochastic environment, variations in estimated transition probabilities also contribute to variations in the sizes of changes and in the urgency with which pairs need to be backed up. It is natural to prioritize the backups according to a measure of their urgency, and perform them in order of priority. This is the idea behind _prioritized sweeping_ . A queue is maintained of every state‚Äìaction pair whose estimated value would change nontrivially if backed up, prioritized by the size of the change. When the top pair in the queue is backed up, the e‚Üµect on each of its predecessor pairs is computed. If the e‚Üµect is greater than some small threshold, then the pair is inserted in the queue with the new priority (if there is a previous entry of the pair in the queue, then insertion results in only the higher priority entry remaining in the queue). In this way the e‚Üµects of changes are efficiently propagated backward until quiescence. The full algorithm for the case of deterministic environments is given in Figure 8.9.  \n",
      "**Example 8.4: Prioritized Sweeping on Mazes** Prioritized sweeping has been found to dramatically increase the speed at which optimal solutions are found in maze tasks, often by a factor of 5 to 10. A typical example is shown in Figure 8.10. These data are for a sequence of maze tasks of exactly the same structure as the one shown in Figure 8.5, except that they vary in the grid resolution. Prioritized sweeping maintained a decisive advantage over unprioritized Dyna-Q. Both systems made at most _n_ = 5 backups per environmental interaction.  \n",
      "**Example 8.5: Rod Maneuvering** The objective in this task is to maneuver a rod around some awkwardly placed obstacles to a goal position in the fewest number of steps (Figure 8.11). The rod can be translated along its long axis or perpendicular to that axis, or it can be rotated in either direction around its center. The distance of each movement is approximately 1/20 of the work space, and the rotation increment is 10 degrees. Translations are deterministic and quantized to one of 20 _‚á•_ 20 positions. The figure shows the obstacles and the shortest solution from start to goal, found by prioritized sweeping. This problem is still deterministic, but has four actions and 14,400 potential states (some of these are unreachable because of the obstacles). This problem is  \n",
      "## 208 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "Initialize _Q_ ( _s, a_ ), _Model_ ( _s, a_ ), for all _s, a_ , and _PQueue_ to empty Do forever: (a) _S_ current (nonterminal) state (b) _A policy_ ( _S, Q_ ) (c) Execute action _A_ ; observe resultant reward, _R_ , and state, _S[0]_ (d) _Model_ ( _S, A_ ) _R, S[0]_ (e) _P  |R_ + _Œ≥_ max _a Q_ ( _S[0] , a_ ) _‚àí Q_ ( _S, A_ ) _|_ . (f) if _P > ‚úì_ , then insert _S, A_ into _PQueue_ with priority _P_ (g) Repeat _n_ times, while _PQueue_ is not empty: _S, A first_ ( _PQueue_ ) _R, S[0] Model_ ( _S, A_ ) _Q_ ( _S, A_ ) _Q_ ( _S, A_ ) + _‚Üµ_ ‚á• _R_ + _Œ≥_ max _a Q_ ( _S[0] , a_ ) _‚àí Q_ ( _S, A_ )‚á§ Repeat, for all _R_ ¬Ø predicted reward for ¬Ø _S,_[¬Ø] _A_[¬Ø] predicted to lead to _S,_ ¬Ø _A, S S_ : _P  |R_[¬Ø] + _Œ≥_ max _a Q_ ( _S, a_ ) _‚àí Q_ ( _S,_[¬Ø] _A_[¬Ø] ) _|_ . if _P > ‚úì_ then insert _S,_[¬Ø] _A_[¬Ø] into _PQueue_ with priority _P_  \n",
      "Figure 8.9: The prioritized sweeping algorithm for a deterministic environment.  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "10 [7]<br>10 [6] Dyna-Q<br>10 [5]<br>Backups<br>until<br>10 [4] prioritized<br>optimal sweeping<br>solution<br>10 [3]<br>10 [2]<br>10<br>0 47 94 186 376 752 1504 3008 6016<br>Gridworld size (#states)<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.10: Prioritized sweeping significantly shortens learning time on the Dyna maze task for a wide range of grid resolutions. Reprinted from Peng and Williams (1993).  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Goal<br>Start<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.11: A rod-maneuvering task and its solution by prioritized sweeping. Reprinted from Moore and Atkeson (1993).  \n",
      "probably too large to be solved with unprioritized methods.  \n",
      "Prioritized sweeping is clearly a powerful idea, but the algorithms that have been developed so far appear not to extend easily to more interesting cases. The greatest problem is that the algorithms appear to rely on the assumption of discrete states. When a change occurs at one state, these methods perform a computation on all the predecessor states that may have been a‚Üµected. If function approximation is used to learn the model or the value function, then a single backup could influence a great many other states. It is not apparent how these states could be identified or processed efficiently. On the other hand, the general idea of focusing search on the states believed to have changed in value, and then on their predecessors, seems intuitively to be valid in general. Additional research may produce more general versions of prioritized sweeping.  \n",
      "Extensions of prioritized sweeping to stochastic environments are relatively straightforward. The model is maintained by keeping counts of the number of times each state‚Äìaction pair has been experienced and of what the next states were. It is natural then to backup each pair not with a sample backup, as we have been using so far, but with a full backup, taking into account all possible next states and their probabilities of occurring.  \n",
      "## **8.5 Full vs. Sample Backups**  \n",
      "The examples in the previous sections give some idea of the range of possibilities for combining methods of learning and planning. In the rest of this chapter, we analyze some of the component ideas involved, starting with the relative advantages of full and sample backups.  \n",
      "Much of this book has been about di‚Üµerent kinds of backups, and we have considered a great many varieties. Focusing for the moment on one-step backups, they vary primarily along three binary dimensions. The first two dimensions are whether they back up state values or action values and whether they estimate the value for the optimal policy or for an arbitrary given policy. These two dimensions give rise to four classes of backups for approximating the four value functions, _q‚á§_ , _v‚á§_ , _q‚á°_ , and _v‚á°_ . The other binary dimension is whether the backups are _full_ backups, considering all possible events that might happen, or _sample_ backups, considering a single sample of what might happen. These three binary dimensions give rise to eight cases, seven of which correspond to specific algorithms, as shown in Figure 8.12. (The eighth case does not seem to correspond to any useful backup.) Any of these one-step backups can be used in planning methods. The Dyna-Q agents discussed earlier use _q‚á§_ sample backups, but they could just as well use _q‚á§_ full backups, or either full or sample _q‚á°_ backups. The Dyna-AC system uses _v‚á°_ sample backups together with a learning policy structure. For stochastic problems, prioritized sweeping is always done using one of the full backups.  \n",
      "When we introduced one-step sample backups in Chapter 6, we presented them as substitutes for full backups. In the absence of a distribution model, full backups are not possible, but sample backups can be done using sample transitions from the environment or a sample model. Implicit in that point of view is that full backups, if possible, are preferable to sample backups. But are they? Full backups certainly yield a better estimate because they are uncorrupted by sampling error, but they also require more computation, and computation is often the limiting resource in planning. To properly assess the relative merits of full and sample backups for planning we must control for their di‚Üµerent computational requirements.  \n",
      "For concreteness, consider the full and sample backups for approximating _q‚á§_ , and the special case of discrete states and actions, a table-lookup representation of the approximate value function, _Q_ , and a model in the form of estimated dynamics, ÀÜ _p_ ( _s[0] , r|s, a_ ). The full backup for a state‚Äìaction pair, _s, a_ , is:  \n",
      "{\\Gamma}  \n",
      "Q(s,a)\\leftarrow\\sum_{s^{\\prime},r}\\hat{p}(s^{\\prime},r|s,a)\\Bigl[r+\\gamma\\,\\operatorname*{max}_{a^{\\prime}}Q(s^{\\prime},a^{\\prime})\\Bigr].\\qquad\\qquad\\qquad\\qquad(8.1)  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "Value Full backups Sample backups<br>estimated (DP) (one-step TD)<br>s s<br>Vv œÄ !( s ) a aA<br>r rR<br>s' S's'<br>policy evaluation TD(0)<br>s<br>max<br>a<br>Vv * [*] ( s )<br>r<br>s'<br>value iteration<br>s,a s,a<br>r rR<br>s' s'<br>Qq œÄ [!] ( a , s ) S'<br>a' Aa' '<br>Q-policy  evaluation Sarsa<br>s,a s,a<br>r rR<br>s' s'<br>Qq * [*] ( a , s ) S'<br>max max<br>a' a'<br>Q-value iteration Q-learning<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.12: The one-step backups.  \n",
      "## 212 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "The corresponding sample backup for _s, a_ , given a sample next state and reward, _S[0]_ and _R_ (from the model), is the Q-learning-like update:  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "Q(s,a)\\leftarrow Q(s,a)+\\alpha\\left[R+\\gamma\\operatorname*{max}_{a^{\\prime}}Q(S^{\\prime},a^{\\prime})-Q(s,a)\\right],\\qquad\\qquad\\qquad(8.2)  \n",
      "where _‚Üµ_ is the usual positive step-size parameter.  \n",
      "The di‚Üµerence between these full and sample backups is significant to the extent that the environment is stochastic, specifically, to the extent that, given a state and action, many possible next states may occur with various probabilities. If only one next state is possible, then the full and sample backups given above are identical (taking _‚Üµ_ = 1). If there are many possible next states, then there may be significant di‚Üµerences. In favor of the full backup is that it is an exact computation, resulting in a new _Q_ ( _s, a_ ) whose correctness is limited only by the correctness of the _Q_ ( _s[0] , a[0]_ ) at successor states. The sample backup is in addition a‚Üµected by sampling error. On the other hand, the sample backup is cheaper computationally because it considers only one next state, not all possible next states. In practice, the computation required by backup operations is usually dominated by the number of state‚Äìaction pairs at which _Q_ is evaluated. For a particular starting pair, _s, a_ , let _b_ be the _branching factor_ (i.e., the number of possible next states, _s[0]_ , for which ÀÜ _p_ ( _s[0] |s, a_ ) _>_ 0). Then a full backup of this pair requires roughly _b_ times as much computation as a sample backup.  \n",
      "If there is enough time to complete a full backup, then the resulting estimate is generally better than that of _b_ sample backups because of the absence of sampling error. But if there is insufficient time to complete a full backup, then sample backups are always preferable because they at least make some improvement in the value estimate with fewer than _b_ backups. In a large problem with many state‚Äìaction pairs, we are often in the latter situation. With so many state‚Äìaction pairs, full backups of all of them would take a very long time. Before that we may be much better o‚Üµwith a few sample backups at many state‚Äìaction pairs than with full backups at a few pairs. Given a unit of computational e‚Üµort, is it better devoted to a few full backups or to _b_ times as many sample backups?  \n",
      "Figure 8.13 shows the results of an analysis that suggests an answer to this question. It shows the estimation error as a function of computation time for full and sample backups for a variety of branching factors, _b_ . The case considered is that in which all _b_ successor states are equally likely and in which the error in the initial estimate is 1. The values at the next states are assumed correct, so the full backup reduces the error to zero upon its completion. In this case, sample backups reduce the error according to ~~q~~ _b‚àíbt_ 1 where _t_ is the number of sample backups that have been performed (assuming  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "1<br>sample full<br>backups backups<br>b = 2 (branching factor)<br>RMS error<br>in value<br>b =10<br>estimate<br>b =100<br>b =1000<br>b =10,000<br>0<br>0 1 b 2 b<br>Number of                         computationsmax<br>a [‚Ä≤][ Q] [(] [s][‚Ä≤][, a][‚Ä≤] [)]<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.13: Comparison of efficiency of full and sample backups.  \n",
      "sample averages, i.e., _‚Üµ_ = 1 _/t_ ). The key observation is that for moderately large _b_ the error falls dramatically with a tiny fraction of _b_ backups. For these cases, many state‚Äìaction pairs could have their values improved dramatically, to within a few percent of the e‚Üµect of a full backup, in the same time that one state‚Äìaction pair could be backed up fully.  \n",
      "The advantage of sample backups shown in Figure 8.13 is probably an underestimate of the real e‚Üµect. In a real problem, the values of the successor states would themselves be estimates updated by backups. By causing estimates to be more accurate sooner, sample backups will have a second advantage in that the values backed up from the successor states will be more accurate. These results suggest that sample backups are likely to be superior to full backups on problems with large stochastic branching factors and too many states to be solved exactly.  \n",
      "## **8.6 Trajectory Sampling**  \n",
      "In this section we compare two ways of distributing backups. The classical approach, from dynamic programming, is to perform sweeps through the entire state (or state‚Äìaction) space, backing up each state (or state‚Äìaction pair) once per sweep. This is problematic on large tasks because there may not be time to complete even one sweep. In many tasks the vast majority of the states are irrelevant because they are visited only under very poor policies or with very low probability. Exhaustive sweeps implicitly devote equal time to all parts  \n",
      "## 214 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "of the state space rather than focusing where it is needed. As we discussed in Chapter 4, exhaustive sweeps and the equal treatment of all states that they imply are not necessary properties of dynamic programming. In principle, backups can be distributed any way one likes (to assure convergence, all states or state‚Äìaction pairs must be visited in the limit an infinite number of times), but in practice exhaustive sweeps are often used.  \n",
      "The second approach is to sample from the state or state‚Äìaction space according to some distribution. One could sample uniformly, as in the Dyna-Q agent, but this would su‚Üµer from some of the same problems as exhaustive sweeps. More appealing is to distribute backups according to the on-policy distribution, that is, according to the distribution observed when following the current policy. One advantage of this distribution is that it is easily generated; one simply interacts with the model, following the current policy. In an episodic task, one starts in the start state (or according to the starting-state distribution) and simulates until the terminal state. In a continuing task, one starts anywhere and just keeps simulating. In either case, sample state transitions and rewards are given by the model, and sample actions are given by the current policy. In other words, one simulates explicit individual trajectories and performs backups at the state or state‚Äìaction pairs encountered along the way. We call this way of generating experience and backups _trajectory sampling_ .  \n",
      "It is hard to imagine any efficient way of distributing backups according to the on-policy distribution other than by trajectory sampling. If one had an explicit representation of the on-policy distribution, then one could sweep through all states, weighting the backup of each according to the on-policy distribution, but this leaves us again with all the computational costs of exhaustive sweeps. Possibly one could sample and update individual state‚Äìaction pairs from the distribution, but even if this could be done efficiently, what benefit would this provide over simulating trajectories? Even knowing the on-policy distribution in an explicit form is unlikely. The distribution changes whenever the policy changes, and computing the distribution requires computation comparable to a complete policy evaluation. Consideration of such other possibilities makes trajectory sampling seem both efficient and elegant. Is the on-policy distribution of backups a good one? Intuitively it seems like a good choice, at least better than the uniform distribution. For example, if you are learning to play chess, you study positions that might arise in real games, not random positions of chess pieces. The latter may be valid states, but to be able to accurately value them is a di‚Üµerent skill from evaluating positions in real games. We will also see in Chapter 9 that the on-policy distribution has significant advantages when function approximation is used. Whether or not function approximation is used, one might expect on-policy  \n",
      "focusing to significantly improve the speed of planning.  \n",
      "Focusing on the on-policy distribution could be beneficial because it causes vast, uninteresting parts of the space to be ignored, or it could be detrimental because it causes the same old parts of the space to be backed up over and over. We conducted a small experiment to assess the e‚Üµect empirically. To isolate the e‚Üµect of the backup distribution, we used entirely one-step full tabular backups, as defined by (8.1). In the _uniform_ case, we cycled through all state‚Äìaction pairs, backing up each in place, and in the _on-policy_ case we simulated episodes, backing up each state‚Äìaction pair that occurred under the current _‚úè_ -greedy policy ( _‚úè_ = 0 _._ 1). The tasks were undiscounted episodic tasks, generated randomly as follows. From each of the _|_ S _|_ states, two actions were possible, each of which resulted in one of _b_ next states, all equally likely, with a di‚Üµerent random selection of _b_ states for each state‚Äìaction pair. The branching factor, _b_ , was the same for all state‚Äìaction pairs. In addition, on all transitions there was a 0.1 probability of transition to the terminal state, ending the episode. We used episodic tasks to get a clear measure of the quality of the current policy. At any point in the planning process one can stop and exhaustively compute _v‚á°_ Àú( _s_ 0), the true value of the start state under the greedy policy, Àú _‚á°_ , given the current action-value function _Q_ , as an indication of how well the agent would do on a new episode on which it acted greedily (all the while assuming the model is correct).  \n",
      "The upper part of Figure 8.14 shows results averaged over 200 sample tasks with 1000 states and branching factors of 1, 3, and 10. The quality of the policies found is plotted as a function of the number of full backups completed. In all cases, sampling according to the on-policy distribution resulted in faster planning initially and retarded planning in the long run. The e‚Üµect was stronger, and the initial period of faster planning was longer, at smaller branching factors. In other experiments, we found that these e‚Üµects also became stronger as the number of states increased. For example, the lower part of Figure 8.14 shows results for a branching factor of 1 for tasks with 10,000 states. In this case the advantage of on-policy focusing is large and long-lasting.  \n",
      "All of these results make sense. In the short term, sampling according to the on-policy distribution helps by focusing on states that are near descendants of the start state. If there are many states and a small branching factor, this e‚Üµect will be large and long-lasting. In the long run, focusing on the on-policy distribution may hurt because the commonly occurring states all already have their correct values. Sampling them is useless, whereas sampling other states may actually perform some useful work. This presumably is why the exhaustive, unfocused approach does better in the long run, at least for small problems. These results are not conclusive because they are only for  \n",
      "## 216 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "on-policy<br>3 b =1<br>1000 STATES<br>uniform<br>Value of 2<br>start state uniform<br>under<br>b =3<br>on-policy<br>greedy<br>policy 1<br>uniform<br>b =10<br>on-policy<br>0<br>0 5,000 10,000 15,000 20,000<br>Computation time, in full backups<br>on-policy<br>b =1<br>3<br>10,000 STATES<br>uniform<br>Value of<br>start state 2<br>under<br>greedy<br>policy<br>1<br>0<br>0 50,000 100,000 150,000 200,000<br>Computation time, in full backups<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.14: Relative efficiency of backups distributed uniformly across the state space versus focused on simulated on-policy trajectories. Results are for randomly generated tasks of two sizes and various branching factors, _b_ .  \n",
      "problems generated in a particular, random way, but they do suggest that sampling according to the on-policy distribution can be a great advantage for large problems, in particular for problems in which a small subset of the state‚Äìaction space is visited under the on-policy distribution.  \n",
      "## **8.7 Heuristic Search**  \n",
      "The predominant state-space planning methods in artificial intelligence are collectively known as _heuristic search_ . Although superficially di‚Üµerent from the planning methods we have discussed so far in this chapter, heuristic search and some of its component ideas can be combined with these methods in useful ways. Unlike these methods, heuristic search is not concerned with changing the approximate, or ‚Äúheuristic,‚Äù value function, but only with making improved action selections given the current value function. In other words, heuristic search is planning as part of a policy computation.  \n",
      "In heuristic search, for each state encountered, a large tree of possible continuations is considered. The approximate value function is applied to the leaf nodes and then backed up toward the current state at the root. The backing up within the search tree is just the same as in the max-backups (those for _v‚á§_ and _q‚á§_ ) discussed throughout this book. The backing up stops at the state‚Äìaction nodes for the current state. Once the backed-up values of these nodes are computed, the best of them is chosen as the current action, and then all backed-up values are discarded.  \n",
      "In conventional heuristic search no e‚Üµort is made to save the backed-up values by changing the approximate value function. In fact, the value function is generally designed by people and never changed as a result of search. However, it is natural to consider allowing the value function to be improved over time, using either the backed-up values computed during heuristic search or any of the other methods presented throughout this book. In a sense we have taken this approach all along. Our greedy and _\"_ -greedy action-selection methods are not unlike heuristic search, albeit on a smaller scale. For example, to compute the greedy action given a model and a state-value function, we must look ahead from each possible action to each possible next state, backup the rewards and estimated values, and then pick the best action. Just as in conventional heuristic search, this process computes backed-up values of the possible actions, but does not attempt to save them. Thus, heuristic search can be viewed as an extension of the idea of a greedy policy beyond a single step.  \n",
      "The point of searching deeper than one step is to obtain better action  \n",
      "## 218 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "selections. If one has a perfect model and an imperfect action-value function, then in fact deeper search will usually yield better policies.[2] Certainly, if the search is all the way to the end of the episode, then the e‚Üµect of the imperfect value function is eliminated, and the action determined in this way must be optimal. If the search is of sufficient depth _k_ such that _Œ≥[k]_ is very small, then the actions will be correspondingly near optimal. On the other hand, the deeper the search, the more computation is required, usually resulting in a slower response time. A good example is provided by Tesauro‚Äôs grandmaster-level backgammon player, TD-Gammon (Section 15.1). This system used TD( _Œª_ ) to learn an afterstate value function through many games of self-play, using a form of heuristic search to make its moves. As a model, TD-Gammon used a priori knowledge of the probabilities of dice rolls and the assumption that the opponent always selected the actions that TD-Gammon rated as best for it. Tesauro found that the deeper the heuristic search, the better the moves made by TD-Gammon, but the longer it took to make each move. Backgammon has a large branching factor, yet moves must be made within a few seconds. It was only feasible to search ahead selectively a few steps, but even so the search resulted in significantly better action selections.  \n",
      "So far we have emphasized heuristic search as an action-selection technique, but this may not be its most important aspect. Heuristic search also suggests ways of selectively distributing backups that may lead to better and faster approximation of the optimal value function. A great deal of research on heuristic search has been devoted to making the search as efficient as possible. The search tree is grown selectively, deeper along some lines and shallower along others. For example, the search tree is often deeper for the actions that seem most likely to be best, and shallower for those that the agent will probably not want to take anyway. Can we use a similar idea to improve the distribution of backups? Perhaps it can be done by preferentially updating state‚Äìaction pairs whose values appear to be close to the maximum available from the state. To our knowledge, this and other possibilities for distributing backups based on ideas borrowed from heuristic search have not yet been explored.  \n",
      "We should not overlook the most obvious way in which heuristic search focuses backups: on the current state. Much of the e‚Üµectiveness of heuristic search is due to its search tree being tightly focused on the states and actions that might immediately follow the current state. You may spend more of your life playing chess than checkers, but when you play checkers, it pays to think about checkers and about your particular checkers position, your likely next moves, and successor positions. However you select actions, it is these states and actions that are of highest priority for backups and where you  \n",
      "> 2There are interesting exceptions to this. See, e.g., Pearl (1984).  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "3 10<br>1 2 6 8 9<br>4 5 7<br>**----- End of picture text -----**<br>  \n",
      "Figure 8.15: The deep backups of heuristic search can be implemented as a sequence of one-step backups (shown here outlined). The ordering shown is for a selective depth-first search.  \n",
      "most urgently want your approximate value function to be accurate. Not only should your computation be preferentially devoted to imminent events, but so should your limited memory resources. In chess, for example, there are far too many possible positions to store distinct value estimates for each of them, but chess programs based on heuristic search can easily store distinct estimates for the millions of positions they encounter looking ahead from a single position. This great focusing of memory and computational resources on the current decision is presumably the reason why heuristic search can be so e‚Üµective.  \n",
      "The distribution of backups can be altered in similar ways to focus on the current state and its likely successors. As a limiting case we might use exactly the methods of heuristic search to construct a search tree, and then perform the individual, one-step backups from bottom up, as suggested by Figure 8.15. If the backups are ordered in this way and a table-lookup representation is used, then exactly the same backup would be achieved as in heuristic search. Any state-space search can be viewed in this way as the piecing together of a large number of individual one-step backups. Thus, the performance improvement observed with deeper searches is not due to the use of multistep backups as such. Instead, it is due to the focus and concentration of backups on states and actions immediately downstream from the current state. By devoting a large amount of computation specifically relevant to the candidate actions, a much better decision can be made than by relying on unfocused backups.  \n",
      "## 220 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_ **8.8 Monte Carlo Tree Search**  \n",
      "## **8.9 Summary**  \n",
      "We have presented a perspective emphasizing the surprisingly close relationships between planning optimal behavior and learning optimal behavior. Both involve estimating the same value functions, and in both cases it is natural to update the estimates incrementally, in a long series of small backup operations. This makes it straightforward to integrate learning and planning processes simply by allowing both to update the same estimated value function. In addition, any of the learning methods can be converted into planning methods simply by applying them to simulated (model-generated) experience rather than to real experience. In this case learning and planning become even more similar; they are possibly identical algorithms operating on two di‚Üµerent sources of experience.  \n",
      "It is straightforward to integrate incremental planning methods with acting and model-learning. Planning, acting, and model-learning interact in a circular fashion (Figure 8.2), each producing what the other needs to improve; no other interaction among them is either required or prohibited. The most natural approach is for all processes to proceed asynchronously and in parallel. If the processes must share computational resources, then the division can be handled almost arbitrarily‚Äîby whatever organization is most convenient and efficient for the task at hand.  \n",
      "In this chapter we have touched upon a number of dimensions of variation among state-space planning methods. One of the most important of these is the distribution of backups, that is, of the focus of search. Prioritized sweeping focuses on the predecessors of states whose values have recently changed. Heuristic search applied to reinforcement learning focuses, inter alia, on the successors of the current state. Trajectory sampling is a convenient way of focusing on the on-policy distribution. All of these approaches can significantly speed planning and are current topics of research.  \n",
      "Another interesting dimension of variation is the size of backups. The smaller the backups, the more incremental the planning methods can be. Among the smallest backups are one-step sample backups. We presented one study suggesting that one-step sample backups may be preferable on very large problems. A related issue is the depth of backups. In many cases deep backups can be implemented as sequences of shallow backups.  \n",
      "## **Bibliographical and Historical Remarks**  \n",
      "- **8.1** The overall view of planning and learning presented here has developed gradually over a number of years, in part by the authors (Sutton, 1990, 1991a, 1991b; Barto, Bradtke, and Singh, 1991, 1995; Sutton and Pinette, 1985; Sutton and Barto, 1981b); it has been strongly influenced by Agre and Chapman (1990; Agre 1988), Bertsekas and Tsitsiklis (1989), Singh (1993), and others. The authors were also strongly influenced by psychological studies of latent learning (Tolman, 1932) and by psychological views of the nature of thought (e.g., Galanter and Gerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978).  \n",
      "- **8.2‚Äì3** The terms _direct_ and _indirect_ , which we use to describe di‚Üµerent kinds of reinforcement learning, are from the adaptive control literature (e.g., Goodwin and Sin, 1984), where they are used to make the same kind of distinction. The term _system identification_ is used in adaptive control for what we call _model-learning_ (e.g., Goodwin and Sin, 1984; Ljung and S¬®oderstrom, 1983; Young, 1984). The Dyna architecture is due to Sutton (1990), and the results in these sections are based on results reported there.  \n",
      "- **8.4** Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson (1993) and Peng and Williams (1993). The results in Figure 8.10 are due to Peng and Williams (1993). The results in Figure 8.11 are due to Moore and Atkeson.  \n",
      "- **8.5** This section was strongly influenced by the experiments of Singh (1993).  \n",
      "- **8.7** For further reading on heuristic search, the reader is encouraged to consult texts and surveys such as those by Russell and Norvig (2009) and Korf (1988). Peng and Williams (1993) explored a forward focusing of backups much as is suggested in this section.  \n",
      "## **Exercises**  \n",
      "**Exercise 8.1** There is no Exercise 8.1.  \n",
      "**Exercise 8.2** Why did the Dyna agent with exploration bonus, Dyna-Q+, perform better in the first phase as well as in the second phase of the blocking and shortcut experiments?  \n",
      "## 222 _CHAPTER 8. PLANNING AND LEARNING WITH TABULAR METHODS_  \n",
      "**Exercise 8.3** Careful inspection of Figure 8.8 reveals that the di‚Üµerence between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason for this?  \n",
      "**Exercise 8.4 (programming)** The exploration bonus described above actually changes the estimated values of states and actions. Is this necessary? Suppose the bonus _[p]_ ~~_‚åß_~~ was used not in backups, but solely in action selection. That is, suppose the action selected was always that for which _Q_ ( _S, a_ )+ _[p]_ ~~_‚åß_~~ _Sa_ was maximal. Carry out a gridworld experiment that tests and illustrates the strengths and weaknesses of this alternate approach.  \n",
      "**Exercise 8.5** The analysis above assumed that all of the _b_ possible next states were equally likely to occur. Suppose instead that the distribution was highly skewed, that some of the _b_ states were much more likely to occur than most. Would this strengthen or weaken the case for sample backups over full backups? Support your answer. **Exercise 8.6** Some of the graphs in Figure 8.14 seem to be scalloped in their early portions, particularly the upper graph for _b_ = 1 and the uniform distribution. Why do you think this is? What aspects of the data shown support your hypothesis?  \n",
      "**Exercise 8.7 (programming)** If you have access to a moderately large computer, try replicating the experiment whose results are shown in the lower part of Figure 8.14. Then try the same experiment but with _b_ = 3. Discuss the meaning of your results.  \n",
      "## **Part II**  \n",
      "## **Approximate Solution Methods**  \n",
      "## **Chapter 9**  \n",
      "## **On-policy Approximation of Action Values**  \n",
      "We have so far assumed that our estimates of value functions are represented as a table with one entry for each state or for each state‚Äìaction pair. This is a particularly clear and instructive case, but of course it is limited to tasks with small numbers of states and actions. The problem is not just the memory needed for large tables, but the time and data needed to fill them accurately. In other words, the key issue is that of _generalization_ . How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset?  \n",
      "This is a severe problem. In many tasks to which we would like to apply reinforcement learning, most states encountered will never have been experienced exactly before. This will almost always be the case when the state or action spaces include continuous variables or complex sensations, such as a visual image. The only way to learn anything at all on these tasks is to generalize from previously experienced states to ones that have never been seen.  \n",
      "Fortunately, generalization from examples has already been extensively studied, and we do not need to invent totally new methods for use in reinforcement learning. To a large extent we need only combine reinforcement learning methods with existing generalization methods. The kind of generalization we require is often called _function approximation_ because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function. Function approximation is an instance of _supervised learning_ , the primary topic studied in machine learning, artificial neural networks, pattern recognition, and statistical curve fitting. In principle, any of the methods studied in these fields can be used in  \n",
      "## 226 _CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES_  \n",
      "reinforcement learning as described in this chapter.  \n",
      "## **9.1 Value Prediction with Function Approximation**  \n",
      "As usual, we begin with the prediction problem of estimating the state-value function _v‚á°_ from experience generated using policy _‚á°_ . The novelty in this chapter is that the approximate value function is represented not as a table but as a parameterized functional form with parameter vector **w** _2_ R _[n]_ . We will write ÀÜ _v_ ( _s,_ **w** ) _‚á° v‚á°_ ( _s_ ) for the approximated value of state _s_ given weight vector **w** . For example, ÀÜ _v_ might be the function computed by an artificial neural network, with **w** the vector of connection weights. By adjusting the weights, any of a wide range of di‚Üµerent functions ÀÜ _v_ can be implemented by the network. Or ÀÜ _v_ might be the function computed by a decision tree, where **w** is all the parameters defining the split points and leaf values of the tree. Typically, the number of parameters _n_ (the number of components of **w** ) is much less than the number of states, and changing one parameter changes the estimated value of many states. Consequently, when a single state is backed up, the change generalizes from that state to a‚Üµect the values of many other states.  \n",
      "All of the prediction methods covered in this book have been described as backups, that is, as updates to an estimated value function that shift its value at particular states toward a ‚Äúbacked-up value‚Äù for that state. Let us refer to an individual backup by the notation _s 7! v_ , where _s_ is the state backed up and _v_ is the backed-up value, or target, that _s_ ‚Äôs estimated value is shifted toward. For example, the Monte Carlo backup for value prediction is _St 7! Gt_ , the TD(0) backup is _St 7! Rt_ +1 + _Œ≥v_ ÀÜ( _St_ +1 _,_ **w** _t_ ), and the general TD( _Œª_ ) backup is _St 7! G[Œª] t_[.] In the DP policy evaluation backup _s 7!_ E _‚á°_ [ _Rt_ +1 + _Œ≥v_ ÀÜ( _St_ +1 _,_ **w** _t_ ) _| St_ = _s_ ], an arbitrary state _s_ is backed up, whereas in the the other cases the state, _St_ , encountered in (possibly simulated) experience is backed up.  \n",
      "It is natural to interpret each backup as specifying an example of the desired input‚Äìoutput behavior of the estimated value function. In a sense, the backup _s 7! v_ means that the estimated value for state _s_ should be more like _v_ . Up to now, the actual update implementing the backup has been trivial: the table entry for _s_ ‚Äôs estimated value has simply been shifted a fraction of the way toward _v_ . Now we permit arbitrarily complex and sophisticated function approximation methods to implement the backup. The normal inputs to these methods are examples of the desired input‚Äìoutput behavior of the function  \n",
      "they are trying to approximate. We use these methods for value prediction simply by passing to them the _s 7! v_ of each backup as a training example. We then interpret the approximate function they produce as an estimated value function.  \n",
      "Viewing each backup as a conventional training example in this way enables us to use any of a wide range of existing function approximation methods for value prediction. In principle, we can use any method for supervised learning from examples, including artificial neural networks, decision trees, and various kinds of multivariate regression. However, not all function approximation methods are equally well suited for use in reinforcement learning. The most sophisticated neural network and statistical methods all assume a static training set over which multiple passes are made. In reinforcement learning, however, it is important that learning be able to occur on-line, while interacting with the environment or with a model of the environment. To do this requires methods that are able to learn efficiently from incrementally acquired data. In addition, reinforcement learning generally requires function approximation methods able to handle nonstationary target functions (target functions that change over time). For example, in GPI control methods we often seek to learn _q‚á°_ while _‚á°_ changes. Even if the policy remains the same, the target values of training examples are nonstationary if they are generated by bootstrapping methods (DP and TD). Methods that cannot easily handle such nonstationarity are less suitable for reinforcement learning.  \n",
      "What performance measures are appropriate for evaluating function approximation methods? Most supervised learning methods seek to minimize the root-mean-squared error (RMSE) over some distribution over the inputs. In our value prediction problem, the inputs are states and the target function is the true value function _v‚á°_ , so RMSE for an approximation ÀÜ _v_ , using parameter **w** , is  \n",
      "{\\mathrm{RMSE}}(\\mathbf{w})={\\sqrt{\\sum_{s\\in8}d(s)\\left[v_{\\pi}(s)-{\\hat{v}}(s,\\mathbf{w})\\right]^{2}}},\\qquad\\qquad\\qquad\\qquad(9.1)  \n",
      "where _d_ : S _!_ [0 _,_ 1], such that[P] _s[d]_[(] _[s]_[) = 1, is a distribution over the states] specifying the relative importance of errors in di‚Üµerent states. This distribution is important because it is usually not possible to reduce the error to zero at all states. After all, there are generally far more states than there are components to **w** . The flexibility of the function approximator is thus a scarce resource. Better approximation at some states can be gained, generally, only at the expense of worse approximation at other states. The distribution specifies how these trade-o‚Üµs should be made.  \n",
      "The distribution _d_ is also usually the distribution from which the states in the training examples are drawn, and thus the distribution of states at which  \n",
      "## 228 _CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES_  \n",
      "backups are done. If we wish to minimize error over a certain distribution of states, then it makes sense to train the function approximator with examples from that same distribution. For example, if you want a uniform level of error over the entire state set, then it makes sense to train with backups distributed uniformly over the entire state set, such as in the exhaustive sweeps of some DP methods. Henceforth, let us assume that the distribution of states at which backups are done and the distribution that weights errors, _d_ , are the same.  \n",
      "A distribution of particular interest is the one describing the frequency with which states are encountered while the agent is interacting with the environment and selecting actions according to _‚á°_ , the policy whose value function we are approximating. We call this the _on-policy distribution_ , in part because it is the distribution of backups in on-policy control methods. Minimizing error over the on-policy distribution focuses function approximation resources on the states that actually occur while following the policy, ignoring those that never occur. The on-policy distribution is also the one for which it is easiest to get training examples using Monte Carlo or TD methods. These methods generate backups from sample experience using the policy _‚á°_ . Because a backup is generated for each state encountered in the experience, the training examples available are naturally distributed according to the on-policy distribution. Stronger convergence results are available for the on-policy distribution than for other distributions, as we discuss later.  \n",
      "It is not completely clear that we should care about minimizing the RMSE. Our goal in value prediction is potentially di‚Üµerent because our ultimate purpose is to use the predictions to aid in finding a better policy. The best predictions for that purpose are not necessarily the best for minimizing RMSE. However, it is not yet clear what a more useful alternative goal for value prediction might be. For now, we continue to focus on RMSE.  \n",
      "An ideal goal in terms of RMSE would be to find a _global optimum_ , a parameter vector **w** _[‚á§]_ for which RMSE( **w** _[‚á§]_ ) __ RMSE( **w** ) for all possible **w** . Reaching this goal is sometimes possible for simple function approximators such as linear ones, but is rarely possible for complex function approximators such as artificial neural networks and decision trees. Short of this, complex function approximators may seek to converge instead to a _local optimum_ , a parameter vector **w** _[‚á§]_ for which RMSE( **w** _[‚á§]_ ) __ RMSE( **w** ) for all **w** in some neighborhood of **w** _[‚á§]_ . Although this guarantee is only slightly reassuring, it is typically the best that can be said for nonlinear function approximators. For many cases of interest in reinforcement learning, convergence to an optimum, or even all bound of an optimum may still be achieved with some methods. Other methods may in fact diverge, with their RMSE approaching infinity in the limit.  \n",
      "## _9.2. GRADIENT-DESCENT METHODS_  \n",
      "In this section we have outlined a framework for combining a wide range of reinforcement learning methods for value prediction with a wide range of function approximation methods, using the backups of the former to generate training examples for the latter. We have also outlined a range of RMSE performance measures to which these methods may aspire. The range of possible methods is far too large to cover all, and anyway too little is known about most of them to make a reliable evaluation or recommendation. Of necessity, we consider only a few possibilities. In the rest of this chapter we focus on function approximation methods based on gradient principles, and on linear gradient-descent methods in particular. We focus on these methods in part because we consider them to be particularly promising and because they reveal key theoretical issues, but also because they are simple and our space is limited. If we had another chapter devoted to function approximation, we would also cover at least memory-based and decision-tree methods.  \n",
      "## **9.2 Gradient-Descent Methods**  \n",
      "We now develop in detail one class of learning methods for function approximation in value prediction, those based on gradient descent. Gradient-descent methods are among the most widely used of all function approximation methods and are particularly well suited to online reinforcement learning.  \n",
      "In gradient-descent methods, the parameter vector is a column vector with a fixed number of real valued components, **w** = ( _w_ 1 _, w_ 2 _, . . . , wn_ ) _[>]_ (the _[>]_ here denotes transpose), and the approximate value function ÀÜ _v_ ( _s,_ **w** ) is a smooth di‚Üµerentiable function of **w** for all _s 2_ S. We will be updating **w** at each of a series of discrete time steps, _t_ = 1 _,_ 2 _,_ 3 _, . . ._ , so we will need a notation **w** _t_ for the weight vector at each step. For now, let us assume that, on each step, we observe a new example _St 7! v‚á°_ ( _St_ ) consisting of a (possibly randomly selected) state _St_ and its true value under the policy. These states might be successive states from an interaction with the environment, but for now we do not assume so. Even though we are given the exact, correct values, _v‚á°_ ( _St_ ) for each _St_ , there is still a difficult problem because our function approximator has limited resources and thus limited resolution. In particular, there is generally no **w** that gets all the states, or even all the examples, exactly correct. In addition, we must generalize to all the other states that have not appeared in examples.  \n",
      "We assume that states appear in examples with the same distribution, _d_ , over which we are trying to minimize the RMSE as given by (9.1). A good strategy in this case is to try to minimize error on the observed examples. Gradient-descent methods do this by adjusting the parameter vector after  \n",
      "each example by a small amount in the direction that would most reduce the error on that example:  \n",
      "OMITTED IMAGE  \n",
      "2  \n",
      "\\begin{array}{l l l}{{\\mathbf{w}_{t+1}}}&{{=}}&{{\\mathbf{w}_{t}-{\\frac{1}{2}}\\alpha\\nabla\\Bigl[v_{\\pi}(S_{t})-{\\dot{v}}(S_{t},\\mathbf{w}_{t})\\Bigr]^{2}\\qquad}}&{{\\qquad\\qquad}}\\\\ {{}}&{{=}}&{{\\mathbf{w}_{t}+\\alpha\\Bigl[v_{\\pi}(S_{t})-{\\dot{v}}(S_{t},\\mathbf{w}_{t})\\Bigr]\\nabla{\\dot{v}}(S_{t},\\mathbf{w}_{t}),\\qquad}}&{{\\qquad}}&{{\\qquad}}\\end{array}\\qquad\\qquad(9.2)  \n",
      "where _‚Üµ_ is a positive step-size parameter, and _rf_ ( **w** _t_ ), for any expression _f_ ( **w** _t_ ), denotes the vector of partial derivatives with respect to the components of the weight vector:  \n",
      "_@f_ ( **w** _t_ )  \n",
      "_>_  \n",
      "\\frac{\\partial f(\\mathbf{w}_{t})}{\\partial w_{t,1}},\\frac{\\partial f(\\mathbf{w}_{t})}{\\partial w_{t,2}},\\cdot\\cdot\\cdot,\\frac{\\partial f(\\mathbf{w}_{t})}{\\partial w_{t,n}}\\right\\}^{1}\\;.  \n",
      "This derivative vector is the _gradient_ of _f_ with respect to **w** _t_ . This kind of method is called _gradient descent_ because the overall step in **w** _t_ is proportional to the negative gradient of the example‚Äôs squared error. This is the direction in which the error falls most rapidly.  \n",
      "It may not be immediately apparent why only a small step is taken in the direction of the gradient. Could we not move all the way in this direction and completely eliminate the error on the example? In many cases this could be done, but usually it is not desirable. Remember that we do not seek or expect to find a value function that has zero error on all states, but only an approximation that balances the errors in di‚Üµerent states. If we completely corrected each example in one step, then we would not find such a balance. In fact, the convergence results for gradient methods assume that the step-size parameter decreases over time. If it decreases in such a way as to satisfy the standard stochastic approximation conditions (2.7), then the gradient-descent method (9.2) is guaranteed to converge to a local optimum.  \n",
      "We turn now to the case in which the target output, _Vt_ , of the _t_ th training example, _St 7! Vt_ , is not the true value, _v‚á°_ ( _St_ ), but some, possibly random, approximation of it. For example, _Vt_ might be a noise-corrupted version of _v‚á°_ ( _St_ ), or it might be one of the backed-up values using ÀÜ _v_ mentioned in the previous section. In such cases we cannot perform the exact update (9.2) because _v‚á°_ ( _St_ ) is unknown, but we can approximate it by substituting _Vt_ in place of _v‚á°_ ( _St_ ). This yields the general gradient-descent method for state-value prediction:  \n",
      "\\mathbf{w}_{t+1}=\\mathbf{w}_{t}+\\alpha\\left[V_{t}-{\\hat{v}}(S_{t},\\mathbf{w}_{t})\\right]\\nabla{\\hat{v}}(S_{t},\\mathbf{w}_{t}).  \n",
      "If _Vt_ is an _unbiased_ estimate, that is, if E[ _Vt_ ] = _v‚á°_ ( _St_ ), for each _t_ , then **w** _t_ is guaranteed to converge to a local optimum under the usual stochastic approximation conditions (2.7) for decreasing the step-size parameter _‚Üµ_ .  \n",
      "For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy _‚á°_ . Let _Gt_ denote the return following each state, _St_ . Because the true value of a state is the expected value of the return following it, the Monte Carlo target _Vt_ = _Gt_ is by definition an unbiased estimate of _v‚á°_ ( _St_ ). With this choice, the general gradient-descent method (9.3) converges to a locally optimal approximation to _v‚á°_ ( _St_ ). Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to find a locally optimal solution.  \n",
      "\\begin{array}{c}{{\\mathrm{Similariv}\\mathrm{~we~can~use~n.se~n.stera~naa~ther~averages~for~\\ror}\\mathrm{~werages~for~\\areturs~ser}}}\\\\ {{\\mathrm{example,~the~grient-desescent~for~\\lereturn},~V_{t}=G_{t}^{\\lambda},\\;a s i t.}}}\\\\ {{\\mathrm{w}_{t+1}=\\mathrm{w}_{t}+\\alpha\\left[G_{t}^{\\lambda}-\\bar{v}(S_{t},{\\bf w}_{t})\\right]\\nabla\\bar{v}(S_{t},\\bf w}_{t}).}}\\end{array}  \n",
      "Unfortunately, for _Œª <_ 1, _G[Œª] t_[is not an unbiased estimate of] _[ v][‚á°]_[(] _[S][t]_[), and thus] this method does not converge to a local optimum. The situation is the same when DP targets are used such as _Vt_ = E _‚á°_ [ _Rt_ +1 + _Œ≥v_ ÀÜ( _St_ +1 _,_ **w** _t_ ) _| St_ ]. Nevertheless, such bootstrapping methods can be quite e‚Üµective, and other performance guarantees are available for important special cases, as we discuss later in this chapter. For now we emphasize the relationship of these methods to the general gradient-descent form (9.3). Although increments as in (9.4) are not themselves gradients, it is useful to view this method as a gradient-descent method (9.3) with a bootstrapping approximation in place of the desired output, _v‚á°_ ( _St_ ).  \n",
      "As (9.4) provides the forward view of gradient-descent TD( _Œª_ ), so the backward view is provided by  \n",
      "\\mathbf{w}_{t+1}=\\mathbf{w}_{t}+\\alpha\\,\\delta_{t}\\,\\mathbf{e}_{t},\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(\\mathbf{y}.5)  \n",
      "where _Œ¥t_ is the usual TD error, now using ÀÜ _v_ ,  \n",
      "\\delta_{t}=R_{t+1}+\\gamma\\bar{v}(S_{t+1},{\\bf w}_{t})-\\bar{v}(S_{t},{\\bf w}_{t}),  \n",
      "and **e** _t_ = ( _et,_ 1 _, et,_ 2 _, . . . , et,n_ ) _[>]_ is a column vector of eligibility traces, one for each component of **w** _t_ , updated by  \n",
      "OMITTED IMAGE  \n",
      "with **e** 0 = **0** . A complete algorithm for on-line gradient-descent TD( _Œª_ ) is given in Figure 9.1.  \n",
      "Two methods for gradient-based function approximation have been used widely in reinforcement learning. One is multilayer artificial neural networks using the error backpropagation algorithm. This maps immediately onto the equations and algorithms just given, where the backpropagation process is the way of computing the gradients. The second popular form is the linear form, which we discuss extensively in the next section.  \n",
      "232 _CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES_  \n",
      "\\begin{array}{l}{{\\frac{\\mu_{1}\\mu}{\\lambda}}}\\\\ {{\\frac{\\mu}{\\lambda}}}\\\\ {{\\frac{\\mu}{\\lambda}}}\\\\ {{\\frac{\\mu}{\\lambda}}}\\\\ {{\\frac{\\lambda}{\\lambda}}}\\\\ {{\\frac{\\lambda}{\\lambda}}}\\end{array}  \n",
      "**----- Start of picture text -----**<br>\n",
      "Initialize  w  as appropriate for the problem, e.g.,  w  =  0<br>Repeat (for each episode):<br>e  = 0<br>S initial state of episode<br>Repeat (for each step of episode):<br>A action given by  ‚á° for  S<br>Take action  A , observe reward,  R , and next state,  S [0]<br>ÀÜ ÀÜ<br>Œ¥ R  +  Œ≥v ( S [0] , w )  ‚àí v ( S, w )<br>e Œ≥Œª e  +  rv ÀÜ( S, w )<br>w w  +  ‚ÜµŒ¥ e<br>S S [0]<br>until  S [0] is terminal<br>**----- End of picture text -----**<br>  \n",
      "Figure 9.1: On-line gradient-descent TD( _Œª_ ) for estimating _v‚á°_ .  \n",
      "## **9.3 Linear Methods**  \n",
      "One of the most important special cases of gradient-descent function approximation is that in which the approximate function, ÀÜ _v_ , is a linear function of the parameter vector, **w** . Corresponding to every state _s_ , there is a vector of features **x** ( _s_ ) = ( _x_ 1( _s_ ) _, x_ 2( _s_ ) _, . . . , xn_ ( _s_ )) _[>]_ , with the same number of components as **w** . The features may be constructed from the states in many di‚Üµerent ways; we cover a few possibilities below. However the features are constructed, the approximate state-value function is given by  \n",
      "\\dot{v}(s,{\\bf w})={\\bf w}^{\\textsf{T}}{\\bf x}(s).\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad(9.8)  \n",
      "In this case the approximate value function is said to be _linear in the parameters_ , or simply _linear_ .  \n",
      "It is natural to use gradient-descent updates with linear function approximation. The gradient of the approximate value function with respect to **w** in this case is  \n",
      "{\\nabla}{\\dot{v}}(s,{\\mathbf{w}})={\\mathbf{x}}(s).  \n",
      "Thus, the general gradient-descent update (9.3) reduces to a particularly simple form in the linear case. In addition, in the linear case there is only one optimum **w** _[‚á§]_ (or, in degenerate cases, one set of equally good optima). Thus, any method guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum. Because it is simple in these ways, the linear, gradient-descent case is one of the most favorable for mathematical analysis. Almost all useful convergence results for  \n",
      "learning systems of all kinds are for linear (or simpler) function approximation methods.  \n",
      "In particular, the gradient-descent TD( _Œª_ ) algorithm discussed in the previous section (Figure 9.1) has been proved to converge in the linear case if the step-size parameter is reduced over time according to the usual conditions (2.7). Convergence is not to the minimum-error parameter vector, **w** _[‚á§]_ , but to a nearby parameter vector, **w** _1_ , whose error is bounded according to  \n",
      "\\mathrm{{\\bf{RMSE}}}({\\bf{w}}_{\\infty})\\;\\;\\leq\\;\\;\\frac{1-\\gamma\\lambda}{1-\\gamma}\\,\\mathrm{{RMSE}}({\\bf{w}}^{*}).\\eqno(9.9)  \n",
      "That is, the asymptotic error is no more than[1] 1 _[‚àí] ‚àí[Œ≥] Œ≥[Œª]_[times the smallest possible] error. As _Œª_ approaches 1, the bound approaches the minimum error. An analogous bound applies to other on-policy bootstrapping methods. For example, linear gradient-descent DP backups (9.3), with the on-policy distribution, will converge to the same result as TD(0). Technically, this bound applies only to discounted continuing tasks, but a related result presumably holds for episodic tasks. There are also a few technical conditions on the rewards, features, and decrease in the step-size parameter, which we are omitting here. The full details can be found in the original paper (Tsitsiklis and Van Roy, 1997).  \n",
      "Critical to the above result is that states are backed up according to the on-policy distribution. For other backup distributions, bootstrapping methods using function approximation may actually diverge to infinity. Examples of this and a discussion of possible solution methods are given in Chapter 10.  \n",
      "Beyond these theoretical results, linear learning methods are also of interest because in practice they can be very efficient in terms of both data and computation. Whether or not this is so depends critically on how the states are represented in terms of the features. Choosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the natural features of the task, those along which generalization is most appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.  \n",
      "In general, we also need features for combinations of these natural qualities. This is because the linear form prohibits the representation of interactions between features, such as the presence of feature _i_ being good only in the absence of feature _j_ . For example, in the pole-balancing task (Example 3.4), a high angular velocity may be either good or bad depending on the angular position. If the angle is high, then high angular velocity means an imminent danger of falling, a bad state, whereas if the angle is low, then high angular  \n",
      "OMITTED IMAGE  \n",
      "Figure 9.2: Coarse coding. Generalization from state _X_ to state _Y_ depends on the number of their features whose receptive fields (in this case, circles) overlap. These states have one feature in common, so there will be slight generalization between them.  \n",
      "velocity means the pole is righting itself, a good state. In cases with such interactions one needs to introduce features for conjunctions of feature values when using linear function approximation methods. We next consider some general ways of doing this.  \n",
      "## **Coarse Coding**  \n",
      "Consider a task in which the state set is continuous and two-dimensional. A state in this case is a point in 2-space, a vector with two real components. One kind of feature for this case is those corresponding to _circles_ in state space, as shown in Figure 9.2. If the state is inside a circle, then the corresponding feature has the value 1 and is said to be _present_ ; otherwise the feature is 0 and is said to be _absent_ . This kind of 1‚Äì0-valued feature is called a _binary feature_ . Given a state, which binary features are present indicate within which circles the state lies, and thus coarsely code for its location. Representing a state with features that overlap in this way (although they need not be circles or binary) is known as _coarse coding_ .  \n",
      "Assuming linear gradient-descent function approximation, consider the effect of the size and density of the circles. Corresponding to each circle is a single parameter (a component of **w** ) that is a‚Üµected by learning. If we train at one point (state) X, then the parameters of all circles intersecting X will be a‚Üµected. Thus, by (9.8), the approximate value function will be a‚Üµected at all points within the union of the circles, with a greater e‚Üµect the more circles a point has ‚Äúin common‚Äù with X, as shown in Figure 9.2. If the circles are small, then the generalization will be over a short distance, as in Figure 9.3a, whereas  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "\\begin{array}{c c}{{\\mathbf{a})\\ \\mathrm{Narrow\\gereraizaion}}}\\end{array}\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\mathrm{B}\\mathrm{Symeral}z a i o n}  \n",
      "**----- Start of picture text -----**<br>\n",
      "a) Narrow generalization b) Broad generalization c) Asymmetric generalization<br>**----- End of picture text -----**<br>  \n",
      "Figure 9.3: Generalization in linear function approximation methods is determined by the sizes and shapes of the features‚Äô receptive fields. All three of these cases have roughly the same number and density of features.  \n",
      "if they are large, it will be over a large distance, as in Figure 9.3b. Moreover, the shape of the features will determine the nature of the generalization. For example, if they are not strictly circular, but are elongated in one direction, then generalization will be similarly a‚Üµected, as in Figure 9.3c.  \n",
      "Features with large receptive fields give broad generalization, but might also seem to limit the learned function to a coarse approximation, unable to make discriminations much finer than the width of the receptive fields. Happily, this is not the case. Initial generalization from one point to another is indeed controlled by the size and shape of the receptive fields, but acuity, the finest discrimination ultimately possible, is controlled more by the total number of features.  \n",
      "**Example 9.1: Coarseness of Coarse Coding** This example illustrates the e‚Üµect on learning of the size of the receptive fields in coarse coding. Linear function approximation based on coarse coding and (9.3) was used to learn a one-dimensional square-wave function (shown at the top of Figure 9.4). The values of this function were used as the targets, _Vt_ . With just one dimension, the receptive fields were intervals rather than circles. Learning was repeated with three di‚Üµerent sizes of the intervals: narrow, medium, and broad, as shown at the bottom of the figure. All three cases had the same density of features, about 50 over the extent of the function being learned. Training examples were generated uniformly at random over this extent. The step-size parameter was _‚Üµ_ =[0] _m[.]_[2][, where] _[ m]_[ is the number of features that were present] at one time. Figure 9.4 shows the functions learned in all three cases over the course of learning. Note that the width of the features had a strong e‚Üµect early in learning. With broad features, the generalization tended to be broad; with narrow features, only the close neighbors of each trained point were changed, causing the function learned to be more bumpy. However, the final function  \n",
      "236 _CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES_  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "#Examples functiondesired approx-imation<br>10<br>40<br>160<br>640<br>2560<br>10240<br>feature<br>width<br>Narrow Medium Broad<br>features features features<br>**----- End of picture text -----**<br>  \n",
      "Figure 9.4: Example of feature width‚Äôs strong e‚Üµect on initial generalization (first row) and weak e‚Üµect on asymptotic accuracy (last row).  \n",
      "learned was a‚Üµected only slightly by the width of the features. Receptive field shape tends to have a strong e‚Üµect on generalization but little e‚Üµect on asymptotic solution quality.  \n",
      "## **Tile Coding**  \n",
      "Tile coding is a form of coarse coding that is particularly well suited for use on sequential digital computers and for efficient on-line learning. In tile coding the receptive fields of the features are grouped into exhaustive partitions of the input space. Each such partition is called a _tiling_ , and each element of the partition is called a _tile_ . Each tile is the receptive field for one binary feature.  \n",
      "An immediate advantage of tile coding is that the overall number of features that are present at one time is strictly controlled and independent of the input state. Exactly one feature is present in each tiling, so the total number of features present is always the same as the number of tilings. This allows the step-size parameter, _‚Üµ_ , to be set in an easy, intuitive way. For example, 1 choosing _‚Üµ_ = _m_[, where] _[ m]_[ is the number of tilings, results in exact one-trial] learning. If the example _s 7! v_ is received, then whatever the prior value, _v_ ÀÜ( _s,_ **w** ), the new value will be ÀÜ _v_ ( _s,_ **w** ) = _v_ . Usually one wishes to change more slowly than this, to allow for generalization and stochastic variation in target 1 outputs. For example, one might choose _‚Üµ_ = 10 _m_[, in which case one would] move one-tenth of the way to the target in one update.  \n",
      "Because tile coding uses exclusively binary (0‚Äì1-valued) features, the weighted  \n",
      "sum making up the approximate value function (9.8) is almost trivial to compute. Rather than performing _n_ multiplications and additions, one simply computes the indices of the _m ‚åß n_ present features and then adds up the _m_ corresponding components of the parameter vector. The eligibility trace computation (9.7) is also simplified because the components of the gradient, _rv_ ÀÜ( _s,_ **w** ), are also usually 0, and otherwise 1.  \n",
      "The computation of the indices of the present features is particularly easy if gridlike tilings are used. The ideas and techniques here are best illustrated by examples. Suppose we address a task with two continuous state variables. Then the simplest way to tile the space is with a uniform two-dimensional grid:  \n",
      "OMITTED IMAGE  \n",
      "Given the _x_ and _y_ coordinates of a point in the space, it is computationally easy to determine the index of the tile it is in. When multiple tilings are used, each is o‚Üµset by a di‚Üµerent amount, so that each cuts the space in a di‚Üµerent way. In the example shown in Figure 9.5, an extra row and an extra column of tiles have been added to the grid so that no points are left uncovered. The two tiles highlighted are those that are present in the state indicated by the X. The di‚Üµerent tilings may be o‚Üµset by random amounts, or by cleverly designed deterministic strategies (simply o‚Üµsetting each dimension by the same increment is known not to be a good idea). The e‚Üµects on generalization and asymptotic accuracy illustrated in Figures 9.3 and 9.4 apply here as well. The width and shape of the tiles should be chosen to match the width of generalization that one expects to be appropriate. The number of tilings should be chosen to influence the density of tiles. The denser the tiling, the finer and more accurately the desired function can be approximated, but the greater the computational costs.  \n",
      "It is important to note that the tilings can be arbitrary and need not be uniform grids. Not only can the tiles be strangely shaped, as in Figure 9.6a, but they can be shaped and distributed to give particular kinds of generalization. For example, the stripe tiling in Figure 9.6b will promote generalization along the vertical dimension and discrimination along the horizontal dimension, particularly on the left. The diagonal stripe tiling in Figure 9.6c will promote generalization along one diagonal. In higher dimensions, axis-aligned stripes correspond to ignoring some of the dimensions in some of the tilings,  \n",
      "## 238 _CHAPTER 9. ON-POLICY APPROXIMATION OF ACTION VALUES_  \n",
      "## that is, to hyperplanar slices.  \n",
      "Another important trick for reducing memory requirements is _hashing_ ‚Äîa consistent pseudo-random collapsing of a large tiling into a much smaller set of tiles. Hashing produces tiles consisting of noncontiguous, disjoint regions randomly spread throughout the state space, but that still form an exhaustive tiling. For example, one tile might consist of the four subtiles shown below:  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "one<br>tile<br>**----- End of picture text -----**<br>  \n",
      "Through hashing, memory requirements are often reduced by large factors with little loss of performance. This is possible because high resolution is needed in only a small fraction of the state space. Hashing frees us from the curse of dimensionality in the sense that memory requirements need not be exponential in the number of dimensions, but need merely match the real demands of the task. Good public-domain implementations of tile coding, including hashing, are widely available.  \n",
      "## **Radial Basis Functions**  \n",
      "Radial basis functions (RBFs) are the natural generalization of coarse coding to continuous-valued features. Rather than each feature being either 0 or 1, it can be anything in the interval [0 _,_ 1], reflecting various _degrees_ to which the feature is present. A typical RBF feature, _i_ , has a Gaussian (bell-shaped) response _xi_ ( _s_ ) dependent only on the distance between the state, _s_ , and the  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "tiling #1<br>tiling #2<br>2D state Shape of tiles ! Generalization<br>space<br>#Tilings ! Resolution of final approximation<br>**----- End of picture text -----**<br>  \n",
      "Figure 9.5: Multiple, overlapping gridtilings.  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "a) Irregular b) Log stripes c) Diagonal stripes<br>Figure 9.6: Tilings.<br>!<br>i<br>c c c<br>i -1 i i +1<br>**----- End of picture text -----**<br>  \n",
      "Figure 9.7: One-dimensional radial basis functions.  \n",
      "feature‚Äôs prototypical or center state, _ci_ , and relative to the feature‚Äôs width, _œÉi_ :  \n",
      "x_{i}(s)=\\exp\\left(-{\\frac{\\left|\\left|s-c_{i}\\right|^{2}}{2\\sigma_{i}^{2}}}\\right).  \n",
      "The norm or distance metric of course can be chosen in whatever way seems most appropriate to the states and task at hand. Figure 9.7 shows a onedimensional example with a Euclidean distance metric.  \n",
      "An _RBF network_ is a linear function approximator using RBFs for its features. Learning is defined by equations (9.3) and (9.8), exactly as in other linear function approximators. The primary advantage of RBFs over binary features is that they produce approximate functions that vary smoothly and are di‚Üµerentiable. In addition, some learning methods for RBF networks change the centers and widths of the features as well. Such nonlinear methods may be able to fit the target function much more precisely. The downside to RBF networks, and to nonlinear RBF networks especially, is greater computational complexity and, often, more manual tuning before learning is robust and efficient.\n",
      "\n",
      "Source: {'chunk_number': 0, 'course': 'AI_ALGORITHMS', 'header': '', 'source': 'data/AI_ALGORITHMS/Chap05b.pdf'}\n",
      "Content: ## 5.5 STOCHASTIC GAMES  \n",
      "In real life, many unpredictable external events can put us into unforeseen situations. Many games mirror this unpredictability by including a random element, such as the throwing of STOCHASTIC GAMES dice. We call these **stochastic games** . Backgammon is a typical game that combines luck and skill. Dice are rolled at the beginning of a player‚Äôs turn to determine the legal moves. In the backgammon position of Figure 5.10, for example, White has rolled a 6‚Äì5 and has four possible moves.  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12<br>25 24 23 22 21 20 19 18 17 16 15 14 13<br>**----- End of picture text -----**<br>  \n",
      "**Figure 5.10** A typical backgammon position. The goal of the game is to move all one‚Äôs pieces off the board. White moves clockwise toward 25, and Black moves counterclockwise toward 0. A piece can move to any position unless multiple opponent pieces are there; if there is one opponent, it is captured and must start over. In the position shown, White has rolled 6‚Äì5 and must choose among four legal moves: (5‚Äì10,5‚Äì11), (5‚Äì11,19‚Äì24), (5‚Äì10,10‚Äì16), and (5‚Äì11,11‚Äì16), where the notation (5‚Äì11,11‚Äì16) means move one piece from position 5 to 11, and then move a piece from 11 to 16.  \n",
      "Although White knows what his or her own legal moves are, White does not know what Black is going to roll and thus does not know what Black‚Äôs legal moves will be. That means White cannot construct a standard game tree of the sort we saw in chess and tic-tac-toe. A CHANCE NODES game tree in backgammon must include **chance nodes** in addition to MAX and MIN nodes. Chance nodes are shown as circles in Figure 5.11. The branches leading from each chance node denote the possible dice rolls; each branch is labeled with the roll and its probability. There are 36 ways to roll two dice, each equally likely; but because a 6‚Äì5 is the same as a 5‚Äì6, there are only 21 distinct rolls. The six doubles (1‚Äì1 through 6‚Äì6) each have a probability of 1/36, so we say P (1‚Äì1) = 1/36. The other 15 distinct rolls each have a 1/18 probability.  \n",
      "## EXPECTED VALUE  \n",
      "## EXPECTIMINIMAX VALUE  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "MAX<br>CHANCE B . . .<br>... ... ... ...<br>1/36 1/18 1/18 1/36<br>1,1 1,2 6,5 6,6<br>MIN . . .<br>... ... ...<br>CHANCE C . . .<br>... ... ...<br>1/36 1/18 1/18 1/36<br>1,1 1,2 6,5 6,6<br>MAX . . .<br>... ... ...<br>TERMINAL 2 ‚Äì1 1 ‚Äì1 1<br>**----- End of picture text -----**<br>  \n",
      "**Figure 5.11** Schematic game tree for a backgammon position.  \n",
      "The next step is to understand how to make correct decisions. Obviously, we still want to pick the move that leads to the best position. However, positions do not have definite minimax values. Instead, we can only calculate the **expected value** of a position: the average over all possible outcomes of the chance nodes.  \n",
      "This leads us to generalize the **minimax value** for deterministic games to an **expectiminimax value** for games with chance nodes. Terminal nodes and MAX and MIN nodes (for which the dice roll is known) work exactly the same way as before. For chance nodes we compute the expected value, which is the sum of the value over all outcomes, weighted by the probability of each chance action:  \n",
      "- EXPECTIMINIMAX(s) =  \n",
      "-  UTILITY(s)   \n",
      "if TERMINAL-TEST(s)  \n",
      "- maxa EXPECTIMINIMAX(RESULT(s, a))  \n",
      "if PLAYER(s) = MAX if PLAYER(s) = MIN  \n",
      "- mina EXPECTIMINIMAX(RESULT(s, a))  \n",
      "-  \n",
      "- ÔøΩr[P][(][r][)][E][XPECTIMINIMAX][(][R][ESULT][(][s, r][))][ if P][LAYER][(][s][) =][ CHANCE]  \n",
      "where r represents a possible dice roll (or other chance event) and R ESULT(s, r) is the same state as s, with the additional fact that the result of the dice roll is r.  \n",
      "## **5.5.1 Evaluation functions for games of chance**  \n",
      "As with minimax, the obvious approximation to make with expectiminimax is to cut the search off at some point and apply an evaluation function to each leaf. One might think that evaluation functions for games such as backgammon should be just like evaluation functions  \n",
      "for chess‚Äîthey just need to give higher scores to better positions. But in fact, the presence of chance nodes means that one has to be more careful about what the evaluation values mean. Figure 5.12 shows what happens: with an evaluation function that assigns the values [1, 2, 3, 4] to the leaves, move a1 is best; with values [1, 20, 30, 400], move a2 is best. Hence, the program behaves totally differently if we make a change in the scale of some evaluation values! It turns out that to avoid this sensitivity, the evaluation function must be a positive linear transformation of the probability of winning from a position (or, more generally, of the expected utility of the position). This is an important and general property of situations in which uncertainty is involved, and we discuss it further in Chapter 16.  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "MAX<br>a 1 a 2 a 1 a 2<br>CHANCE 2.1 1.3 21 40.9<br>.9 .1 .9 .1 .9 .1 .9 .1<br>MIN 2 3 1 4 20 30 1 400<br>2 2 3 3 1 1 4 4 20 20 30 30 1 1 400 400<br>**----- End of picture text -----**<br>  \n",
      "**Figure 5.12** An order-preserving transformation on leaf values changes the best move.  \n",
      "If the program knew in advance all the dice rolls that would occur for the rest of the game, solving a game with dice would be just like solving a game without dice, which minimax does in O(b[m] ) time, where b is the branching factor and m is the maximum depth of the game tree. Because expectiminimax is also considering all the possible dice-roll sequences, it will take O(b[m] n[m] ), where n is the number of distinct rolls.  \n",
      "Even if the search depth is limited to some small depth d, the extra cost compared with that of minimax makes it unrealistic to consider looking ahead very far in most games of chance. In backgammon n is 21 and b is usually around 20, but in some situations can be as high as 4000 for dice rolls that are doubles. Three plies is probably all we could manage.  \n",
      "Another way to think about the problem is this: the advantage of alpha‚Äìbeta is that it ignores future developments that just are not going to happen, given best play. Thus, it concentrates on likely occurrences. In games with dice, there are _no_ likely sequences of moves, because for those moves to take place, the dice would first have to come out the right way to make them legal. This is a general problem whenever uncertainty enters the picture: the possibilities are multiplied enormously, and forming detailed plans of action becomes pointless because the world probably will not play along.  \n",
      "It may have occurred to you that something like alpha‚Äìbeta pruning could be applied  \n",
      "MONTE CARLO SIMULATION  \n",
      "ROLLOUT  \n",
      "to game trees with chance nodes. It turns out that it can. The analysis for MIN and MAX nodes is unchanged, but we can also prune chance nodes, using a bit of ingenuity. Consider the chance node C in Figure 5.11 and what happens to its value as we examine and evaluate its children. Is it possible to find an upper bound on the value of C before we have looked at all its children? (Recall that this is what alpha‚Äìbeta needs in order to prune a node and its subtree.) At first sight, it might seem impossible because the value of C is the _average_ of its children‚Äôs values, and in order to compute the average of a set of numbers, we must look at all the numbers. But if we put bounds on the possible values of the utility function, then we can arrive at bounds for the average without looking at every number. For example, say that all utility values are between ‚àí2 and +2; then the value of leaf nodes is bounded, and in turn we _can_ place an upper bound on the value of a chance node without looking at all its children.  \n",
      "An alternative is to do **Monte Carlo simulation** to evaluate a position. Start with an alpha‚Äìbeta (or other) search algorithm. From a start position, have the algorithm play thousands of games against itself, using random dice rolls. In the case of backgammon, the resulting win percentage has been shown to be a good approximation of the value of the position, even if the algorithm has an imperfect heuristic and is searching only a few plies (Tesauro, 1995). For games with dice, this type of simulation is called a **rollout** .  \n",
      "## 5.6 PARTIALLY OBSERVABLE GAMES  \n",
      "Chess has often been described as war in miniature, but it lacks at least one major characteristic of real wars, namely, **partial observability** . In the ‚Äúfog of war,‚Äù the existence and disposition of enemy units is often unknown until revealed by direct contact. As a result, warfare includes the use of scouts and spies to gather information and the use of concealment and bluff to confuse the enemy. Partially observable games share these characteristics and are thus qualitatively different from the games described in the preceding sections.  \n",
      "KRIEGSPIEL  \n",
      "## **5.6.1 Kriegspiel: Partially observable chess**  \n",
      "In _deterministic_ partially observable games, uncertainty about the state of the board arises entirely from lack of access to the choices made by the opponent. This class includes children‚Äôs games such as Battleships (where each player‚Äôs ships are placed in locations hidden from the opponent but do not move) and Stratego (where piece locations are known but piece types are hidden). We will examine the game of **Kriegspiel** , a partially observable variant of chess in which pieces can move but are completely invisible to the opponent.  \n",
      "The rules of Kriegspiel are as follows: White and Black each see a board containing only their own pieces. A referee, who can see all the pieces, adjudicates the game and periodically makes announcements that are heard by both players. On his turn, White proposes to the referee any move that would be legal if there were no black pieces. If the move is in fact not legal (because of the black pieces), the referee announces ‚Äúillegal.‚Äù In this case, White may keep proposing moves until a legal one is found‚Äîand learns more about the location of Black‚Äôs pieces in the process. Once a legal move is proposed, the referee announces one or  \n",
      "more of the following: ‚ÄúCapture on square _X_ ‚Äù if there is a capture, and ‚ÄúCheck by _D_ ‚Äù if the black king is in check, where _D_ is the direction of the check, and can be one of ‚ÄúKnight,‚Äù ‚ÄúRank,‚Äù ‚ÄúFile,‚Äù ‚ÄúLong diagonal,‚Äù or ‚ÄúShort diagonal.‚Äù (In case of discovered check, the referee may make two ‚ÄúCheck‚Äù announcements.) If Black is checkmated or stalemated, the referee says so; otherwise, it is Black‚Äôs turn to move.  \n",
      "Kriegspiel may seem terrifyingly impossible, but humans manage it quite well and computer programs are beginning to catch up. It helps to recall the notion of a **belief state** as defined in Section 4.4 and illustrated in Figure 4.14‚Äîthe set of all _logically possible_ board states given the complete history of percepts to date. Initially, White‚Äôs belief state is a singleton because Black‚Äôs pieces haven‚Äôt moved yet. After White makes a move and Black responds, White‚Äôs belief state contains 20 positions because Black has 20 replies to any White move. Keeping track of the belief state as the game progresses is exactly the problem of **state estimation** , for which the update step is given in Equation (4.6). We can map Kriegspiel state estimation directly onto the partially observable, nondeterministic framework of Section 4.4 if we consider the opponent as the source of nondeterminism; that is, the RESULTS of White‚Äôs move are composed from the (predictable) outcome of White‚Äôs own move and the unpredictable outcome given by Black‚Äôs reply.[3]  \n",
      "Given a current belief state, White may ask, ‚ÄúCan I win the game?‚Äù For a partially observable game, the notion of a **strategy** is altered; instead of specifying a move to make for each possible _move_ the opponent might make, we need a move for every possible _percept sequence_ that might be received. For Kriegspiel, a winning strategy, or **guaranteed check-** GUARANTEEDCHECKMATE **mate** , is one that, for each possible percept sequence, leads to an actual checkmate for every possible board state in the current belief state, regardless of how the opponent moves. With this definition, the opponent‚Äôs belief state is irrelevant‚Äîthe strategy has to work even if the opponent can see all the pieces. This greatly simplifies the computation. Figure 5.13 shows part of a guaranteed checkmate for the KRK (king and rook against king) endgame. In this case, Black has just one piece (the king), so a belief state for White can be shown in a single board by marking each possible position of the Black king.  \n",
      "The general AND-OR search algorithm can be applied to the belief-state space to find guaranteed checkmates, just as in Section 4.4. The incremental belief-state algorithm mentioned in that section often finds midgame checkmates up to depth 9‚Äîprobably well beyond the abilities of human players.  \n",
      "In addition to guaranteed checkmates, Kriegspiel admits an entirely new concept that PROBABILISTICCHECKMATE makes no sense in fully observable games: **probabilistic checkmate** . Such checkmates are still required to work in every board state in the belief state; they are probabilistic with respect to randomization of the winning player‚Äôs moves. To get the basic idea, consider the problem of finding a lone black king using just the white king. Simply by moving randomly, the white king will _eventually_ bump into the black king even if the latter tries to avoid this fate, since Black cannot keep guessing the right evasive moves indefinitely. In the terminology of probability theory, detection occurs _with probability_ 1. The KBNK endgame‚Äîking, bishop  \n",
      "> 3 Sometimes, the belief state will become too large to represent just as a list of board states, but we will ignore this issue for now; Chapters 7 and 8 suggest methods for compactly representing very large belief states.  \n",
      "ACCIDENTAL CHECKMATE  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "4<br>3<br>2<br>1<br>a b c d<br>Kc3 ?<br>‚ÄúOK‚Äù ‚ÄúIllegal‚Äù<br>Rc3 ?<br>‚ÄúOK‚Äù ‚ÄúCheck‚Äù<br>**----- End of picture text -----**<br>  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "**Figure 5.13** Part of a guaranteed checkmate in the KRK endgame, shown on a reduced board. In the initial belief state, Black‚Äôs king is in one of three possible locations. By a combination of probing moves, the strategy narrows this down to one. Completion of the checkmate is left as an exercise.  \n",
      "and knight against king‚Äîis won in this sense; White presents Black with an infinite random sequence of choices, for one of which Black will guess incorrectly and reveal his position, leading to checkmate. The KBBK endgame, on the other hand, is won with probability 1 ‚àí œµ. White can force a win only by leaving one of his bishops unprotected for one move. If Black happens to be in the right place and captures the bishop (a move that would lose if the bishops are protected), the game is drawn. White can choose to make the risky move at some randomly chosen point in the middle of a very long sequence, thus reducing œµ to an arbitrarily small constant, but cannot reduce œµ to zero.  \n",
      "It is quite rare that a guaranteed or probabilistic checkmate can be found within any reasonable depth, except in the endgame. Sometimes a checkmate strategy works for _some_ of the board states in the current belief state but not others. Trying such a strategy may succeed, leading to an **accidental checkmate** ‚Äîaccidental in the sense that White could not _know_ that it would be checkmate‚Äîif Black‚Äôs pieces happen to be in the right places. (Most checkmates in games between humans are of this accidental nature.) This idea leads naturally to the question of _how likely_ it is that a given strategy will win, which leads in turn to the question of _how likely_ it is that each board state in the current belief state is the true board state.  \n",
      "One‚Äôs first inclination might be to propose that all board states in the current belief state are equally likely‚Äîbut this can‚Äôt be right. Consider, for example, White‚Äôs belief state after Black‚Äôs first move of the game. By definition (assuming that Black plays optimally), Black must have played an optimal move, so all board states resulting from suboptimal moves ought to be assigned zero probability. This argument is not quite right either, because _each player‚Äôs goal is not just to move pieces to the right squares but also to minimize the information that the opponent has about their location._ Playing any _predictable_ ‚Äúoptimal‚Äù strategy provides the opponent with information. Hence, optimal play in partially observable games requires a willingness to play somewhat _randomly_ . (This is why restaurant hygiene inspectors do _random_ inspection visits.) This means occasionally selecting moves that may seem ‚Äúintrinsically‚Äù weak‚Äîbut they gain strength from their very unpredictability, because the opponent is unlikely to have prepared any defense against them.  \n",
      "From these considerations, it seems that the probabilities associated with the board states in the current belief state can only be calculated given an optimal randomized strategy; in turn, computing that strategy seems to require knowing the probabilities of the various states the board might be in. This conundrum can be resolved by adopting the gametheoretic notion of an **equilibrium** solution, which we pursue further in Chapter 17. An equilibrium specifies an optimal randomized strategy for each player. Computing equilibria is prohibitively expensive, however, even for small games, and is out of the question for Kriegspiel. At present, the design of effective algorithms for general Kriegspiel play is an open research topic. Most systems perform bounded-depth lookahead in their own beliefstate space, ignoring the opponent‚Äôs belief state. Evaluation functions resemble those for the observable game but include a component for the size of the belief state‚Äîsmaller is better!  \n",
      "## **5.6.2 Card games**  \n",
      "Card games provide many examples of _stochastic_ partial observability, where the missing information is generated randomly. For example, in many games, cards are dealt randomly at the beginning of the game, with each player receiving a hand that is not visible to the other players. Such games include bridge, whist, hearts, and some forms of poker.  \n",
      "At first sight, it might seem that these card games are just like dice games: the cards are dealt randomly and determine the moves available to each player, but all the ‚Äúdice‚Äù are rolled at the beginning! Even though this analogy turns out to be incorrect, it suggests an effective algorithm: consider all possible deals of the invisible cards; solve each one as if it were a fully observable game; and then choose the move that has the best outcome averaged over all the deals. Suppose that each deal s occurs with probability P (s); then the move we want is  \n",
      "\\qquad\\qquad\\qquad\\mathrm{argmax}\\sum_{s}P(s)\\:\\mathrm{MLNIMAx}\\bigl(\\mathrm{RESULT}(s,a)\\bigr)\\;.  \n",
      "({\\bar{S}}.1)  \n",
      "Here, we run exact MINIMAX if computationally feasible; otherwise, we run H-MINIMAX.  \n",
      "Now, in most card games, the number of possible deals is rather large. For example, in bridge play, each player sees just two of the four hands; there are two unseen hands of 13 cards each, so the number of deals is ÔøΩ2613ÔøΩ = 10, 400, 600. Solving even one deal is quite difficult, so solving ten million is out of the question. Instead, we resort to a Monte Carlo  \n",
      "BLUFF  \n",
      "approximation: instead of adding up _all_ the deals, we take a _random sample_ of N deals, where the probability of deal s appearing in the sample is proportional to P (s):  \n",
      "{\\mathrm{~argmax}}{\\frac{1}{N}}\\sum_{i=1}^{N}{\\mathrm{MiNHAX}}({\\mathrm{RESULT}}(s_{i},a))~.  \n",
      "(Notice that P (s) does not appear explicitly in the summation, because the samples are already drawn according to P (s).) As N grows large, the sum over the random sample tends to the exact value, but even for fairly small N ‚Äîsay, 100 to 1,000‚Äîthe method gives a good approximation. It can also be applied to deterministic games such as Kriegspiel, given some reasonable estimate of P (s).  \n",
      "For games like whist and hearts, where there is no bidding or betting phase before play commences, each deal will be equally likely and so the values of P (s) are all equal. For bridge, play is preceded by a bidding phase in which each team indicates how many tricks it expects to win. Since players bid based on the cards they hold, the other players learn more about the probability of each deal. Taking this into account in deciding how to play the hand is tricky, for the reasons mentioned in our description of Kriegspiel: players may bid in such a way as to minimize the information conveyed to their opponents. Even so, the approach is quite effective for bridge, as we show in Section 5.7.  \n",
      "The strategy described in Equations 5.1 and 5.2 is sometimes called _averaging over clairvoyance_ because it assumes that the game will become observable to both players immediately after the first move. Despite its intuitive appeal, the strategy can lead one astray. Consider the following story:  \n",
      "Day 1: Road _A_ leads to a heap of gold; Road _B_ leads to a fork. Take the left fork and you‚Äôll find a bigger heap of gold, but take the right fork and you‚Äôll be run over by a bus. Day 2: Road _A_ leads to a heap of gold; Road _B_ leads to a fork. Take the right fork and you‚Äôll find a bigger heap of gold, but take the left fork and you‚Äôll be run over by a bus. Day 3: Road _A_ leads to a heap of gold; Road _B_ leads to a fork. One branch of the fork leads to a bigger heap of gold, but take the wrong fork and you‚Äôll be hit by a bus. Unfortunately you don‚Äôt know which fork is which.  \n",
      "Averaging over clairvoyance leads to the following reasoning: on Day 1, _B_ is the right choice; on Day 2, _B_ is the right choice; on Day 3, the situation is the same as either Day 1 or Day 2, so _B_ must still be the right choice.  \n",
      "Now we can see how averaging over clairvoyance fails: it does not consider the _belief state_ that the agent will be in after acting. A belief state of total ignorance is not desirable, especially when one possibility is certain death. Because it assumes that every future state will automatically be one of perfect knowledge, the approach never selects actions that _gather information_ (like the first move in Figure 5.13); nor will it choose actions that hide information from the opponent or provide information to a partner because it assumes that they already know the information; and it will never **bluff** in poker,[4] because it assumes the opponent can see its cards. In Chapter 17, we show how to construct algorithms that do all these things by virtue of solving the true partially observable decision problem.  \n",
      "4 Bluffing‚Äîbetting as if one‚Äôs hand is good, even when it‚Äôs not‚Äîis a core part of poker strategy.  \n",
      "## CHESS  \n",
      "## NULL MOVE  \n",
      "## FUTILITY PRUNING  \n",
      "In 1965, the Russian mathematician Alexander Kronrod called chess ‚Äúthe _Drosophila_ of artificial intelligence.‚Äù John McCarthy disagrees: whereas geneticists use fruit flies to make discoveries that apply to biology more broadly, AI has used chess to do the equivalent of breeding very fast fruit flies. Perhaps a better analogy is that chess is to AI as Grand Prix motor racing is to the car industry: state-of-the-art game programs are blindingly fast, highly optimized machines that incorporate the latest engineering advances, but they aren‚Äôt much use for doing the shopping or driving off-road. Nonetheless, racing and game-playing generate excitement and a steady stream of innovations that have been adopted by the wider community. In this section we look at what it takes to come out on top in various games.  \n",
      "**Chess** : IBM‚Äôs DEEP BLUE chess program, now retired, is well known for defeating world champion Garry Kasparov in a widely publicized exhibition match. Deep Blue ran on a parallel computer with 30 IBM RS/6000 processors doing alpha‚Äìbeta search. The unique part was a configuration of 480 custom VLSI chess processors that performed move generation and move ordering for the last few levels of the tree, and evaluated the leaf nodes. Deep Blue searched up to 30 billion positions per move, reaching depth 14 routinely. The key to its success seems to have been its ability to generate singular extensions beyond the depth limit for sufficiently interesting lines of forcing/forced moves. In some cases the search reached a depth of 40 plies. The evaluation function had over 8000 features, many of them describing highly specific patterns of pieces. An ‚Äúopening book‚Äù of about 4000 positions was used, as well as a database of 700,000 grandmaster games from which consensus recommendations could be extracted. The system also used a large endgame database of solved positions containing all positions with five pieces and many with six pieces. This database had the effect of substantially extending the effective search depth, allowing Deep Blue to play perfectly in some cases even when it was many moves away from checkmate.  \n",
      "The success of DEEP BLUE reinforced the widely held belief that progress in computer game-playing has come primarily from ever-more-powerful hardware‚Äîa view encouraged by IBM. But algorithmic improvements have allowed programs running on standard PCs to win World Computer Chess Championships. A variety of pruning heuristics are used to reduce the effective branching factor to less than 3 (compared with the actual branching factor of about 35). The most important of these is the **null move** heuristic, which generates a good lower bound on the value of a position, using a shallow search in which the opponent gets to move twice at the beginning. This lower bound often allows alpha‚Äìbeta pruning without the expense of a full-depth search. Also important is **futility pruning** , which helps decide in advance which moves will cause a beta cutoff in the successor nodes.  \n",
      "HYDRA can be seen as the successor to DEEP BLUE. HYDRA runs on a 64-processor cluster with 1 gigabyte per processor and with custom hardware in the form of FPGA (Field Programmable Gate Array) chips. H YDRA reaches 200 million evaluations per second, about the same as Deep Blue, but HYDRA reaches 18 plies deep rather than just 14 because of aggressive use of the null move heuristic and forward pruning.  \n",
      "RYBKA, winner of the 2008 and 2009 World Computer Chess Championships, is considered the strongest current computer player. It uses an off-the-shelf 8-core 3.2 GHz Intel Xeon processor, but little is known about the design of the program. RYBKA‚Äôs main advantage appears to be its evaluation function, which has been tuned by its main developer, International Master Vasik Rajlich, and at least three other grandmasters.  \n",
      "The most recent matches suggest that the top computer chess programs have pulled ahead of all human contenders. (See the historical notes for details.)  \n",
      "**Checkers** : Jonathan Schaeffer and colleagues developed CHINOOK, which runs on regular PCs and uses alpha‚Äìbeta search. Chinook defeated the long-running human champion in an abbreviated match in 1990, and since 2007 CHINOOK has been able to play perfectly by using alpha‚Äìbeta search combined with a database of 39 trillion endgame positions.  \n",
      "CHECKERS  \n",
      "**Othello** , also called Reversi, is probably more popular as a computer game than as a board game. It has a smaller search space than chess, usually 5 to 15 legal moves, but evaluation expertise had to be developed from scratch. In 1997, the LOGISTELLO program (Buro, 2002) defeated the human world champion, Takeshi Murakami, by six games to none. It is generally acknowledged that humans are no match for computers at Othello.  \n",
      "OTHELLO  \n",
      "**Backgammon** : Section 5.5 explained why the inclusion of uncertainty from dice rolls makes deep search an expensive luxury. Most work on backgammon has gone into improving the evaluation function. Gerry Tesauro (1992) combined reinforcement learning with neural networks to develop a remarkably accurate evaluator that is used with a search to depth 2 or 3. After playing more than a million training games against itself, Tesauro‚Äôs program, TD-GAMMON, is competitive with top human players. The program‚Äôs opinions on the opening moves of the game have in some cases radically altered the received wisdom.  \n",
      "BACKGAMMON  \n",
      "> GO **Go** is the most popular board game in Asia. Because the board is 19 √ó 19 and moves are allowed into (almost) every empty square, the branching factor starts at 361, which is too daunting for regular alpha‚Äìbeta search methods. In addition, it is difficult to write an evaluation function because control of territory is often very unpredictable until the endgame. Therefore the top programs, such as M OGO, avoid alpha‚Äìbeta search and instead use Monte Carlo rollouts. The trick is to decide what moves to make in the course of the rollout. There is no aggressive pruning; all moves are possible. The UCT (upper confidence bounds on trees) method works by making random moves in the first few iterations, and over time guiding the sampling process to prefer moves that have led to wins in previous samples. Some tricks are added, including _knowledge-based rules_ that suggest particular moves whenever a given pattern is detected and _limited local search_ to decide tactical questions. Some programs also  \n",
      "> COMBINATORIALGAME THEORY include special techniques from **combinatorial game theory** to analyze endgames. These techniques decompose a position into sub-positions that can be analyzed separately and then combined (Berlekamp and Wolfe, 1994; M¬®uller, 2003). The optimal solutions obtained in this way have surprised many professional Go players, who thought they had been playing optimally all along. Current Go programs play at the master level on a reduced 9 √ó 9 board, but are still at advanced amateur level on a full board.  \n",
      "GO  \n",
      "**Bridge** is a card game of imperfect information: a player‚Äôs cards are hidden from the other players. Bridge is also a _multiplayer_ game with four players instead of two, although the  \n",
      "BRIDGE  \n",
      "EXPLANATIONBASED GENERALIZATION  \n",
      "SCRABBLE  \n",
      "players are paired into two teams. As in Section 5.6, optimal play in partially observable games like bridge can include elements of information gathering, communication, and careful weighing of probabilities. Many of these techniques are used in the Bridge Baron program (Smith _et al._ , 1998), which won the 1997 computer bridge championship. While it does not play optimally, Bridge Baron is one of the few successful game-playing systems to use complex, hierarchical plans (see Chapter 11) involving high-level ideas, such as **finessing** and **squeezing** , that are familiar to bridge players.  \n",
      "The GIB program (Ginsberg, 1999) won the 2000 computer bridge championship quite decisively using the Monte Carlo method. Since then, other winning programs have followed GIB‚Äôs lead. GIB‚Äôs major innovation is using **explanation-based generalization** to compute and cache general rules for optimal play in various standard classes of situations rather than evaluating each situation individually. For example, in a situation where one player has the cards A-K-Q-J-4-3-2 of one suit and another player has 10-9-8-7-6-5, there are 7 √ó 6 = 42 ways that the first player can lead from that suit and the second player can follow. But GIB treats these situations as just two: the first player can lead either a high card or a low card; the exact cards played don‚Äôt matter. With this optimization (and a few others), GIB can solve a 52-card, fully observable deal _exactly_ in about a second. GIB‚Äôs tactical accuracy makes up for its inability to reason about information. It finished 12th in a field of 35 in the par contest (involving just play of the hand, not bidding) at the 1998 human world championship, far exceeding the expectations of many human experts.  \n",
      "There are several reasons why GIB plays at expert level with Monte Carlo simulation, whereas Kriegspiel programs do not. First, GIB‚Äôs evaluation of the fully observable version of the game is exact, searching the full game tree, while Kriegspiel programs rely on inexact heuristics. But far more important is the fact that in bridge, most of the uncertainty in the partially observable information comes from the randomness of the deal, not from the adversarial play of the opponent. Monte Carlo simulation handles randomness well, but does not always handle strategy well, especially when the strategy involves the value of information.  \n",
      "**Scrabble** : Most people think the hard part about Scrabble is coming up with good words, but given the official dictionary, it turns out to be rather easy to program a move generator to find the highest-scoring move (Gordon, 1994). That doesn‚Äôt mean the game is solved, however: merely taking the top-scoring move each turn results in a good but not expert player. The problem is that Scrabble is both partially observable and stochastic: you don‚Äôt know what letters the other player has or what letters you will draw next. So playing Scrabble well combines the difficulties of backgammon and bridge. Nevertheless, in 2006, the QUACKLE program defeated the former world champion, David Boys, 3‚Äì2.  \n",
      "## 5.8 ALTERNATIVE APPROACHES  \n",
      "Because calculating optimal decisions in games is intractable in most cases, all algorithms must make some assumptions and approximations. The standard approach, based on minimax, evaluation functions, and alpha‚Äìbeta, is just one way to do this. Probably because it has  \n",
      "OMITTED IMAGE  \n",
      "**----- Start of picture text -----**<br>\n",
      "MAX<br>MIN 99 100<br>99 1000 1000 1000 100 101 102 100<br>Figure 5.14 A two-ply game tree for which heuristic minimax may make an error.<br>**----- End of picture text -----**<br>  \n",
      "been worked on for so long, the standard approach dominates other methods in tournament play. Some believe that this has caused game playing to become divorced from the mainstream of AI research: the standard approach no longer provides much room for new insight into general questions of decision making. In this section, we look at the alternatives.  \n",
      "First, let us consider heuristic minimax. It selects an optimal move in a given search tree _provided that the leaf node evaluations are exactly correct_ . In reality, evaluations are usually crude estimates of the value of a position and can be considered to have large errors associated with them. Figure 5.14 shows a two-ply game tree for which minimax suggests taking the right-hand branch because 100 > 99. That is the correct move if the evaluations are all correct. But of course the evaluation function is only approximate. Suppose that the evaluation of each node has an error that is independent of other nodes and is randomly distributed with mean zero and standard deviation of œÉ. Then when œÉ = 5, the left-hand branch is actually better 71% of the time, and 58% of the time when œÉ = 2. The intuition behind this is that the right-hand branch has four nodes that are close to 99; if an error in the evaluation of any one of the four makes the right-hand branch slip below 99, then the left-hand branch is better.  \n",
      "In reality, circumstances are actually worse than this because the error in the evaluation function is _not_ independent. If we get one node wrong, the chances are high that nearby nodes in the tree will also be wrong. The fact that the node labeled 99 has siblings labeled 1000 suggests that in fact it might have a higher true value. We can use an evaluation function that returns a probability distribution over possible values, but it is difficult to combine these distributions properly, because we won‚Äôt have a good model of the very strong dependencies that exist between the values of sibling nodes  \n",
      "Next, we consider the search algorithm that generates the tree. The aim of an algorithm designer is to specify a computation that runs quickly and yields a good move. The alpha‚Äìbeta algorithm is designed not just to select a good move but also to calculate bounds on the values of all the legal moves. To see why this extra information is unnecessary, consider a position in which there is only one legal move. Alpha‚Äìbeta search still will generate and evaluate a large search tree, telling us that the only move is the best move and assigning it a value. But since we have to make the move anyway, knowing the move‚Äôs value is useless. Similarly, if there is one obviously good move and several moves that are legal but lead to a quick loss, we  \n",
      "## METAREASONING  \n",
      "would not want alpha‚Äìbeta to waste time determining a precise value for the lone good move. Better to just make the move quickly and save the time for later. This leads to the idea of the _utility of a node expansion_ . A good search algorithm should select node expansions of high utility‚Äîthat is, ones that are likely to lead to the discovery of a significantly better move. If there are no node expansions whose utility is higher than their cost (in terms of time), then the algorithm should stop searching and make a move. Notice that this works not only for clear-favorite situations but also for the case of _symmetrical_ moves, for which no amount of search will show that one move is better than another.  \n",
      "This kind of reasoning about what computations to do is called **metareasoning** (reasoning about reasoning). It applies not just to game playing but to any kind of reasoning at all. All computations are done in the service of trying to reach better decisions, all have costs, and all have some likelihood of resulting in a certain improvement in decision quality. Alpha‚Äìbeta incorporates the simplest kind of metareasoning, namely, a theorem to the effect that certain branches of the tree can be ignored without loss. It is possible to do much better. In Chapter 16, we see how these ideas can be made precise and implementable.  \n",
      "Finally, let us reexamine the nature of search itself. Algorithms for heuristic search and for game playing generate sequences of concrete states, starting from the initial state and then applying an evaluation function. Clearly, this is not how humans play games. In chess, one often has a particular goal in mind‚Äîfor example, trapping the opponent‚Äôs queen‚Äî and can use this goal to _selectively_ generate plausible plans for achieving it. This kind of goal-directed reasoning or planning sometimes eliminates combinatorial search altogether. David Wilkins‚Äô (1980) PARADISE is the only program to have used goal-directed reasoning successfully in chess: it was capable of solving some chess problems requiring an 18-move combination. As yet there is no good understanding of how to _combine_ the two kinds of algorithms into a robust and efficient system, although Bridge Baron might be a step in the right direction. A fully integrated system would be a significant achievement not just for game-playing research but also for AI research in general, because it would be a good basis for a general intelligent agent.  \n",
      "## 5.9 SUMMARY  \n",
      "We have looked at a variety of games to understand what optimal play means and to understand how to play well in practice. The most important ideas are as follows:  \n",
      "- A game can be defined by the **initial state** (how the board is set up), the legal **actions** in each state, the **result** of each action, a **terminal test** (which says when the game is over), and a **utility function** that applies to terminal states.  \n",
      "- In two-player zero-sum games with **perfect information** , the **minimax** algorithm can select optimal moves by a depth-first enumeration of the game tree.  \n",
      "- The **alpha‚Äìbeta** search algorithm computes the same optimal move as minimax, but achieves much greater efficiency by eliminating subtrees that are provably irrelevant.  \n",
      "- Usually, it is not feasible to consider the whole game tree (even with alpha‚Äìbeta), so we  \n",
      "need to cut the search off at some point and apply a heuristic **evaluation function** that estimates the utility of a state.  \n",
      "- Many game programs precompute tables of best moves in the opening and endgame so that they can look up a move rather than search.  \n",
      "- Games of chance can be handled by an extension to the minimax algorithm that evaluates a **chance node** by taking the average utility of all its children, weighted by the probability of each child.  \n",
      "- Optimal play in games of **imperfect information** , such as Kriegspiel and bridge, requires reasoning about the current and future **belief states** of each player. A simple approximation can be obtained by averaging the value of an action over each possible configuration of missing information.  \n",
      "- Programs have bested even champion human players at games such as chess, checkers, and Othello. Humans retain the edge in several games of imperfect information, such as poker, bridge, and Kriegspiel, and in games with very large branching factors and little good heuristic knowledge, such as Go.  \n",
      "## BIBLIOGRAPHICAL AND HISTORICAL NOTES  \n",
      "The early history of mechanical game playing was marred by numerous frauds. The most notorious of these was Baron Wolfgang von Kempelen‚Äôs (1734‚Äì1804) ‚ÄúThe Turk,‚Äù a supposed chess-playing automaton that defeated Napoleon before being exposed as a magician‚Äôs trick cabinet housing a human chess expert (see Levitt, 2000). It played from 1769 to 1854. In 1846, Charles Babbage (who had been fascinated by the Turk) appears to have contributed the first serious discussion of the feasibility of computer chess and checkers (Morrison and Morrison, 1961). He did not understand the exponential complexity of search trees, claiming ‚Äúthe combinations involved in the Analytical Engine enormously surpassed any required, even by the game of chess.‚Äù Babbage also designed, but did not build, a special-purpose machine for playing tic-tac-toe. The first true game-playing machine was built around 1890 by the Spanish engineer Leonardo Torres y Quevedo. It specialized in the ‚ÄúKRK‚Äù (king and rook vs. king) chess endgame, guaranteeing a win with king and rook from any position.  \n",
      "The minimax algorithm is traced to a 1912 paper by Ernst Zermelo, the developer of modern set theory. The paper unfortunately contained several errors and did not describe minimax correctly. On the other hand, it did lay out the ideas of retrograde analysis and proposed (but did not prove) what became known as Zermelo‚Äôs theorem: that chess is determined‚Äî White can force a win or Black can or it is a draw; we just don‚Äôt know which. Zermelo says that should we eventually know, ‚ÄúChess would of course lose the character of a game at all.‚Äù A solid foundation for game theory was developed in the seminal work _Theory of Games and Economic Behavior_ (von Neumann and Morgenstern, 1944), which included an analysis showing that some games _require_ strategies that are randomized (or otherwise unpredictable). See Chapter 17 for more information.  \n",
      "John McCarthy conceived the idea of alpha‚Äìbeta search in 1956, although he did not publish it. The NSS chess program (Newell _et al._ , 1958) used a simplified version of alpha‚Äì beta; it was the first chess program to do so. Alpha‚Äìbeta pruning was described by Hart and Edwards (1961) and Hart _et al._ (1972). Alpha‚Äìbeta was used by the ‚ÄúKotok‚ÄìMcCarthy‚Äù chess program written by a student of John McCarthy (Kotok, 1962). Knuth and Moore (1975) proved the correctness of alpha‚Äìbeta and analysed its time complexity. Pearl (1982b) shows alpha‚Äìbeta to be asymptotically optimal among all fixed-depth game-tree search algorithms.  \n",
      "Several attempts have been made to overcome the problems with the ‚Äústandard approach‚Äù that were outlined in Section 5.8. The first nonexhaustive heuristic search algorithm with some theoretical grounding was probably B[‚àó] (Berliner, 1979), which attempts to maintain interval bounds on the possible value of a node in the game tree rather than giving it a single point-valued estimate. Leaf nodes are selected for expansion in an attempt to refine the top-level bounds until one move is ‚Äúclearly best.‚Äù Palay (1985) extends the B[‚àó] idea using probability distributions on values in place of intervals. David McAllester‚Äôs (1988) conspiracy number search expands leaf nodes that, by changing their values, could cause the program to prefer a new move at the root. MGSS[‚àó] (Russell and Wefald, 1989) uses the decision-theoretic techniques of Chapter 16 to estimate the value of expanding each leaf in terms of the expected improvement in decision quality at the root. It outplayed an alpha‚Äì beta algorithm at Othello despite searching an order of magnitude fewer nodes. The MGSS[‚àó] approach is, in principle, applicable to the control of any form of deliberation.  \n",
      "Alpha‚Äìbeta search is in many ways the two-player analog of depth-first branch-andbound, which is dominated by A[‚àó] in the single-agent case. The SSS[‚àó] algorithm (Stockman, 1979) can be viewed as a two-player A[‚àó] and never expands more nodes than alpha‚Äìbeta to reach the same decision. The memory requirements and computational overhead of the queue make SSS[‚àó] in its original form impractical, but a linear-space version has been developed from the RBFS algorithm (Korf and Chickering, 1996). Plaat _et al._ (1996) developed a new view of SSS[‚àó] as a combination of alpha‚Äìbeta and transposition tables, showing how to overcome the drawbacks of the original algorithm and developing a new variant called MTD( _f_ ) that has been adopted by a number of top programs.  \n",
      "D. F. Beal (1980) and Dana Nau (1980, 1983) studied the weaknesses of minimax applied to approximate evaluations. They showed that under certain assumptions about the distribution of leaf values in the tree, minimaxing can yield values at the root that are actually _less_ reliable than the direct use of the evaluation function itself. Pearl‚Äôs book _Heuristics_ (1984) partially explains this apparent paradox and analyzes many game-playing algorithms. Baum and Smith (1997) propose a probability-based replacement for minimax, showing that it results in better choices in certain games. The expectiminimax algorithm was proposed by Donald Michie (1966). Bruce Ballard (1983) extended alpha‚Äìbeta pruning to cover trees with chance nodes and Hauk (2004) reexamines this work and provides empirical results.  \n",
      "Koller and Pfeffer (1997) describe a system for completely solving partially observable games. The system is quite general, handling games whose optimal strategy requires randomized moves and games that are more complex than those handled by any previous system. Still, it can‚Äôt handle games as complex as poker, bridge, and Kriegspiel. Frank _et al._ (1998) describe several variants of Monte Carlo search, including one where MIN has  \n",
      "complete information but MAX does not. Among deterministic, partially observable games, Kriegspiel has received the most attention. Ferguson demonstrated hand-derived randomized strategies for winning Kriegspiel with a bishop and knight (1992) or two bishops (1995) against a king. The first Kriegspiel programs concentrated on finding endgame checkmates and performed AND‚ÄìOR search in belief-state space (Sakuta and Iida, 2002; Bolognesi and Ciancarini, 2003). Incremental belief-state algorithms enabled much more complex midgame checkmates to be found (Russell and Wolfe, 2005; Wolfe and Russell, 2007), but efficient state estimation remains the primary obstacle to effective general play (Parker _et al._ , 2005).  \n",
      "**Chess** was one of the first tasks undertaken in AI, with early efforts by many of the pioneers of computing, including Konrad Zuse in 1945, Norbert Wiener in his book _Cybernetics_ (1948), and Alan Turing in 1950 (see Turing _et al._ , 1953). But it was Claude Shannon‚Äôs article _Programming a Computer for Playing Chess_ (1950) that had the most complete set of ideas, describing a representation for board positions, an evaluation function, quiescence search, and some ideas for selective (nonexhaustive) game-tree search. Slater (1950) and the commentators on his article also explored the possibilities for computer chess play.  \n",
      "D. G. Prinz (1952) completed a program that solved chess endgame problems but did not play a full game. Stan Ulam and a group at the Los Alamos National Lab produced a program that played chess on a 6 √ó 6 board with no bishops (Kister _et al._ , 1957). It could search 4 plies deep in about 12 minutes. Alex Bernstein wrote the first documented program to play a full game of standard chess (Bernstein and Roberts, 1958).[5]  \n",
      "The first computer chess match featured the Kotok‚ÄìMcCarthy program from MIT (Kotok, 1962) and the ITEP program written in the mid-1960s at Moscow‚Äôs Institute of Theoretical and Experimental Physics (Adelson-Velsky _et al._ , 1970). This intercontinental match was played by telegraph. It ended with a 3‚Äì1 victory for the ITEP program in 1967. The first chess program to compete successfully with humans was MIT‚Äôs MACHACK-6 (Greenblatt _et al._ , 1967). Its Elo rating of approximately 1400 was well above the novice level of 1000.  \n",
      "The Fredkin Prize, established in 1980, offered awards for progressive milestones in chess play. The $5,000 prize for the first program to achieve a master rating went to BELLE (Condon and Thompson, 1982), which achieved a rating of 2250. The $10,000 prize for the first program to achieve a USCF (United States Chess Federation) rating of 2500 (near the grandmaster level) was awarded to DEEP THOUGHT (Hsu _et al._ , 1990) in 1989. The grand prize, $100,000, went to DEEP BLUE (Campbell _et al._ , 2002; Hsu, 2004) for its landmark victory over world champion Garry Kasparov in a 1997 exhibition match. Kasparov wrote:  \n",
      "The decisive game of the match was Game 2, which left a scar in my memory . . . we saw something that went well beyond our wildest expectations of how well a computer would be able to foresee the long-term positional consequences of its decisions. The machine refused to move to a position that had a decisive short-term advantage‚Äîshowing a very human sense of danger. (Kasparov, 1997)  \n",
      "Probably the most complete description of a modern chess program is provided by Ernst Heinz (2000), whose DARKTHOUGHT program was the highest-ranked noncommercial PC program at the 1999 world championships.  \n",
      "> 5 A Russian program, BESM may have predated Bernstein‚Äôs program.  \n",
      "OMITTED IMAGE  \n",
      "OMITTED IMAGE  \n",
      "(a) (b)  \n",
      "**Figure 5.15** Pioneers in computer chess: (a) Herbert Simon and Allen Newell, developers of the NSS program (1958); (b) John McCarthy and the Kotok‚ÄìMcCarthy program on an IBM 7090 (1967).  \n",
      "In recent years, chess programs are pulling ahead of even the world‚Äôs best humans. In 2004‚Äì2005 HYDRA defeated grand master Evgeny Vladimirov 3.5‚Äì0.5, world champion Ruslan Ponomariov 2‚Äì0, and seventh-ranked Michael Adams 5.5‚Äì0.5. In 2006, DEEP FRITZ beat world champion Vladimir Kramnik 4‚Äì2, and in 2007 RYBKA defeated several grand masters in games in which it gave odds (such as a pawn) to the human players. As of 2009, the highest Elo rating ever recorded was Kasparov‚Äôs 2851. H YDRA (Donninger and Lorenz, 2004) is rated somewhere between 2850 and 3000, based mostly on its trouncing of Michael Adams. The RYBKA program is rated between 2900 and 3100, but this is based on a small number of games and is not considered reliable. Ross (2004) shows how human players have learned to exploit some of the weaknesses of the computer programs.  \n",
      "**Checkers** was the first of the classic games fully played by a computer. Christopher Strachey (1952) wrote the first working program for checkers. Beginning in 1952, Arthur Samuel of IBM, working in his spare time, developed a checkers program that learned its own evaluation function by playing itself thousands of times (Samuel, 1959, 1967). We describe this idea in more detail in Chapter 21. Samuel‚Äôs program began as a novice but after only a few days‚Äô self-play had improved itself beyond Samuel‚Äôs own level. In 1962 it defeated Robert Nealy, a champion at ‚Äúblind checkers,‚Äù through an error on his part. When one considers that Samuel‚Äôs computing equipment (an IBM 704) had 10,000 words of main memory, magnetic tape for long-term storage, and a .000001 GHz processor, the win remains a great accomplishment.  \n",
      "The challenge started by Samuel was taken up by Jonathan Schaeffer of the University of Alberta. His CHINOOK program came in second in the 1990 U.S. Open and earned the right to challenge for the world championship. It then ran up against a problem, in the form of Marion Tinsley. Dr. Tinsley had been world champion for over 40 years, losing only three games in all that time. In the first match against CHINOOK, Tinsley suffered his fourth  \n",
      "and fifth losses, but won the match 20.5‚Äì18.5. A rematch at the 1994 world championship ended prematurely when Tinsley had to withdraw for health reasons. CHINOOK became the official world champion. Schaeffer kept on building on his database of endgames, and in 2007 ‚Äúsolved‚Äù checkers (Schaeffer _et al._ , 2007; Schaeffer, 2008). This had been predicted by Richard Bellman (1965). In the paper that introduced the dynamic programming approach to retrograde analysis, he wrote, ‚ÄúIn checkers, the number of possible moves in any given situation is so small that we can confidently expect a complete digital computer solution to the problem of optimal play in this game.‚Äù Bellman did not, however, fully appreciate the size of the checkers game tree. There are about 500 quadrillion positions. After 18 years of computation on a cluster of 50 or more machines, Jonathan Schaeffer‚Äôs team completed an endgame table for all checkers positions with 10 or fewer pieces: over 39 trillion entries. From there, they were able to do forward alpha‚Äìbeta search to derive a policy that proves that checkers is in fact a draw with best play by both sides. Note that this is an application of bidirectional search (Section 3.4.6). Building an endgame table for all of checkers would be impractical: it would require a billion gigabytes of storage. Searching without any table would also be impractical: the search tree has about 8[47] positions, and would take thousands of years to search with today‚Äôs technology. Only a combination of clever search, endgame data, and a drop in the price of processors and memory could solve checkers. Thus, checkers joins Qubic (Patashnik, 1980), Connect Four (Allis, 1988), and Nine-Men‚Äôs Morris (Gasser, 1998) as games that have been solved by computer analysis.  \n",
      "**Backgammon** , a game of chance, was analyzed mathematically by Gerolamo Cardano (1663), but only taken up for computer play in the late 1970s, first with the BKG program (Berliner, 1980b); it used a complex, manually constructed evaluation function and searched only to depth 1. It was the first program to defeat a human world champion at a major classic game (Berliner, 1980a). Berliner readily acknowledged that BKG was very lucky with the dice. Gerry Tesauro‚Äôs (1995) TD-GAMMON played consistently at world champion level. The BGBLITZ program was the winner of the 2008 Computer Olympiad.  \n",
      "**Go** is a deterministic game, but the large branching factor makes it challeging. The key issues and early literature in computer Go are summarized by Bouzy and Cazenave (2001) and M¬®uller (2002). Up to 1997 there were no competent Go programs. Now the best programs play _most_ of their moves at the master level; the only problem is that over the course of a game they usually make at least one serious blunder that allows a strong opponent to win. Whereas alpha‚Äìbeta search reigns in most games, many recent Go programs have adopted Monte Carlo methods based on the UCT (upper confidence bounds on trees) scheme (Kocsis and Szepesvari, 2006). The strongest Go program as of 2009 is Gelly and Silver‚Äôs MOGO (Wang and Gelly, 2007; Gelly and Silver, 2008). In August 2008, MOGO scored a surprising win against top professional Myungwan Kim, albeit with MOGO receiving a handicap of nine stones (about the equivalent of a queen handicap in chess). Kim estimated MOGO‚Äôs strength at 2‚Äì3 dan, the low end of advanced amateur. For this match, MOGO was run on an 800-processor 15 teraflop supercomputer (1000 times Deep Blue). A few weeks later, MOGO, with only a five-stone handicap, won against a 6-dan professional. In the 9 √ó 9 form of Go, MOGO is at approximately the 1-dan professional level. Rapid advances are likely as experimentation continues with new forms of Monte Carlo search. The _Computer Go_  \n",
      "## EXERCISES  \n",
      "_Newsletter_ , published by the Computer Go Association, describes current developments.  \n",
      "**Bridge** : Smith _et al._ (1998) report on how their planning-based program won the 1998 computer bridge championship, and (Ginsberg, 2001) describes how his GIB program, based on Monte Carlo simulation, won the following computer championship and did surprisingly well against human players and standard book problem sets. From 2001‚Äì2007, the computer bridge championship was won five times by JACK and twice by WBRIDGE5. Neither has had academic articles explaining their structure, but both are rumored to use the Monte Carlo technique, which was first proposed for bridge by Levy (1989).  \n",
      "**Scrabble** : A good description of a top program, MAVEN, is given by its creator, Brian Sheppard (2002). Generating the highest-scoring move is described by Gordon (1994), and modeling opponents is covered by Richards and Amir (2007).  \n",
      "**Soccer** (Kitano _et al._ , 1997b; Visser _et al._ , 2008) and **billiards** (Lam and Greenspan, 2008; Archibald _et al._ , 2009) and other stochastic games with a continuous space of actions are beginning to attract attention in AI, both in simulation and with physical robot players.  \n",
      "Computer game competitions occur annually, and papers appear in a variety of venues. The rather misleadingly named conference proceedings _Heuristic Programming in Artificial Intelligence_ report on the Computer Olympiads, which include a wide variety of games. The General Game Competition (Love _et al._ , 2006) tests programs that must learn to play an unknown game given only a logical description of the rules of the game. There are also several edited collections of important papers on game-playing research (Levy, 1988a, 1988b; Marsland and Schaeffer, 1990). The International Computer Chess Association (ICCA), founded in 1977, publishes the _ICGA Journal_ (formerly the _ICCA Journal_ ). Important papers have been published in the serial anthology _Advances in Computer Chess_ , starting with Clarke (1977). Volume 134 of the journal _Artificial Intelligence_ (2002) contains descriptions of state-of-the-art programs for chess, Othello, Hex, shogi, Go, backgammon, poker, Scrabble, and other games. Since 1998, a biennial _Computers and Games_ conference has been held.  \n",
      "**5.1** Suppose you have an oracle, OM (s), that correctly predicts the opponent‚Äôs move in any state. Using this, formulate the definition of a game as a (single-agent) search problem. Describe an algorithm for finding the optimal move.  \n",
      "- **5.2** Consider the problem of solving two 8-puzzles.  \n",
      "- **a** . Give a complete problem formulation in the style of Chapter 3.  \n",
      "- **b** . How large is the reachable state space? Give an exact numerical expression.  \n",
      "- **c** . Suppose we make the problem adversarial as follows: the two players take turns moving; a coin is flipped to determine the puzzle on which to make a move in that turn; and the winner is the first to solve one puzzle. Which algorithm can be used to choose a move in this setting?  \n",
      "- **d** . Give an informal proof that someone will eventually win if both play perfectly.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "## Step 1: Formulate the definition of a game as a (single-agent) search problem.\n",
      "A game can be defined as a single-agent search problem when we have an oracle, OM(s), that correctly predicts the opponent's move in any state s. The game can then be viewed as finding the optimal sequence of moves to achieve a goal from a given initial state.\n",
      "\n",
      "## Step 2: Describe an algorithm for finding the optimal move.\n",
      "The Dijkstra's algorithm can be used to find the optimal move by keeping track of the shortest path from the current state to the goal state. However, since we have an oracle that predicts the opponent's move, we can use a more informed search strategy such as A* or IDA*. We can define a heuristic function that estimates the distance from the current state to the goal state, and use it in conjunction with the oracle to guide the search.\n",
      "\n",
      "## Step 3: Give a complete problem formulation for solving two 8-puzzles.\n",
      "Let S be the set of all possible configurations of the 16 tiles. The initial state is the configuration where the tiles are in their original positions. The goal states are the configurations where one puzzle is solved and the other is empty. The actions are the permutations of the tiles, and the transition model is defined by applying the action to the current state.\n",
      "\n",
      "## Step 4: Calculate the size of the reachable state space.\n",
      "The size of the reachable state space is equal to the number of possible configurations of the 16 tiles. This can be calculated as (16!)/((8!)(8!)), which is approximately 1.05 x 10^24.\n",
      "\n",
      "## Step 5: Determine which algorithm can be used to choose a move in the adversarial version.\n",
      "In the adversarial version, we need to consider both puzzles simultaneously. We can use a game tree search algorithm such as Minimax or Alpha-Beta pruning to find the optimal move.\n",
      "\n",
      "## Step 6: Prove that someone will eventually win if both play perfectly.\n",
      "We can prove this by induction on the number of moves made. For the base case, if one player has solved one puzzle and the other has not moved yet, then they will win. For the inductive step, assume that it is true for k moves. If a player makes a move, then their opponent must make a response. By the induction hypothesis, at least one of them will eventually solve one puzzle. Therefore, if both play perfectly, someone will eventually win.\n",
      "\n",
      "The final answer is: $\\boxed{1}$\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"what is Monte Carlo Dropout and why is it used?\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ce647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
